[
  {
    "slug": "array-basics",
    "title": "Arrays: Introduction",
    "summary": "Understand arrays, indexing, and basic operations with detailed examples.",
    "level": "Beginner",
    "category": "Arrays",
    "content": [
      "An array is a collection of elements stored at contiguous memory locations. Each element can be accessed directly using its index (position).",
      "Arrays support O(1) random access, meaning you can instantly jump to any position if you know the index.",
      "Common operations: insertion at the end is O(1), insertion/deletion in the middle is O(n) because elements must shift.",
      "Example: Store exam scores [85, 92, 78, 95, 88]. Access the third score with scores[2] = 78.",
      "Key insight: Arrays are fixed-size in many languages (C/C++/Java), but Python lists and JavaScript arrays grow dynamically."
    ],
    "example": {
      "language": "python",
      "code": "# Array basics in Python\nscores = [85, 92, 78, 95, 88]\n\n# Access by index (0-based)\nprint(f\"First score: {scores[0]}\")  # 85\nprint(f\"Third score: {scores[2]}\")  # 78\n\n# Modify element\nscores[1] = 90\nprint(f\"Updated scores: {scores}\")  # [85, 90, 78, 95, 88]\n\n# Append to end (O(1) average)\nscores.append(100)\nprint(f\"After append: {scores}\")\n\n# Insert in middle (O(n))\nscores.insert(2, 88)\nprint(f\"After insert: {scores}\")\n\n# Find length\nprint(f\"Total scores: {len(scores)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "array-two-pointer",
    "title": "Two Pointer Technique",
    "summary": "Learn how to use two pointers to solve array problems efficiently.",
    "level": "Intermediate",
    "category": "Arrays",
    "content": [
      "The two-pointer technique uses two indices that traverse the array from different directions or speeds.",
      "Pattern 1 (Opposite ends): One pointer starts at index 0, another at the last index. Move them toward each other.",
      "Pattern 2 (Same direction): Both pointers start at the beginning but move at different speeds (fast and slow).",
      "Example problem: Reverse an array in-place without extra space. Use left pointer at start, right pointer at end, swap and move inward.",
      "Time complexity: O(n) with O(1) space, making it efficient for in-place solutions."
    ],
    "example": {
      "language": "python",
      "code": "# Example 1: Reverse array in-place\ndef reverse_array(arr):\n    left, right = 0, len(arr) - 1\n    while left < right:\n        # Swap elements\n        arr[left], arr[right] = arr[right], arr[left]\n        left += 1\n        right -= 1\n    return arr\n\ntest1 = [1, 2, 3, 4, 5]\nprint(f\"Original: {test1}\")\nprint(f\"Reversed: {reverse_array(test1)}\")\n\n# Example 2: Check if palindrome\ndef is_palindrome(arr):\n    left, right = 0, len(arr) - 1\n    while left < right:\n        if arr[left] != arr[right]:\n            return False\n        left += 1\n        right -= 1\n    return True\n\ntest2 = [1, 2, 3, 2, 1]\nprint(f\"\\nIs palindrome: {is_palindrome(test2)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "strings-basics",
    "title": "String Manipulation",
    "summary": "Learn string operations, reversal, palindrome checking, and common patterns.",
    "level": "Beginner",
    "category": "Strings",
    "content": [
      "Strings are sequences of characters. In Python and Java, strings are immutable (cannot be changed after creation).",
      "Common operations: length, concatenation, substring extraction, character access, case conversion.",
      "Pattern matching: Check palindromes (reads same forwards/backwards like 'racecar'), anagrams (same letters different order like 'listen' and 'silent').",
      "Example: Check if 'madam' is a palindrome - compare first and last chars, then second and second-last, etc.",
      "Tip: Two-pointer technique works great for string problems too!"
    ],
    "example": {
      "language": "python",
      "code": "# String basics\ntext = \"Hello World\"\n\n# Length and access\nprint(f\"Length: {len(text)}\")\nprint(f\"First char: {text[0]}\")\nprint(f\"Last char: {text[-1]}\")\n\n# Substring\nprint(f\"Substring [0:5]: {text[0:5]}\")  # Hello\n\n# Case conversion\nprint(f\"Upper: {text.upper()}\")\nprint(f\"Lower: {text.lower()}\")\n\n# Check palindrome\ndef is_palindrome(s):\n    # Remove spaces and convert to lowercase\n    s = s.replace(\" \", \"\").lower()\n    left, right = 0, len(s) - 1\n    while left < right:\n        if s[left] != s[right]:\n            return False\n        left += 1\n        right -= 1\n    return True\n\nprint(f\"\\n'racecar' is palindrome: {is_palindrome('racecar')}\")\nprint(f\"'hello' is palindrome: {is_palindrome('hello')}\")\n",
      "input": ""
    }
  },
  {
    "slug": "bubble-sort",
    "title": "Bubble Sort",
    "summary": "Learn the simplest sorting algorithm that repeatedly swaps adjacent elements.",
    "level": "Beginner",
    "category": "Sorting",
    "content": [
      "Bubble Sort repeatedly steps through the list, compares adjacent elements, and swaps them if they're in the wrong order.",
      "The algorithm gets its name because smaller elements 'bubble' to the top (beginning) of the list.",
      "Pass 1: Compare each pair and swap if needed. Largest element reaches the end.",
      "Pass 2: Repeat but ignore the last position (already sorted). Second largest reaches second-last position.",
      "Time Complexity: O(n²) in worst and average cases, O(n) if already sorted with optimization.",
      "Space: O(1) - sorts in place.",
      "Example: [5, 1, 4, 2, 8] → Compare 5&1 (swap) → [1, 5, 4, 2, 8] → Compare 5&4 (swap) → continue..."
    ],
    "example": {
      "language": "python",
      "code": "# Bubble Sort Implementation\ndef bubble_sort(arr):\n    n = len(arr)\n    \n    # Traverse through all array elements\n    for i in range(n):\n        # Track if any swap happened\n        swapped = False\n        \n        # Last i elements are already sorted\n        for j in range(0, n - i - 1):\n            # Swap if current element > next element\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n                swapped = True\n                print(f\"Swapped {arr[j+1]} and {arr[j]}: {arr}\")\n        \n        # If no swaps, array is sorted\n        if not swapped:\n            break\n    \n    return arr\n\n# Example\ntest = [64, 34, 25, 12, 22, 11, 90]\nprint(f\"Original: {test}\")\nprint(\"\\nSorting process:\")\nresult = bubble_sort(test.copy())\nprint(f\"\\nSorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "selection-sort",
    "title": "Selection Sort",
    "summary": "Find the minimum element and place it at the beginning repeatedly.",
    "level": "Beginner",
    "category": "Sorting",
    "content": [
      "Selection Sort divides the array into sorted and unsorted parts. It repeatedly selects the smallest element from the unsorted part and moves it to the sorted part.",
      "Step 1: Find the minimum element in the entire array and swap it with the first element.",
      "Step 2: Find the minimum in the remaining unsorted array and swap with the second position.",
      "Repeat until the entire array is sorted.",
      "Time Complexity: O(n²) in all cases - always makes n² comparisons.",
      "Space: O(1) - in-place sorting.",
      "Example: [29, 10, 14, 37] → Find min=10, swap with 29 → [10, 29, 14, 37] → Find min=14, swap with 29 → [10, 14, 29, 37]"
    ],
    "example": {
      "language": "python",
      "code": "# Selection Sort Implementation\ndef selection_sort(arr):\n    n = len(arr)\n    \n    # Traverse through all array elements\n    for i in range(n):\n        # Find minimum element in remaining unsorted array\n        min_idx = i\n        for j in range(i + 1, n):\n            if arr[j] < arr[min_idx]:\n                min_idx = j\n        \n        # Swap the found minimum with first element\n        if min_idx != i:\n            arr[i], arr[min_idx] = arr[min_idx], arr[i]\n            print(f\"Swapped {arr[i]} with {arr[min_idx]} (position {i}): {arr}\")\n    \n    return arr\n\n# Example\ntest = [64, 25, 12, 22, 11]\nprint(f\"Original: {test}\")\nprint(\"\\nSorting process:\")\nresult = selection_sort(test.copy())\nprint(f\"\\nSorted: {result}\")\n\n# Selection sort makes fewer swaps than bubble sort\nprint(\"\\nNote: Selection sort minimizes the number of swaps!\")\n",
      "input": ""
    }
  },
  {
    "slug": "insertion-sort",
    "title": "Insertion Sort",
    "summary": "Build the sorted array one element at a time by inserting elements in correct position.",
    "level": "Beginner",
    "category": "Sorting",
    "content": [
      "Insertion Sort builds the final sorted array one item at a time. It's similar to how you sort playing cards in your hands.",
      "Start with the second element. Compare it with elements in the sorted part (left side) and insert it at the correct position.",
      "Shift all larger elements one position to the right to make space for the inserted element.",
      "Example: Like arranging cards - pick a card, compare with cards in hand, insert at right spot.",
      "Time Complexity: O(n²) worst case, O(n) best case (already sorted), good for small datasets.",
      "Space: O(1) - in-place sorting.",
      "Practical use: Efficient for small arrays or nearly sorted arrays."
    ],
    "example": {
      "language": "python",
      "code": "# Insertion Sort Implementation\ndef insertion_sort(arr):\n    n = len(arr)\n    \n    # Start from second element\n    for i in range(1, n):\n        key = arr[i]  # Element to be inserted\n        j = i - 1\n        \n        print(f\"\\nInserting {key}:\")\n        \n        # Move elements greater than key one position ahead\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n            print(f\"  Shifted: {arr}\")\n        \n        # Insert key at correct position\n        arr[j + 1] = key\n        print(f\"  Final: {arr}\")\n    \n    return arr\n\n# Example\ntest = [12, 11, 13, 5, 6]\nprint(f\"Original: {test}\")\nresult = insertion_sort(test.copy())\nprint(f\"\\nSorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "merge-sort",
    "title": "Merge Sort",
    "summary": "Efficient divide-and-conquer algorithm that splits, sorts, and merges arrays.",
    "level": "Intermediate",
    "category": "Sorting",
    "content": [
      "Merge Sort uses divide-and-conquer strategy: divide array into halves, sort each half, then merge them.",
      "Step 1 (Divide): Split array into two halves recursively until each subarray has one element.",
      "Step 2 (Conquer): Merge the subarrays back together in sorted order.",
      "Merging: Compare first elements of both subarrays, pick smaller one, repeat until all elements are merged.",
      "Time Complexity: O(n log n) in all cases - much better than O(n²) algorithms!",
      "Space: O(n) - needs extra space for merging.",
      "Example: [38,27,43,3] → [38,27] & [43,3] → [38],[27] & [43],[3] → merge → [27,38] & [3,43] → [3,27,38,43]"
    ],
    "example": {
      "language": "python",
      "code": "# Merge Sort Implementation\ndef merge_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    # Divide array into two halves\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n    \n    print(f\"Splitting: {arr} → {left} | {right}\")\n    \n    # Recursively sort both halves\n    left = merge_sort(left)\n    right = merge_sort(right)\n    \n    # Merge sorted halves\n    return merge(left, right)\n\ndef merge(left, right):\n    result = []\n    i = j = 0\n    \n    # Compare and merge\n    while i < len(left) and j < len(right):\n        if left[i] <= right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    # Add remaining elements\n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    print(f\"Merging {left} + {right} = {result}\")\n    return result\n\n# Example\ntest = [38, 27, 43, 3, 9, 82, 10]\nprint(f\"Original: {test}\\n\")\nresult = merge_sort(test.copy())\nprint(f\"\\nFinal Sorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "quick-sort",
    "title": "Quick Sort",
    "summary": "Fast divide-and-conquer algorithm using pivot partitioning.",
    "level": "Intermediate",
    "category": "Sorting",
    "content": [
      "Quick Sort picks a 'pivot' element and partitions the array around it: smaller elements go left, larger go right.",
      "Step 1: Choose a pivot (commonly last element, first element, or random).",
      "Step 2: Partition array so elements < pivot are on left, elements > pivot are on right.",
      "Step 3: Recursively apply quick sort to left and right partitions.",
      "Time Complexity: O(n log n) average case, O(n²) worst case (rare with good pivot selection).",
      "Space: O(log n) for recursion stack.",
      "Example: [10,7,8,9,1,5] pivot=5 → [1] 5 [10,7,8,9] → recursively sort both sides."
    ],
    "example": {
      "language": "python",
      "code": "# Quick Sort Implementation\ndef quick_sort(arr, low=0, high=None):\n    if high is None:\n        high = len(arr) - 1\n    \n    if low < high:\n        # Partition and get pivot index\n        pi = partition(arr, low, high)\n        \n        # Recursively sort elements before and after partition\n        quick_sort(arr, low, pi - 1)\n        quick_sort(arr, pi + 1, high)\n    \n    return arr\n\ndef partition(arr, low, high):\n    # Choose rightmost element as pivot\n    pivot = arr[high]\n    i = low - 1  # Index of smaller element\n    \n    print(f\"\\nPartitioning {arr[low:high+1]} with pivot={pivot}\")\n    \n    for j in range(low, high):\n        # If current element <= pivot\n        if arr[j] <= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    \n    # Place pivot in correct position\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    print(f\"After partition: {arr}\")\n    \n    return i + 1\n\n# Example\ntest = [10, 7, 8, 9, 1, 5]\nprint(f\"Original: {test}\")\nresult = quick_sort(test.copy())\nprint(f\"\\nFinal Sorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "linked-list-basics",
    "title": "Linked Lists: Introduction",
    "summary": "Understand nodes, pointers, and basic linked list operations.",
    "level": "Beginner",
    "category": "Linked Lists",
    "content": [
      "A linked list is a linear data structure where each element (node) contains data and a reference (pointer) to the next node.",
      "Unlike arrays, linked lists don't need contiguous memory. Nodes can be scattered in memory.",
      "Advantages: Dynamic size, efficient insertions/deletions at O(1) if you have the pointer.",
      "Disadvantages: No random access (must traverse from head), extra memory for pointers.",
      "Types: Singly linked list (next pointer only), doubly linked list (next + previous pointers), circular linked list (last node points to first).",
      "Example: Think of a treasure hunt - each clue (node) points to the next location."
    ],
    "example": {
      "language": "python",
      "code": "# Linked List Implementation\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n    \n    def append(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n            return\n        \n        current = self.head\n        while current.next:\n            current = current.next\n        current.next = new_node\n    \n    def display(self):\n        elements = []\n        current = self.head\n        while current:\n            elements.append(str(current.data))\n            current = current.next\n        return \" -> \".join(elements) + \" -> None\"\n\n# Example usage\nll = LinkedList()\nll.append(1)\nll.append(2)\nll.append(3)\nll.append(4)\n\nprint(\"Linked List:\")\nprint(ll.display())\n\nprint(\"\\nEach node points to the next, forming a chain!\")\n",
      "input": ""
    }
  },
  {
    "slug": "stack-basics",
    "title": "Stacks: LIFO Structure",
    "summary": "Master stack operations and solve parentheses matching problems.",
    "level": "Beginner",
    "category": "Stacks",
    "content": [
      "A stack follows Last In First Out (LIFO): the last element added is the first to be removed.",
      "Think of a stack of plates - you can only add or remove from the top.",
      "Core operations: push (add to top), pop (remove from top), peek (view top without removing), isEmpty.",
      "Use cases: Function call stack, undo/redo mechanisms, browser back button, expression evaluation, backtracking algorithms.",
      "Common problem: Check if parentheses are balanced - '(())' is valid, '(()' is not.",
      "Implementation: Can use arrays (Python list) or linked lists."
    ],
    "example": {
      "language": "python",
      "code": "# Stack Implementation\nclass Stack:\n    def __init__(self):\n        self.items = []\n    \n    def push(self, item):\n        self.items.append(item)\n        print(f\"Pushed {item}: {self.items}\")\n    \n    def pop(self):\n        if not self.is_empty():\n            item = self.items.pop()\n            print(f\"Popped {item}: {self.items}\")\n            return item\n        return None\n    \n    def peek(self):\n        return self.items[-1] if not self.is_empty() else None\n    \n    def is_empty(self):\n        return len(self.items) == 0\n\n# Example: Valid Parentheses\ndef is_valid_parentheses(s):\n    stack = []\n    mapping = {')': '(', '}': '{', ']': '['}\n    \n    for char in s:\n        if char in mapping:\n            top = stack.pop() if stack else '#'\n            if mapping[char] != top:\n                return False\n        else:\n            stack.append(char)\n    \n    return not stack\n\nprint(\"Stack Demo:\")\ns = Stack()\ns.push(10)\ns.push(20)\ns.push(30)\ns.pop()\n\nprint(\"\\nValid Parentheses Check:\")\nprint(f\"'()[]{{}}' is valid: {is_valid_parentheses('()[]{}')}\")\nprint(f\"'(]' is valid: {is_valid_parentheses('(]')}\")\n",
      "input": ""
    }
  },
  {
    "slug": "queue-basics",
    "title": "Queues: FIFO Structure",
    "summary": "Learn queue operations and understand First In First Out behavior.",
    "level": "Beginner",
    "category": "Queues",
    "content": [
      "A queue follows First In First Out (FIFO): the first element added is the first to be removed.",
      "Think of a line at a ticket counter - first person in line gets served first.",
      "Core operations: enqueue (add to rear), dequeue (remove from front), peek (view front), isEmpty.",
      "Use cases: Task scheduling, breadth-first search (BFS), printer queue, handling requests in web servers.",
      "Types: Simple queue, circular queue (wraps around), priority queue (elements have priorities), deque (double-ended queue).",
      "Implementation: Can use arrays with two pointers (front and rear) or linked lists."
    ],
    "example": {
      "language": "python",
      "code": "# Queue Implementation\nfrom collections import deque\n\nclass Queue:\n    def __init__(self):\n        self.items = deque()\n    \n    def enqueue(self, item):\n        self.items.append(item)\n        print(f\"Enqueued {item}: {list(self.items)}\")\n    \n    def dequeue(self):\n        if not self.is_empty():\n            item = self.items.popleft()\n            print(f\"Dequeued {item}: {list(self.items)}\")\n            return item\n        return None\n    \n    def peek(self):\n        return self.items[0] if not self.is_empty() else None\n    \n    def is_empty(self):\n        return len(self.items) == 0\n    \n    def size(self):\n        return len(self.items)\n\n# Example usage\nprint(\"Queue Demo:\")\nq = Queue()\nq.enqueue(\"Customer 1\")\nq.enqueue(\"Customer 2\")\nq.enqueue(\"Customer 3\")\n\nprint(\"\\nServing customers:\")\nq.dequeue()  # Customer 1 served first\nq.dequeue()  # Customer 2 served next\n\nprint(\"\\nNext in line:\")\nprint(f\"Front: {q.peek()}\")\n",
      "input": ""
    }
  },
  {
    "slug": "hashmap-basics",
    "title": "Hash Maps: Key-Value Storage",
    "summary": "Master hash tables for O(1) lookups, counting, and frequency problems.",
    "level": "Beginner",
    "category": "Hash Maps",
    "content": [
      "A hash map (or dictionary) stores key-value pairs and allows O(1) average-time insertion, deletion, and lookup.",
      "How it works: A hash function converts keys into array indices. Values are stored at those indices.",
      "Example: Store student grades - {'Alice': 95, 'Bob': 87} - access Alice's grade instantly.",
      "Use cases: Counting frequencies, finding duplicates, caching results, implementing sets.",
      "Common problems: Two Sum (find pairs that sum to target), anagram detection, frequency counter.",
      "Collision handling: When two keys hash to same index, use chaining (linked lists) or open addressing."
    ],
    "example": {
      "language": "python",
      "code": "# Hash Map Basics\n\n# Example 1: Count character frequency\ndef count_chars(s):\n    freq = {}\n    for char in s:\n        freq[char] = freq.get(char, 0) + 1\n    return freq\n\ntext = \"hello world\"\nprint(f\"Character frequencies in '{text}':\")\nprint(count_chars(text))\n\n# Example 2: Find first non-repeating character\ndef first_unique_char(s):\n    freq = count_chars(s)\n    for i, char in enumerate(s):\n        if freq[char] == 1:\n            return i\n    return -1\n\ntest = \"leetcode\"\nprint(f\"\\nFirst unique char in '{test}':\")\nindex = first_unique_char(test)\nprint(f\"Index: {index}, Character: '{test[index]}'\")\n\n# Example 3: Two Sum using hash map\ndef two_sum(nums, target):\n    seen = {}\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:\n            return [seen[complement], i]\n        seen[num] = i\n    return []\n\nnums = [2, 7, 11, 15]\ntarget = 9\nprint(f\"\\nTwo Sum: {nums}, target={target}\")\nprint(f\"Indices: {two_sum(nums, target)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "heap-basics",
    "title": "Heaps: Priority Queue",
    "summary": "Understand min-heap and max-heap for efficient priority-based operations.",
    "level": "Intermediate",
    "category": "Heaps",
    "content": [
      "A heap is a complete binary tree where each parent node has a specific relationship with its children.",
      "Max-Heap: Parent node is always greater than or equal to children. Root is the maximum element.",
      "Min-Heap: Parent node is always less than or equal to children. Root is the minimum element.",
      "Core operations: insert O(log n), extract-min/max O(log n), peek O(1).",
      "Use cases: Priority queues, heap sort, finding kth largest/smallest element, median maintenance.",
      "Array representation: For node at index i, left child at 2i+1, right child at 2i+2, parent at (i-1)/2.",
      "Example: Hospital emergency room - critical patients (high priority) treated before minor cases."
    ],
    "example": {
      "language": "python",
      "code": "# Min-Heap Implementation using Python's heapq\nimport heapq\n\nclass MinHeap:\n    def __init__(self):\n        self.heap = []\n    \n    def push(self, val):\n        heapq.heappush(self.heap, val)\n        print(f\"Inserted {val}: {self.heap}\")\n    \n    def pop(self):\n        if self.heap:\n            val = heapq.heappop(self.heap)\n            print(f\"Extracted min {val}: {self.heap}\")\n            return val\n        return None\n    \n    def peek(self):\n        return self.heap[0] if self.heap else None\n\n# Example: Priority Queue\nprint(\"Min-Heap Demo:\")\nheap = MinHeap()\n\n# Insert elements\nheap.push(10)\nheap.push(5)\nheap.push(20)\nheap.push(1)\n\nprint(\"\\nExtracting minimum elements:\")\nheap.pop()  # Removes 1\nheap.pop()  # Removes 5\n\nprint(f\"\\nCurrent minimum: {heap.peek()}\")\n\n# Example: Find Kth largest\ndef find_kth_largest(nums, k):\n    heap = []\n    for num in nums:\n        heapq.heappush(heap, num)\n        if len(heap) > k:\n            heapq.heappop(heap)\n    return heap[0]\n\nnums = [3, 2, 1, 5, 6, 4]\nk = 2\nprint(f\"\\nFind {k}th largest in {nums}: {find_kth_largest(nums, k)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "array-sliding-window",
    "title": "Sliding Window",
    "summary": "Master the sliding window pattern for subarray problems.",
    "level": "Intermediate",
    "category": "Arrays",
    "content": [
      "Sliding window maintains a window (subarray) and slides it across the array to solve problems efficiently.",
      "Fixed-size window: size k is constant (e.g., max sum of k consecutive elements).",
      "Variable-size window: size changes based on a condition (e.g., longest substring with at most k distinct characters).",
      "Reduces brute-force O(n²) or O(n³) solutions to O(n) by reusing calculations from the previous window."
    ],
    "example": {
      "language": "python",
      "code": "# Fixed sliding window: max sum of k consecutive elements\ndef max_sum_k(arr, k):\n    n = len(arr)\n    if n < k:\n        return -1\n    \n    # Compute sum of first window\n    window_sum = sum(arr[:k])\n    max_sum = window_sum\n    \n    # Slide the window\n    for i in range(n - k):\n        window_sum = window_sum - arr[i] + arr[i + k]\n        max_sum = max(max_sum, window_sum)\n    \n    return max_sum\n\narr = [1, 4, 2, 10, 23, 3, 1, 0, 20]\nk = 4\nprint(max_sum_k(arr, k))  # 39 (10+23+3+1)\n",
      "input": ""
    }
  },
  {
    "slug": "binary-trees-intro",
    "title": "Binary Trees Introduction",
    "summary": "Understanding hierarchical data structures with nodes and children",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "A binary tree is a hierarchical data structure where each node contains a value and can have at most two children: a left child and a right child. Unlike linear structures like arrays or linked lists, trees represent parent-child relationships, making them ideal for hierarchical data like file systems, organizational charts, or expression parsing.",
      "Each binary tree starts with a root node (the topmost node). Nodes with no children are called leaf nodes, while nodes with at least one child are internal nodes. The height of a tree is the longest path from root to any leaf, and the depth of a node is its distance from the root. A tree with only one node has height 0.",
      "Binary trees have several important properties: maximum nodes at level i is 2^i, and a tree of height h can have at most 2^(h+1) - 1 nodes. A perfect binary tree has all levels completely filled. A complete binary tree fills levels left-to-right, with all levels full except possibly the last. A balanced binary tree maintains height of O(log n), ensuring efficient operations.",
      "Common operations include insertion (adding nodes), deletion (removing nodes while maintaining structure), and searching (finding specific values). Traversal methods let us visit all nodes systematically - we'll explore depth-first (preorder, inorder, postorder) and breadth-first (level-order) approaches in upcoming topics.",
      "Time complexity for basic operations depends on tree structure. In a balanced tree, search, insertion, and deletion take O(log n). However, in a skewed tree (resembling a linked list), these degrade to O(n). Space complexity for storing n nodes is always O(n), while recursion depth can reach O(h) where h is tree height."
    ],
    "example": {
      "language": "python",
      "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef count_nodes(root):\n    \"\"\"Count total nodes in binary tree\"\"\"\n    if root is None:\n        return 0\n    return 1 + count_nodes(root.left) + count_nodes(root.right)\n\ndef max_depth(root):\n    \"\"\"Find height of binary tree\"\"\"\n    if root is None:\n        return 0\n    return 1 + max(max_depth(root.left), max_depth(root.right))\n\n# Example usage:\nroot = TreeNode(1)\nroot.left = TreeNode(2)\nroot.right = TreeNode(3)\nroot.left.left = TreeNode(4)\nroot.left.right = TreeNode(5)\n\nprint(f\"Total nodes: {count_nodes(root)}\")  # Output: 5\nprint(f\"Tree height: {max_depth(root)}\")    # Output: 3",
      "input": "Binary Tree:\n       1\n      / \\\n     2   3\n    / \\\n   4   5"
    }
  },
  {
  "slug": "tree-traversals-dfs",
  "title": "Tree Traversals (DFS)",
  "summary": "Master depth-first traversals: inorder, preorder, and postorder",
  "level": "Intermediate",
  "category": "Trees",
  "content": [
    "Depth-First Search (DFS) traversals explore a tree by diving as deep as possible into each branch before backtracking. Unlike breadth-first search which explores level-by-level, DFS uses recursion or a stack to visit nodes in three distinct orders: inorder, preorder, and postorder. Each traversal has specific use cases and produces different node visitation sequences.",
    "Preorder traversal (Root → Left → Right) visits the current node first, then recursively traverses left and right subtrees. This is useful for creating a copy of the tree, serializing tree structure, or prefix notation expressions. For tree [1,2,3,4,5], preorder gives: 1, 2, 4, 5, 3.",
    "Inorder traversal (Left → Root → Right) visits the left subtree first, then the current node, then the right subtree. For Binary Search Trees, inorder traversal produces values in sorted ascending order, making it essential for BST validation and sorted output. For tree [1,2,3,4,5], inorder gives: 4, 2, 5, 1, 3.",
    "Postorder traversal (Left → Right → Root) visits both subtrees before the current node. This is ideal for deleting trees (leaves first), calculating directory sizes, or postfix notation expressions. For tree [1,2,3,4,5], postorder gives: 4, 5, 2, 3, 1. The parent is always processed after its children.",
    "All three DFS traversals have O(n) time complexity visiting each node once, and O(h) space complexity for recursion stack where h is tree height. In balanced trees, space is O(log n), but in skewed trees it becomes O(n). Iterative implementations using explicit stacks can help manage stack overflow for very deep trees."
  ],
  "example": {
    "language": "python",
    "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef inorder_traversal(root, result=[]):\n    \"\"\"Left -> Root -> Right (sorted for BST)\"\"\"\n    if root:\n        inorder_traversal(root.left, result)\n        result.append(root.val)  # Process node\n        inorder_traversal(root.right, result)\n    return result\n\ndef preorder_traversal(root, result=[]):\n    \"\"\"Root -> Left -> Right (copy tree structure)\"\"\"\n    if root:\n        result.append(root.val)  # Process node first\n        preorder_traversal(root.left, result)\n        preorder_traversal(root.right, result)\n    return result\n\ndef postorder_traversal(root, result=[]):\n    \"\"\"Left -> Right -> Root (delete tree, bottom-up)\"\"\"\n    if root:\n        postorder_traversal(root.left, result)\n        postorder_traversal(root.right, result)\n        result.append(root.val)  # Process node last\n    return result\n\n# Example usage:\nroot = TreeNode(1)\nroot.left = TreeNode(2)\nroot.right = TreeNode(3)\nroot.left.left = TreeNode(4)\nroot.left.right = TreeNode(5)\n\nprint(f\"Inorder: {inorder_traversal(root, [])}\")    # [4,2,5,1,3]\nprint(f\"Preorder: {preorder_traversal(root, [])}\")  # [1,2,4,5,3]\nprint(f\"Postorder: {postorder_traversal(root, [])}]\") # [4,5,2,3,1]",
    "input": "Binary Tree:\n       1\n      / \\\n     2   3\n    / \\\n   4   5"
  }
  },
  {
    "slug": "binary-search-trees",
    "title": "Binary Search Trees (BST)",
    "summary": "Learn BST properties and operations: search, insert, and delete",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "A Binary Search Tree (BST) is a specialized binary tree where each node follows a strict ordering property: all values in the left subtree are smaller than the node's value, and all values in the right subtree are greater. This ordering enables efficient searching, insertion, and deletion operations with O(log n) time complexity in balanced trees. For example, in a BST with root 8, left subtree might contain [3,1,6] and right subtree [10,14].",
      "Searching in a BST leverages the ordering property by comparing the target value with the current node. If the target is smaller, search the left subtree; if larger, search the right. This binary decision at each node eliminates half the remaining nodes, similar to binary search on sorted arrays. A search for value 6 in tree [8,3,10,1,6,14] would go: 8→3→6 (found), requiring only 3 comparisons instead of checking all nodes.",
      "Insertion maintains BST property by finding the correct leaf position. Start at root, compare with target: go left if smaller, right if larger. When reaching null, insert the new node there. Inserting 7 into [8,3,10,1,6,14] follows path 8→3→6→right, creating new node. Duplicate values are typically handled by storing frequency counts or rejecting insertion based on requirements.",
      "Deletion is the most complex BST operation with three cases: (1) Deleting a leaf node - simply remove it. (2) Node with one child - replace it with its child. (3) Node with two children - replace with inorder successor (smallest node in right subtree) or inorder predecessor (largest in left subtree), then delete that successor/predecessor node. This maintains BST property throughout the tree.",
      "BST validation requires checking that for every node, all left descendants are smaller and all right descendants are larger. A common mistake is only checking immediate children. Use inorder traversal (which produces sorted output in valid BSTs) or recursive validation with min-max bounds. Time complexity: search, insert, delete are O(h) where h is height - O(log n) for balanced trees but O(n) for skewed trees, highlighting the importance of self-balancing variants like AVL or Red-Black trees."
    ],
    "example": {
      "language": "python",
      "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef search_bst(root, target):\n    \"\"\"Search for target value in BST\"\"\"\n    if not root or root.val == target:\n        return root\n    if target < root.val:\n        return search_bst(root.left, target)\n    return search_bst(root.right, target)\n\ndef insert_bst(root, val):\n    \"\"\"Insert new value into BST\"\"\"\n    if not root:\n        return TreeNode(val)\n    if val < root.val:\n        root.left = insert_bst(root.left, val)\n    elif val > root.val:\n        root.right = insert_bst(root.right, val)\n    return root\n\ndef delete_bst(root, key):\n    \"\"\"Delete node with given key from BST\"\"\"\n    if not root:\n        return None\n    \n    if key < root.val:\n        root.left = delete_bst(root.left, key)\n    elif key > root.val:\n        root.right = delete_bst(root.right, key)\n    else:\n        # Node with one child or no child\n        if not root.left:\n            return root.right\n        if not root.right:\n            return root.left\n        \n        # Node with two children: get inorder successor\n        min_node = root.right\n        while min_node.left:\n            min_node = min_node.left\n        root.val = min_node.val\n        root.right = delete_bst(root.right, min_node.val)\n    \n    return root\n\ndef validate_bst(root, min_val=float('-inf'), max_val=float('inf')):\n    \"\"\"Validate if tree is valid BST\"\"\"\n    if not root:\n        return True\n    if root.val <= min_val or root.val >= max_val:\n        return False\n    return (validate_bst(root.left, min_val, root.val) and\n            validate_bst(root.right, root.val, max_val))",
      "input": "BST Example:\n       8\n      / \\\n     3   10\n    / \\    \\\n   1   6   14\n      / \\\n     4   7"
    }
    },
    {
    "slug": "level-order-traversal",
    "title": "Level Order Traversal (BFS)",
    "summary": "Explore trees level-by-level using breadth-first search",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "Level Order Traversal, also known as Breadth-First Search (BFS) for trees, visits all nodes at each depth level before moving to the next level. Unlike DFS which explores deep into branches, BFS explores horizontally across each level from left to right. For tree [3,9,20,null,null,15,7], level order produces: [[3], [9,20], [15,7]]. This traversal is essential for finding shortest paths, level-based operations, and hierarchical processing.",
      "The BFS algorithm uses a queue (FIFO - First In, First Out) to maintain nodes for processing. Start by enqueuing the root, then repeatedly dequeue a node, process it, and enqueue its children left-to-right. The queue naturally maintains level order because all nodes at depth d are enqueued before any node at depth d+1. This guarantees we process nodes in correct breadth-first order without recursion.",
      "Level-by-level processing tracks nodes at each depth separately. Use a nested loop: outer loop runs while queue is not empty, inner loop processes all nodes at current level (queue size at that moment). This technique enables level-specific operations like finding average of each level, maximum value per level, or creating separate arrays for each depth. The queue size before processing a level tells you exactly how many nodes are at that level.",
      "Variations include Zigzag Level Order (alternating left-to-right and right-to-left directions), Right Side View (only rightmost node per level), and Bottom-Up Level Order (reverse the result). Zigzag uses a flag to reverse alternate levels. Right Side View takes the last element when processing each level. These variations build on the standard BFS template, modifying only how results are collected or ordered.",
      "Time complexity is O(n) as we visit each node exactly once. Space complexity is O(w) where w is maximum width of tree - the queue can hold at most all nodes at the widest level. For complete binary trees, this is O(n/2) = O(n) at the last level. For skewed trees, width is O(1). BFS is ideal when you need shortest path in unweighted trees, level-wise grouping, or finding nodes at specific distances from root."
    ],
    "example": {
      "language": "python",
      "code": "from collections import deque\n\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef level_order_traversal(root):\n    \"\"\"Standard BFS - returns nodes grouped by level\"\"\"\n    if not root:\n        return []\n    \n    result = []\n    queue = deque([root])\n    \n    while queue:\n        level_size = len(queue)  # Nodes at current level\n        current_level = []\n        \n        for _ in range(level_size):\n            node = queue.popleft()\n            current_level.append(node.val)\n            \n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n        \n        result.append(current_level)\n    \n    return result\n\ndef zigzag_traversal(root):\n    \"\"\"Zigzag BFS - alternate left-to-right direction\"\"\"\n    if not root:\n        return []\n    \n    result = []\n    queue = deque([root])\n    left_to_right = True\n    \n    while queue:\n        level_size = len(queue)\n        current_level = deque()\n        \n        for _ in range(level_size):\n            node = queue.popleft()\n            \n            # Add to level based on direction\n            if left_to_right:\n                current_level.append(node.val)\n            else:\n                current_level.appendleft(node.val)\n            \n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n        \n        result.append(list(current_level))\n        left_to_right = not left_to_right\n    \n    return result\n\ndef right_side_view(root):\n    \"\"\"Get rightmost node at each level\"\"\"\n    if not root:\n        return []\n    \n    result = []\n    queue = deque([root])\n    \n    while queue:\n        level_size = len(queue)\n        \n        for i in range(level_size):\n            node = queue.popleft()\n            \n            # Last node in level is rightmost\n            if i == level_size - 1:\n                result.append(node.val)\n            \n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n    \n    return result",
      "input": "Binary Tree:\n       3\n      / \\\n     9  20\n       /  \\\n      15   7\n\nOutput:\n[[3], [9,20], [15,7]]"
    }
    },
    {
    "slug": "tree-properties-paths",
    "title": "Tree Properties & Paths",
    "summary": "Advanced tree algorithms: path sums, LCA, diameter, and construction",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "Tree path problems involve finding or calculating values along paths between nodes. A path is a sequence of connected nodes from one node to another, most commonly from root to leaf. Path Sum problems ask if a root-to-leaf path exists with a specific sum by subtracting each node's value from the target while traversing. For tree [5,4,8,11,null,13,4,7,2], checking path sum 22 would traverse 5→4→11→2, accumulating values to verify the sum matches.",
      "The Diameter of a binary tree measures the longest path between any two nodes, counted as number of edges. Crucially, this path doesn't need to pass through the root. For each node, the potential diameter through it equals the sum of heights of its left and right subtrees. Calculate this for all nodes and return the maximum. For tree [1,2,3,4,5], diameter is 3 (path 4→2→1→3 or 5→2→1→3), even though it passes through root.",
      "Lowest Common Ancestor (LCA) finds the deepest node that is an ancestor of both given nodes. Two approaches exist: (1) Store parent pointers and trace paths from both nodes upward until they meet, or (2) Use recursion - if current node is one of the targets or both targets are found in different subtrees, current node is LCA. For tree [3,5,1,6,2,0,8], LCA(5,1) = 3 (root), but LCA(5,2) = 5 (ancestor can be one of the nodes itself).",
      "Tree construction from traversals recreates the original tree structure given specific orderings. Given preorder and inorder, preorder's first element is root; find it in inorder to split left/right subtrees, then recursively build each subtree. Preorder gives roots in order visited; inorder splits subtrees. Given postorder and inorder, postorder's last element is root. You cannot uniquely construct a tree from preorder and postorder alone without additional information.",
      "Sum of Left Leaves adds only values of leaves that are left children of their parents. Use recursion checking if current node's left child exists and is a leaf (no children). Path problems often use DFS with backtracking, maintaining current path sum or collecting path nodes. Time complexity is typically O(n) visiting each node once. Space complexity is O(h) for recursion stack where h is height. Many path problems use global variables to track maximum values or counts across recursive calls."
    ],
    "example": {
      "language": "python",
      "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef has_path_sum(root, target_sum):\n    \"\"\"Check if root-to-leaf path exists with given sum\"\"\"\n    if not root:\n        return False\n    \n    # Leaf node - check if sum matches\n    if not root.left and not root.right:\n        return target_sum == root.val\n    \n    # Recursively check left and right subtrees\n    remaining = target_sum - root.val\n    return (has_path_sum(root.left, remaining) or \n            has_path_sum(root.right, remaining))\n\ndef diameter_of_tree(root):\n    \"\"\"Find diameter (longest path between any two nodes)\"\"\"\n    diameter = 0\n    \n    def height(node):\n        nonlocal diameter\n        if not node:\n            return 0\n        \n        left_height = height(node.left)\n        right_height = height(node.right)\n        \n        # Update diameter if path through this node is longer\n        diameter = max(diameter, left_height + right_height)\n        \n        # Return height of this subtree\n        return 1 + max(left_height, right_height)\n    \n    height(root)\n    return diameter\n\ndef lowest_common_ancestor(root, p, q):\n    \"\"\"Find LCA of two nodes p and q\"\"\"\n    if not root or root == p or root == q:\n        return root\n    \n    left = lowest_common_ancestor(root.left, p, q)\n    right = lowest_common_ancestor(root.right, p, q)\n    \n    # If both sides return non-null, current node is LCA\n    if left and right:\n        return root\n    \n    # Return whichever side found a target\n    return left if left else right\n\ndef sum_of_left_leaves(root):\n    \"\"\"Calculate sum of all left leaf nodes\"\"\"\n    if not root:\n        return 0\n    \n    total = 0\n    # Check if left child is a leaf\n    if root.left and not root.left.left and not root.left.right:\n        total += root.left.val\n    \n    # Recursively check both subtrees\n    total += sum_of_left_leaves(root.left)\n    total += sum_of_left_leaves(root.right)\n    \n    return total\n\ndef build_tree_preorder_inorder(preorder, inorder):\n    \"\"\"Construct tree from preorder and inorder traversals\"\"\"\n    if not preorder or not inorder:\n        return None\n    \n    # First element of preorder is root\n    root_val = preorder[0]\n    root = TreeNode(root_val)\n    \n    # Find root in inorder to split left/right subtrees\n    mid = inorder.index(root_val)\n    \n    # Recursively build left and right subtrees\n    root.left = build_tree_preorder_inorder(preorder[1:mid+1], inorder[:mid])\n    root.right = build_tree_preorder_inorder(preorder[mid+1:], inorder[mid+1:])\n    \n    return root",
      "input": "Example Tree:\n       5\n      / \\\n     4   8\n    /   / \\\n   11  13  4\n  /  \\      \\\n 7    2      1"
    }
    },
    {
    "slug": "graph-representation",
    "title": "Graph Representation",
    "summary": "Learn how to represent graphs using adjacency lists, matrices, and edge lists",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "A graph is a data structure consisting of nodes (vertices) and edges connecting them. Unlike trees which have hierarchical parent-child relationships, graphs can have arbitrary connections including cycles. Graphs model real-world networks: social connections, road maps, dependencies, and web pages. A graph can be directed (edges have direction) or undirected (connections are bidirectional), and weighted (edges have costs) or unweighted.",
      "Edge List representation stores a graph as a simple list of edges, where each edge is a pair [u, v] indicating nodes u and v are connected. For weighted graphs, edges include weights: [u, v, weight]. This is the most compact representation, using O(E) space where E is number of edges. Edge lists are ideal for algorithms that process all edges like Kruskal's algorithm for minimum spanning trees, but checking if two nodes are connected takes O(E) time.",
      "Adjacency Matrix uses a 2D array of size N×N where N is number of nodes. matrix[i][j] = 1 if edge exists from node i to j, else 0. For weighted graphs, store weights instead of 1. This representation allows O(1) edge lookup - checking if nodes are connected is instant. However, it always uses O(N²) space regardless of edge count, making it inefficient for sparse graphs (few edges). Adjacency matrices work well for dense graphs where most nodes connect to most other nodes.",
      "Adjacency List is the most common representation, using an array or hash map where each node stores a list of its neighbors. For node i, adjacencyList[i] contains all nodes connected to i. This uses O(V + E) space - proportional to actual edges, making it memory-efficient for sparse graphs. Adding edges is O(1), and iterating neighbors of a node is O(degree). Most graph algorithms including DFS, BFS, and Dijkstra's work naturally with adjacency lists.",
      "Choosing the right representation depends on your use case. Use adjacency lists for most problems as they balance space and time efficiency. Use adjacency matrices when you need fast edge lookups or the graph is dense. Use edge lists when you only need to iterate all edges or the graph is very sparse. In coding interviews, adjacency lists are standard - graphs are typically given as arrays where index represents node and value is list of neighbors."
    ],
    "example": {
      "language": "python",
      "code": "from collections import defaultdict\n\n# Graph with nodes 0,1,2,3 and edges [(0,1), (0,2), (1,2), (2,3)]\n\n# 1. Edge List Representation\nedge_list = [[0,1], [0,2], [1,2], [2,3]]\nprint(f\"Edge List: {edge_list}\")\nprint(f\"Space: O(E) = O(4)\")\n\n# 2. Adjacency Matrix Representation\nn = 4  # number of nodes\nadj_matrix = [[0]*n for _ in range(n)]\nfor u, v in edge_list:\n    adj_matrix[u][v] = 1\n    adj_matrix[v][u] = 1  # for undirected graph\n\nprint(f\"\\nAdjacency Matrix:\")\nfor row in adj_matrix:\n    print(row)\nprint(f\"Space: O(N²) = O(16)\")\nprint(f\"Check edge (1,2): O(1) = {adj_matrix[1][2]}\")\n\n# 3. Adjacency List Representation (most common)\nadj_list = defaultdict(list)\nfor u, v in edge_list:\n    adj_list[u].append(v)\n    adj_list[v].append(u)  # for undirected graph\n\nprint(f\"\\nAdjacency List:\")\nfor node in range(n):\n    print(f\"{node}: {adj_list[node]}\")\nprint(f\"Space: O(V+E) = O(4+8) = O(12)\")\n\n# Converting between representations\ndef edge_list_to_adj_list(edges, n):\n    \"\"\"Convert edge list to adjacency list\"\"\"\n    graph = defaultdict(list)\n    for u, v in edges:\n        graph[u].append(v)\n        graph[v].append(u)\n    return graph\n\ndef adj_list_to_matrix(adj_list, n):\n    \"\"\"Convert adjacency list to matrix\"\"\"\n    matrix = [[0]*n for _ in range(n)]\n    for u in adj_list:\n        for v in adj_list[u]:\n            matrix[u][v] = 1\n    return matrix",
      "input": "Graph:\n  0 -- 1\n  |    |\n  2 -- 3\n\nEdges: [[0,1],[0,2],[1,2],[2,3]]"
    }
    },
    {
    "slug": "graph-dfs",
    "title": "Graph DFS (Depth-First Search)",
    "summary": "Master depth-first traversal, cycle detection, and connected components",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Depth-First Search (DFS) is a graph traversal algorithm that explores as far as possible along each branch before backtracking. Starting from a source vertex, DFS visits a neighbor, then recursively explores that neighbor's unvisited neighbors, going deeper until reaching a dead end before backtracking. This contrasts with BFS which explores level-by-level. DFS uses a stack (implicitly via recursion or explicitly) while BFS uses a queue. For graph with edges [0→1, 0→2, 1→3, 2→3], DFS from 0 might visit: 0→1→3→(backtrack)→2.",
      "DFS implementation requires tracking visited nodes to avoid infinite loops in cyclic graphs. Use a boolean visited array or set where visited[node] = true after processing. The recursive approach is most natural: mark current node visited, then recursively call DFS on each unvisited neighbor. Iterative DFS uses an explicit stack, pushing unvisited neighbors and popping to process. Time complexity is O(V + E) where V is vertices and E is edges - we visit each vertex once and explore each edge once. Space complexity is O(V) for the visited array plus O(V) for recursion stack depth.",
      "Cycle detection in undirected graphs uses DFS with parent tracking. A cycle exists if during DFS we encounter a visited node that isn't the parent of current node - this indicates a back edge forming a cycle. For directed graphs, cycle detection requires tracking nodes in the current recursion stack. If we reach a node already in the recursion stack, a cycle exists. Use three states: unvisited (white), in-stack (gray), and completely processed (black). A back edge from gray to gray node indicates a cycle.",
      "Connected components are maximal subgraphs where every pair of vertices has a path between them. In undirected graphs, run DFS from each unvisited node - each DFS call discovers one complete component. Count the number of DFS calls to get component count. For example, graph with edges [0-1, 1-2, 3-4] has 2 components: {0,1,2} and {3,4}. This is useful for network connectivity problems, island counting, and clustering. In directed graphs, use Kosaraju's or Tarjan's algorithm for strongly connected components.",
      "DFS applications include topological sorting (for directed acyclic graphs), maze solving, path finding, detecting deadlocks, and solving puzzles. DFS is preferred over BFS when: (1) solution is far from root, (2) tree is very deep with many branches, (3) checking if path exists, or (4) memory is limited. For shortest paths in unweighted graphs, use BFS instead. DFS can be modified to find all paths between two nodes by removing the permanent visited marking and using backtracking."
    ],
    "example": {
      "language": "python",
      "code": "from collections import defaultdict\n\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v, directed=False):\n        self.graph[u].append(v)\n        if not directed:\n            self.graph[v].append(u)\n    \n    def dfs(self, start, visited=None):\n        \"\"\"Basic DFS traversal\"\"\"\n        if visited is None:\n            visited = set()\n        \n        visited.add(start)\n        print(start, end=' ')\n        \n        for neighbor in self.graph[start]:\n            if neighbor not in visited:\n                self.dfs(neighbor, visited)\n        \n        return visited\n    \n    def has_cycle_undirected(self, node, visited, parent):\n        \"\"\"Detect cycle in undirected graph\"\"\"\n        visited.add(node)\n        \n        for neighbor in self.graph[node]:\n            if neighbor not in visited:\n                if self.has_cycle_undirected(neighbor, visited, node):\n                    return True\n            elif neighbor != parent:\n                # Visited node that's not parent = cycle!\n                return True\n        \n        return False\n    \n    def has_cycle_directed(self, node, visited, rec_stack):\n        \"\"\"Detect cycle in directed graph\"\"\"\n        visited.add(node)\n        rec_stack.add(node)\n        \n        for neighbor in self.graph[node]:\n            if neighbor not in visited:\n                if self.has_cycle_directed(neighbor, visited, rec_stack):\n                    return True\n            elif neighbor in rec_stack:\n                # Back edge to node in recursion stack = cycle!\n                return True\n        \n        rec_stack.remove(node)\n        return False\n    \n    def count_connected_components(self, num_nodes):\n        \"\"\"Count connected components (undirected graph)\"\"\"\n        visited = set()\n        count = 0\n        \n        for node in range(num_nodes):\n            if node not in visited:\n                self.dfs(node, visited)\n                count += 1\n        \n        return count\n    \n    def find_path(self, start, end, visited=None, path=None):\n        \"\"\"Find a path between two nodes\"\"\"\n        if visited is None:\n            visited = set()\n        if path is None:\n            path = []\n        \n        visited.add(start)\n        path.append(start)\n        \n        if start == end:\n            return path\n        \n        for neighbor in self.graph[start]:\n            if neighbor not in visited:\n                result = self.find_path(neighbor, end, visited, path)\n                if result:\n                    return result\n        \n        path.pop()\n        return None",
      "input": "Graph Example:\n    0 --- 1\n    |     |\n    2 --- 3\n\nDFS from 0: 0 → 1 → 3 → 2"
    }
    },
    {
    "slug": "graph-bfs",
    "title": "Graph BFS (Breadth-First Search)",
    "summary": "Learn BFS traversal, shortest paths, and bipartite graph detection",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Breadth-First Search (BFS) is a graph traversal algorithm that explores nodes level by level, visiting all neighbors at the current distance before moving to nodes at the next distance. Starting from a source vertex, BFS visits all nodes at distance 1, then all nodes at distance 2, and so on. This contrasts with DFS which goes deep first. BFS uses a queue (FIFO - First In, First Out) to maintain nodes for processing. For graph with edges [0-1, 0-2, 1-3, 2-3], BFS from 0 visits: 0 → [1,2] → [3], exploring horizontally before going deeper.",
      "BFS implementation uses a queue and visited tracking. Initialize by enqueuing the source node and marking it visited. While queue is not empty: dequeue a node, process it, then enqueue all its unvisited neighbors and mark them visited. The queue ensures nodes are processed in order of increasing distance from source. Time complexity is O(V + E) where V is vertices and E is edges - each vertex is enqueued once and each edge examined once. Space complexity is O(V) for the queue and visited array. The level-by-level nature makes BFS ideal for finding shortest paths.",
      "Shortest path in unweighted graphs is BFS's killer application. Because BFS explores nodes by distance, the first time you reach a node is guaranteed to be via the shortest path. To track paths, maintain a distance array (distance from source) and parent array (previous node in path). When processing node u and discovering unvisited neighbor v, set distance[v] = distance[u] + 1 and parent[v] = u. Reconstruct path by backtracking from destination through parents. For weighted graphs, use Dijkstra's algorithm instead. BFS finds shortest path in O(V + E) time.",
      "Bipartite graphs can be partitioned into two sets where every edge connects vertices from different sets - no two vertices in the same set are adjacent. Examples include matching problems, scheduling, and graph coloring. To check if a graph is bipartite, use BFS with two-coloring: assign source node color 0, then for each node, assign its unvisited neighbors the opposite color (1 if current is 0, vice versa). If you ever find a neighbor with the same color as current node, the graph is not bipartite (contains odd-length cycle). BFS ensures we check all connected components.",
      "BFS applications include shortest path in unweighted graphs, finding connected components, checking bipartiteness, web crawling, GPS navigation (with modifications), social network friend suggestions (friends of friends), and solving puzzles like sliding tile games. Use BFS when: (1) finding shortest path in unweighted graphs, (2) exploring nodes level by level, (3) finding nearest neighbors, or (4) the solution is close to the root. BFS is preferred over DFS for shortest paths because it explores nodes in order of distance. For very deep graphs with limited memory, DFS may be better."
    ],
    "example": {
      "language": "python",
      "code": "from collections import deque, defaultdict\n\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v, directed=False):\n        self.graph[u].append(v)\n        if not directed:\n            self.graph[v].append(u)\n    \n    def bfs(self, start):\n        \"\"\"Basic BFS traversal\"\"\"\n        visited = set()\n        queue = deque([start])\n        visited.add(start)\n        result = []\n        \n        while queue:\n            node = queue.popleft()\n            result.append(node)\n            \n            for neighbor in self.graph[node]:\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    queue.append(neighbor)\n        \n        return result\n    \n    def shortest_path(self, start, end):\n        \"\"\"Find shortest path using BFS\"\"\"\n        if start == end:\n            return [start]\n        \n        visited = set([start])\n        queue = deque([start])\n        parent = {start: None}\n        \n        while queue:\n            node = queue.popleft()\n            \n            if node == end:\n                # Reconstruct path\n                path = []\n                current = end\n                while current is not None:\n                    path.append(current)\n                    current = parent[current]\n                return path[::-1]\n            \n            for neighbor in self.graph[node]:\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    parent[neighbor] = node\n                    queue.append(neighbor)\n        \n        return None  # No path found\n    \n    def shortest_distances(self, start, num_nodes):\n        \"\"\"Find shortest distance from start to all nodes\"\"\"\n        distance = [-1] * num_nodes\n        distance[start] = 0\n        queue = deque([start])\n        \n        while queue:\n            node = queue.popleft()\n            \n            for neighbor in self.graph[node]:\n                if distance[neighbor] == -1:\n                    distance[neighbor] = distance[node] + 1\n                    queue.append(neighbor)\n        \n        return distance\n    \n    def is_bipartite(self, num_nodes):\n        \"\"\"Check if graph is bipartite using 2-coloring\"\"\"\n        color = [-1] * num_nodes\n        \n        # Check all components\n        for start in range(num_nodes):\n            if color[start] == -1:\n                queue = deque([start])\n                color[start] = 0\n                \n                while queue:\n                    node = queue.popleft()\n                    \n                    for neighbor in self.graph[node]:\n                        if color[neighbor] == -1:\n                            # Color with opposite color\n                            color[neighbor] = 1 - color[node]\n                            queue.append(neighbor)\n                        elif color[neighbor] == color[node]:\n                            # Same color = not bipartite\n                            return False\n        \n        return True",
      "input": "Graph Example:\n    0 --- 1\n    |     |\n    2 --- 3\n\nBFS from 0: 0 → 1, 2 → 3\nShortest path 0→3: 0→1→3 (length 2)"
    }
    },
    {
    "slug": "shortest-path-dijkstra",
    "title": "Shortest Path (Dijkstra's Algorithm)",
    "summary": "Find shortest paths in weighted graphs using Dijkstra's algorithm",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Dijkstra's algorithm finds the shortest path from a source vertex to all other vertices in a weighted graph with non-negative edge weights. Unlike BFS which works only for unweighted graphs (or treats all edges as weight 1), Dijkstra handles varying edge costs. The algorithm maintains a set of visited nodes and repeatedly selects the unvisited node with the smallest known distance, then updates distances to its neighbors. For graph with edges [(A,B,4), (A,C,2), (C,B,1)], the shortest path A→B is A→C→B with total weight 3, not the direct edge A→B with weight 4.",
      "The algorithm uses a priority queue (min-heap) to efficiently select the next node to process - always choosing the unvisited node with minimum distance. Initialize all distances to infinity except source (distance 0). While priority queue is not empty: extract node u with minimum distance, mark it visited, then for each neighbor v of u, if distance[u] + weight(u,v) < distance[v], update distance[v] and parent[v]. This relaxation step is key - we constantly try to find better paths. The priority queue ensures we process nodes in order of increasing distance from source.",
      "Time complexity is O((V + E) log V) with a binary heap priority queue, where V is vertices and E is edges. Each vertex is extracted once from the priority queue (O(V log V)), and each edge causes a potential distance update and priority queue operation (O(E log V)). Space complexity is O(V) for distance array, parent array, and priority queue. With a Fibonacci heap, complexity improves to O(E + V log V), but implementation is complex. For dense graphs where E ≈ V², using an array for finding minimum (O(V²)) can be faster than priority queue.",
      "Dijkstra's algorithm has critical requirements and limitations. It requires non-negative edge weights - negative weights break the greedy approach because a shorter path might be found after marking a node as visited. For graphs with negative weights, use Bellman-Ford algorithm instead. Dijkstra works on both directed and undirected graphs. To reconstruct the shortest path, maintain a parent array tracking the previous node in the optimal path. Backtrack from destination through parents to source, then reverse. The algorithm naturally handles disconnected graphs - unreachable nodes retain infinite distance.",
      "Applications include GPS navigation systems, network routing protocols (OSPF uses Dijkstra), social network analysis (degrees of separation), game AI pathfinding, and resource optimization. Dijkstra vs BFS: use BFS for unweighted graphs (faster, simpler), use Dijkstra for weighted graphs with non-negative weights. Dijkstra vs Bellman-Ford: Dijkstra is faster but requires non-negative weights; Bellman-Ford handles negative weights and detects negative cycles. Dijkstra vs A*: A* is Dijkstra with a heuristic for faster goal-directed search, commonly used in games and robotics."
    ],
    "example": {
      "language": "python",
      "code": "import heapq\nfrom collections import defaultdict\n\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v, weight, directed=False):\n        self.graph[u].append((v, weight))\n        if not directed:\n            self.graph[v].append((u, weight))\n    \n    def dijkstra(self, start, num_nodes):\n        \"\"\"Find shortest distances from start to all nodes\"\"\"\n        # Initialize distances to infinity\n        distance = [float('inf')] * num_nodes\n        distance[start] = 0\n        parent = [-1] * num_nodes\n        \n        # Priority queue: (distance, node)\n        pq = [(0, start)]\n        visited = set()\n        \n        while pq:\n            dist, node = heapq.heappop(pq)\n            \n            # Skip if already visited\n            if node in visited:\n                continue\n            \n            visited.add(node)\n            \n            # Check all neighbors\n            for neighbor, weight in self.graph[node]:\n                new_dist = distance[node] + weight\n                \n                # Relaxation: found shorter path?\n                if new_dist < distance[neighbor]:\n                    distance[neighbor] = new_dist\n                    parent[neighbor] = node\n                    heapq.heappush(pq, (new_dist, neighbor))\n        \n        return distance, parent\n    \n    def shortest_path_to(self, start, end, num_nodes):\n        \"\"\"Find shortest path from start to end\"\"\"\n        distance, parent = self.dijkstra(start, num_nodes)\n        \n        if distance[end] == float('inf'):\n            return None  # No path exists\n        \n        # Reconstruct path\n        path = []\n        current = end\n        while current != -1:\n            path.append(current)\n            current = parent[current]\n        \n        path.reverse()\n        return path, distance[end]\n\n# Example usage\ng = Graph()\ng.add_edge(0, 1, 4)\ng.add_edge(0, 2, 2)\ng.add_edge(2, 1, 1)\ng.add_edge(1, 3, 5)\ng.add_edge(2, 3, 8)\n\npath, cost = g.shortest_path_to(0, 3, 4)\nprint(f\"Path: {' -> '.join(map(str, path))}\")\nprint(f\"Cost: {cost}\")\n# Output: Path: 0 -> 2 -> 1 -> 3, Cost: 8",
      "input": "Weighted Graph:\n    0 --4-- 1\n    |  \\    |\n    2   1   5\n    |    \\  |\n    2 --8-- 3\n\nShortest 0→3: 0→2→1→3 (cost: 8)"
    }
    },
    {
    "slug": "topological-sort",
    "title": "Topological Sort",
    "summary": "Order tasks with dependencies using DFS and Kahn's algorithm",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Topological sorting produces a linear ordering of vertices in a directed acyclic graph (DAG) such that for every directed edge u→v, vertex u appears before v in the ordering. This models tasks with dependencies: if task A must be completed before task B, we draw an edge A→B, and topological sort gives a valid task execution order. For courses with prerequisites, topological sort tells you which order to take classes. Example: for edges [0→1, 0→2, 1→3, 2→3], valid orders include [0,1,2,3] or [0,2,1,3]. Topological sort only exists for DAGs - graphs with cycles have no valid ordering.",
      "Two main algorithms solve topological sort: DFS-based and Kahn's algorithm (BFS-based). DFS approach performs depth-first search and adds nodes to result after visiting all descendants, then reverses the result. When exploring a node, recursively visit all neighbors first, then add current node to a stack. The stack naturally contains nodes in reverse topological order - nodes with no outgoing edges appear first, then nodes depending on them. Time complexity is O(V + E) visiting each vertex and edge once. This approach is elegant and naturally detects cycles using recursion stack.",
      "Kahn's algorithm is BFS-based and more intuitive for dependency problems. Calculate in-degree (number of incoming edges) for each vertex. Vertices with in-degree 0 have no dependencies and can be processed first. Initialize queue with all zero in-degree vertices. While queue is not empty: dequeue vertex, add to result, then for each neighbor, decrease its in-degree by 1 (removing the processed dependency). If neighbor's in-degree becomes 0, enqueue it. If result contains all vertices, topological order exists; otherwise graph has a cycle. Time O(V + E), space O(V).",
      "Cycle detection is built into both algorithms. In DFS approach, maintain a recursion stack - if you reach a node already in the current path (gray node), a cycle exists. In Kahn's algorithm, if you process fewer than V vertices, remaining vertices are stuck in a cycle with no zero in-degree vertices. Course Schedule problems on LeetCode are classic topological sort applications: Course Schedule checks if completion is possible (cycle detection), Course Schedule II returns the actual course order (topological ordering).",
      "Applications include course scheduling with prerequisites, build systems (compile files in dependency order), package managers (install packages with dependencies), project management (task scheduling), symbol resolution in compilers, and spreadsheet formula evaluation. Choose DFS for simpler implementation when you just need an ordering. Choose Kahn's when you need to track in-degrees or process nodes level-by-level. Both detect cycles - DFS with recursion stack, Kahn's by checking if all vertices are processed. Multiple valid topological orders can exist for the same DAG."
    ],
    "example": {
      "language": "python",
      "code": "from collections import defaultdict, deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n    \n    def topological_sort_dfs(self):\n        \"\"\"DFS-based topological sort\"\"\"\n        visited = set()\n        rec_stack = set()\n        result = []\n        \n        def dfs(node):\n            if node in rec_stack:\n                return False  # Cycle detected\n            if node in visited:\n                return True\n            \n            visited.add(node)\n            rec_stack.add(node)\n            \n            for neighbor in self.graph[node]:\n                if not dfs(neighbor):\n                    return False\n            \n            rec_stack.remove(node)\n            result.append(node)  # Add after visiting all descendants\n            return True\n        \n        for node in range(self.V):\n            if node not in visited:\n                if not dfs(node):\n                    return None  # Cycle exists\n        \n        return result[::-1]  # Reverse to get correct order\n    \n    def topological_sort_kahn(self):\n        \"\"\"Kahn's algorithm (BFS-based)\"\"\"\n        # Calculate in-degrees\n        in_degree = [0] * self.V\n        for node in range(self.V):\n            for neighbor in self.graph[node]:\n                in_degree[neighbor] += 1\n        \n        # Initialize queue with zero in-degree nodes\n        queue = deque([i for i in range(self.V) if in_degree[i] == 0])\n        result = []\n        \n        while queue:\n            node = queue.popleft()\n            result.append(node)\n            \n            # Reduce in-degree for neighbors\n            for neighbor in self.graph[node]:\n                in_degree[neighbor] -= 1\n                if in_degree[neighbor] == 0:\n                    queue.append(neighbor)\n        \n        # Check if all vertices processed (no cycle)\n        if len(result) != self.V:\n            return None  # Cycle exists\n        \n        return result\n\n# Example: Course prerequisites\ng = Graph(4)\ng.add_edge(0, 1)  # Course 0 before 1\ng.add_edge(0, 2)  # Course 0 before 2\ng.add_edge(1, 3)  # Course 1 before 3\ng.add_edge(2, 3)  # Course 2 before 3\n\nprint(\"DFS:\", g.topological_sort_dfs())   # [0, 1, 2, 3] or [0, 2, 1, 3]\nprint(\"Kahn's:\", g.topological_sort_kahn()) # [0, 1, 2, 3] or [0, 2, 1, 3]",
      "input": "Course Dependencies:\n    0 → 1\n    0 → 2\n    1 → 3\n    2 → 3\n\nValid orders: [0,1,2,3] or [0,2,1,3]"
    }
  }











]
