[
  {
    "slug": "array-basics",
    "title": "Arrays: Introduction",
    "summary": "Understand arrays, indexing, and basic operations with detailed examples.",
    "level": "Beginner",
    "category": "Arrays",
    "content": [
      "An array is a collection of elements stored at contiguous memory locations. Each element can be accessed directly using its index (position).",
      "Arrays support O(1) random access, meaning you can instantly jump to any position if you know the index.",
      "Common operations: insertion at the end is O(1), insertion/deletion in the middle is O(n) because elements must shift.",
      "Example: Store exam scores [85, 92, 78, 95, 88]. Access the third score with scores[2] = 78.",
      "Key insight: Arrays are fixed-size in many languages (C/C++/Java), but Python lists and JavaScript arrays grow dynamically."
    ],
    "example": {
      "language": "python",
      "code": "# Array basics in Python\nscores = [85, 92, 78, 95, 88]\n\n# Access by index (0-based)\nprint(f\"First score: {scores[0]}\")  # 85\nprint(f\"Third score: {scores[2]}\")  # 78\n\n# Modify element\nscores[1] = 90\nprint(f\"Updated scores: {scores}\")  # [85, 90, 78, 95, 88]\n\n# Append to end (O(1) average)\nscores.append(100)\nprint(f\"After append: {scores}\")\n\n# Insert in middle (O(n))\nscores.insert(2, 88)\nprint(f\"After insert: {scores}\")\n\n# Find length\nprint(f\"Total scores: {len(scores)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "array-two-pointer",
    "title": "Two Pointer Technique",
    "summary": "Learn how to use two pointers to solve array problems efficiently.",
    "level": "Intermediate",
    "category": "Arrays",
    "content": [
      "The two-pointer technique uses two indices that traverse the array from different directions or speeds.",
      "Pattern 1 (Opposite ends): One pointer starts at index 0, another at the last index. Move them toward each other.",
      "Pattern 2 (Same direction): Both pointers start at the beginning but move at different speeds (fast and slow).",
      "Example problem: Reverse an array in-place without extra space. Use left pointer at start, right pointer at end, swap and move inward.",
      "Time complexity: O(n) with O(1) space, making it efficient for in-place solutions."
    ],
    "example": {
      "language": "python",
      "code": "# Example 1: Reverse array in-place\ndef reverse_array(arr):\n    left, right = 0, len(arr) - 1\n    while left < right:\n        # Swap elements\n        arr[left], arr[right] = arr[right], arr[left]\n        left += 1\n        right -= 1\n    return arr\n\ntest1 = [1, 2, 3, 4, 5]\nprint(f\"Original: {test1}\")\nprint(f\"Reversed: {reverse_array(test1)}\")\n\n# Example 2: Check if palindrome\ndef is_palindrome(arr):\n    left, right = 0, len(arr) - 1\n    while left < right:\n        if arr[left] != arr[right]:\n            return False\n        left += 1\n        right -= 1\n    return True\n\ntest2 = [1, 2, 3, 2, 1]\nprint(f\"\\nIs palindrome: {is_palindrome(test2)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "strings-basics",
    "title": "String Manipulation",
    "summary": "Learn string operations, reversal, palindrome checking, and common patterns.",
    "level": "Beginner",
    "category": "Strings",
    "content": [
      "Strings are sequences of characters. In Python and Java, strings are immutable (cannot be changed after creation).",
      "Common operations: length, concatenation, substring extraction, character access, case conversion.",
      "Pattern matching: Check palindromes (reads same forwards/backwards like 'racecar'), anagrams (same letters different order like 'listen' and 'silent').",
      "Example: Check if 'madam' is a palindrome - compare first and last chars, then second and second-last, etc.",
      "Tip: Two-pointer technique works great for string problems too!"
    ],
    "example": {
      "language": "python",
      "code": "# String basics\ntext = \"Hello World\"\n\n# Length and access\nprint(f\"Length: {len(text)}\")\nprint(f\"First char: {text[0]}\")\nprint(f\"Last char: {text[-1]}\")\n\n# Substring\nprint(f\"Substring [0:5]: {text[0:5]}\")  # Hello\n\n# Case conversion\nprint(f\"Upper: {text.upper()}\")\nprint(f\"Lower: {text.lower()}\")\n\n# Check palindrome\ndef is_palindrome(s):\n    # Remove spaces and convert to lowercase\n    s = s.replace(\" \", \"\").lower()\n    left, right = 0, len(s) - 1\n    while left < right:\n        if s[left] != s[right]:\n            return False\n        left += 1\n        right -= 1\n    return True\n\nprint(f\"\\n'racecar' is palindrome: {is_palindrome('racecar')}\")\nprint(f\"'hello' is palindrome: {is_palindrome('hello')}\")\n",
      "input": ""
    }
  },
  {
    "slug": "bubble-sort",
    "title": "Bubble Sort",
    "summary": "Learn the simplest sorting algorithm that repeatedly swaps adjacent elements.",
    "level": "Beginner",
    "category": "Sorting",
    "content": [
      "Bubble Sort repeatedly steps through the list, compares adjacent elements, and swaps them if they're in the wrong order.",
      "The algorithm gets its name because smaller elements 'bubble' to the top (beginning) of the list.",
      "Pass 1: Compare each pair and swap if needed. Largest element reaches the end.",
      "Pass 2: Repeat but ignore the last position (already sorted). Second largest reaches second-last position.",
      "Time Complexity: O(n²) in worst and average cases, O(n) if already sorted with optimization.",
      "Space: O(1) - sorts in place.",
      "Example: [5, 1, 4, 2, 8] → Compare 5&1 (swap) → [1, 5, 4, 2, 8] → Compare 5&4 (swap) → continue..."
    ],
    "example": {
      "language": "python",
      "code": "# Bubble Sort Implementation\ndef bubble_sort(arr):\n    n = len(arr)\n    \n    # Traverse through all array elements\n    for i in range(n):\n        # Track if any swap happened\n        swapped = False\n        \n        # Last i elements are already sorted\n        for j in range(0, n - i - 1):\n            # Swap if current element > next element\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n                swapped = True\n                print(f\"Swapped {arr[j+1]} and {arr[j]}: {arr}\")\n        \n        # If no swaps, array is sorted\n        if not swapped:\n            break\n    \n    return arr\n\n# Example\ntest = [64, 34, 25, 12, 22, 11, 90]\nprint(f\"Original: {test}\")\nprint(\"\\nSorting process:\")\nresult = bubble_sort(test.copy())\nprint(f\"\\nSorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "selection-sort",
    "title": "Selection Sort",
    "summary": "Find the minimum element and place it at the beginning repeatedly.",
    "level": "Beginner",
    "category": "Sorting",
    "content": [
      "Selection Sort divides the array into sorted and unsorted parts. It repeatedly selects the smallest element from the unsorted part and moves it to the sorted part.",
      "Step 1: Find the minimum element in the entire array and swap it with the first element.",
      "Step 2: Find the minimum in the remaining unsorted array and swap with the second position.",
      "Repeat until the entire array is sorted.",
      "Time Complexity: O(n²) in all cases - always makes n² comparisons.",
      "Space: O(1) - in-place sorting.",
      "Example: [29, 10, 14, 37] → Find min=10, swap with 29 → [10, 29, 14, 37] → Find min=14, swap with 29 → [10, 14, 29, 37]"
    ],
    "example": {
      "language": "python",
      "code": "# Selection Sort Implementation\ndef selection_sort(arr):\n    n = len(arr)\n    \n    # Traverse through all array elements\n    for i in range(n):\n        # Find minimum element in remaining unsorted array\n        min_idx = i\n        for j in range(i + 1, n):\n            if arr[j] < arr[min_idx]:\n                min_idx = j\n        \n        # Swap the found minimum with first element\n        if min_idx != i:\n            arr[i], arr[min_idx] = arr[min_idx], arr[i]\n            print(f\"Swapped {arr[i]} with {arr[min_idx]} (position {i}): {arr}\")\n    \n    return arr\n\n# Example\ntest = [64, 25, 12, 22, 11]\nprint(f\"Original: {test}\")\nprint(\"\\nSorting process:\")\nresult = selection_sort(test.copy())\nprint(f\"\\nSorted: {result}\")\n\n# Selection sort makes fewer swaps than bubble sort\nprint(\"\\nNote: Selection sort minimizes the number of swaps!\")\n",
      "input": ""
    }
  },
  {
    "slug": "insertion-sort",
    "title": "Insertion Sort",
    "summary": "Build the sorted array one element at a time by inserting elements in correct position.",
    "level": "Beginner",
    "category": "Sorting",
    "content": [
      "Insertion Sort builds the final sorted array one item at a time. It's similar to how you sort playing cards in your hands.",
      "Start with the second element. Compare it with elements in the sorted part (left side) and insert it at the correct position.",
      "Shift all larger elements one position to the right to make space for the inserted element.",
      "Example: Like arranging cards - pick a card, compare with cards in hand, insert at right spot.",
      "Time Complexity: O(n²) worst case, O(n) best case (already sorted), good for small datasets.",
      "Space: O(1) - in-place sorting.",
      "Practical use: Efficient for small arrays or nearly sorted arrays."
    ],
    "example": {
      "language": "python",
      "code": "# Insertion Sort Implementation\ndef insertion_sort(arr):\n    n = len(arr)\n    \n    # Start from second element\n    for i in range(1, n):\n        key = arr[i]  # Element to be inserted\n        j = i - 1\n        \n        print(f\"\\nInserting {key}:\")\n        \n        # Move elements greater than key one position ahead\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n            print(f\"  Shifted: {arr}\")\n        \n        # Insert key at correct position\n        arr[j + 1] = key\n        print(f\"  Final: {arr}\")\n    \n    return arr\n\n# Example\ntest = [12, 11, 13, 5, 6]\nprint(f\"Original: {test}\")\nresult = insertion_sort(test.copy())\nprint(f\"\\nSorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "merge-sort",
    "title": "Merge Sort",
    "summary": "Efficient divide-and-conquer algorithm that splits, sorts, and merges arrays.",
    "level": "Intermediate",
    "category": "Sorting",
    "content": [
      "Merge Sort uses divide-and-conquer strategy: divide array into halves, sort each half, then merge them.",
      "Step 1 (Divide): Split array into two halves recursively until each subarray has one element.",
      "Step 2 (Conquer): Merge the subarrays back together in sorted order.",
      "Merging: Compare first elements of both subarrays, pick smaller one, repeat until all elements are merged.",
      "Time Complexity: O(n log n) in all cases - much better than O(n²) algorithms!",
      "Space: O(n) - needs extra space for merging.",
      "Example: [38,27,43,3] → [38,27] & [43,3] → [38],[27] & [43],[3] → merge → [27,38] & [3,43] → [3,27,38,43]"
    ],
    "example": {
      "language": "python",
      "code": "# Merge Sort Implementation\ndef merge_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    # Divide array into two halves\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n    \n    print(f\"Splitting: {arr} → {left} | {right}\")\n    \n    # Recursively sort both halves\n    left = merge_sort(left)\n    right = merge_sort(right)\n    \n    # Merge sorted halves\n    return merge(left, right)\n\ndef merge(left, right):\n    result = []\n    i = j = 0\n    \n    # Compare and merge\n    while i < len(left) and j < len(right):\n        if left[i] <= right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    # Add remaining elements\n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    print(f\"Merging {left} + {right} = {result}\")\n    return result\n\n# Example\ntest = [38, 27, 43, 3, 9, 82, 10]\nprint(f\"Original: {test}\\n\")\nresult = merge_sort(test.copy())\nprint(f\"\\nFinal Sorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "quick-sort",
    "title": "Quick Sort",
    "summary": "Fast divide-and-conquer algorithm using pivot partitioning.",
    "level": "Intermediate",
    "category": "Sorting",
    "content": [
      "Quick Sort picks a 'pivot' element and partitions the array around it: smaller elements go left, larger go right.",
      "Step 1: Choose a pivot (commonly last element, first element, or random).",
      "Step 2: Partition array so elements < pivot are on left, elements > pivot are on right.",
      "Step 3: Recursively apply quick sort to left and right partitions.",
      "Time Complexity: O(n log n) average case, O(n²) worst case (rare with good pivot selection).",
      "Space: O(log n) for recursion stack.",
      "Example: [10,7,8,9,1,5] pivot=5 → [1] 5 [10,7,8,9] → recursively sort both sides."
    ],
    "example": {
      "language": "python",
      "code": "# Quick Sort Implementation\ndef quick_sort(arr, low=0, high=None):\n    if high is None:\n        high = len(arr) - 1\n    \n    if low < high:\n        # Partition and get pivot index\n        pi = partition(arr, low, high)\n        \n        # Recursively sort elements before and after partition\n        quick_sort(arr, low, pi - 1)\n        quick_sort(arr, pi + 1, high)\n    \n    return arr\n\ndef partition(arr, low, high):\n    # Choose rightmost element as pivot\n    pivot = arr[high]\n    i = low - 1  # Index of smaller element\n    \n    print(f\"\\nPartitioning {arr[low:high+1]} with pivot={pivot}\")\n    \n    for j in range(low, high):\n        # If current element <= pivot\n        if arr[j] <= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    \n    # Place pivot in correct position\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    print(f\"After partition: {arr}\")\n    \n    return i + 1\n\n# Example\ntest = [10, 7, 8, 9, 1, 5]\nprint(f\"Original: {test}\")\nresult = quick_sort(test.copy())\nprint(f\"\\nFinal Sorted: {result}\")\n",
      "input": ""
    }
  },
    {
    "slug": "binary-search-basics",
    "title": "Binary Search: Introduction",
    "summary": "Learn the fundamental technique to search in sorted arrays with O(log n) time complexity.",
    "level": "Beginner",
    "category": "Binary Search",
    "content": [
      "Binary Search is a highly efficient algorithm for finding a target value in a sorted array. Unlike linear search which checks each element one by one taking O(n) time, binary search repeatedly divides the search space in half, achieving O(log n) time complexity. This means searching in an array of 1 million elements takes only about 20 comparisons instead of potentially 1 million. The key requirement is that the array must be sorted beforehand.",
      "The algorithm works by maintaining three pointers: left, right, and mid. Initially, left points to the first index and right to the last. In each iteration, you calculate mid as the middle index between left and right, then compare the element at mid with your target. If they match, you found the answer. If the target is smaller, you know it must be in the left half, so you move right to mid - 1. If the target is larger, it must be in the right half, so you move left to mid + 1. This process continues until you find the target or the search space becomes empty.",
      "A common pitfall is calculating the middle index. Using mid = (left + right) / 2 can cause integer overflow when left and right are large numbers. The safer formula is mid = left + (right - left) / 2, which avoids overflow. Another critical detail is the loop condition: while (left <= right) ensures you don't miss the case where the target is at the exact position where left and right meet. If you use while (left < right), you might exit one iteration too early.",
      "Binary search returns different values based on the problem. For finding exact matches, return the index when found or -1 if not found. For finding insertion positions (where to insert a target in sorted order), return left after the loop ends. For finding first/last occurrence of duplicates, you modify the algorithm to continue searching even after finding a match. These variations make binary search versatile for many problems beyond simple searching.",
      "Real-world applications include searching in databases (indexed columns use binary search trees), dictionaries and phone books (alphabetically sorted), version control systems (finding when a bug was introduced by searching through commits), and even optimizing machine learning hyperparameters (searching for the best learning rate). Any time you have sorted data and need fast lookup, binary search is the go-to technique.",
      "To master binary search, practice on arrays with odd and even lengths, arrays with duplicate elements, and edge cases like single-element arrays or searching for values outside the array bounds. Visualizing the search space shrinking helps build intuition. Always verify your loop invariants: what does left represent? What does right represent? Are they inclusive or exclusive bounds? Clear thinking about these prevents off-by-one errors."
    ],
    "example": {
      "language": "python",
      "code": "def binary_search(arr: list[int], target: int) -> int:\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = left + (right - left) // 2\n        \n        if arr[mid] == target:\n            return mid  # Found target\n        elif arr[mid] < target:\n            left = mid + 1  # Search right half\n        else:\n            right = mid - 1  # Search left half\n    \n    return -1  # Target not found\n\n# Examples\narr = [1, 3, 5, 7, 9, 11, 13, 15]\nprint(binary_search(arr, 7))   # 3\nprint(binary_search(arr, 6))   # -1\nprint(binary_search(arr, 1))   # 0\nprint(binary_search(arr, 15))  # 7",
      "input": "arr = [1, 3, 5, 7, 9, 11, 13, 15], target = 7"
    }
  },

  {
    "slug": "linked-list-basics",
    "title": "Linked Lists: Introduction",
    "summary": "Understand nodes, pointers, and basic linked list operations.",
    "level": "Beginner",
    "category": "Linked Lists",
    "content": [
      "A linked list is a linear data structure where each element (node) contains data and a reference (pointer) to the next node.",
      "Unlike arrays, linked lists don't need contiguous memory. Nodes can be scattered in memory.",
      "Advantages: Dynamic size, efficient insertions/deletions at O(1) if you have the pointer.",
      "Disadvantages: No random access (must traverse from head), extra memory for pointers.",
      "Types: Singly linked list (next pointer only), doubly linked list (next + previous pointers), circular linked list (last node points to first).",
      "Example: Think of a treasure hunt - each clue (node) points to the next location."
    ],
    "example": {
      "language": "python",
      "code": "# Linked List Implementation\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n    \n    def append(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n            return\n        \n        current = self.head\n        while current.next:\n            current = current.next\n        current.next = new_node\n    \n    def display(self):\n        elements = []\n        current = self.head\n        while current:\n            elements.append(str(current.data))\n            current = current.next\n        return \" -> \".join(elements) + \" -> None\"\n\n# Example usage\nll = LinkedList()\nll.append(1)\nll.append(2)\nll.append(3)\nll.append(4)\n\nprint(\"Linked List:\")\nprint(ll.display())\n\nprint(\"\\nEach node points to the next, forming a chain!\")\n",
      "input": ""
    }
  },
  {
    "slug": "stack-basics",
    "title": "Stacks: LIFO Structure",
    "summary": "Master stack operations and solve parentheses matching problems.",
    "level": "Beginner",
    "category": "Stacks",
    "content": [
      "A stack follows Last In First Out (LIFO): the last element added is the first to be removed.",
      "Think of a stack of plates - you can only add or remove from the top.",
      "Core operations: push (add to top), pop (remove from top), peek (view top without removing), isEmpty.",
      "Use cases: Function call stack, undo/redo mechanisms, browser back button, expression evaluation, backtracking algorithms.",
      "Common problem: Check if parentheses are balanced - '(())' is valid, '(()' is not.",
      "Implementation: Can use arrays (Python list) or linked lists."
    ],
    "example": {
      "language": "python",
      "code": "# Stack Implementation\nclass Stack:\n    def __init__(self):\n        self.items = []\n    \n    def push(self, item):\n        self.items.append(item)\n        print(f\"Pushed {item}: {self.items}\")\n    \n    def pop(self):\n        if not self.is_empty():\n            item = self.items.pop()\n            print(f\"Popped {item}: {self.items}\")\n            return item\n        return None\n    \n    def peek(self):\n        return self.items[-1] if not self.is_empty() else None\n    \n    def is_empty(self):\n        return len(self.items) == 0\n\n# Example: Valid Parentheses\ndef is_valid_parentheses(s):\n    stack = []\n    mapping = {')': '(', '}': '{', ']': '['}\n    \n    for char in s:\n        if char in mapping:\n            top = stack.pop() if stack else '#'\n            if mapping[char] != top:\n                return False\n        else:\n            stack.append(char)\n    \n    return not stack\n\nprint(\"Stack Demo:\")\ns = Stack()\ns.push(10)\ns.push(20)\ns.push(30)\ns.pop()\n\nprint(\"\\nValid Parentheses Check:\")\nprint(f\"'()[]{{}}' is valid: {is_valid_parentheses('()[]{}')}\")\nprint(f\"'(]' is valid: {is_valid_parentheses('(]')}\")\n",
      "input": ""
    }
  },
  {
    "slug": "queue-basics",
    "title": "Queues: FIFO Structure",
    "summary": "Learn queue operations and understand First In First Out behavior.",
    "level": "Beginner",
    "category": "Queues",
    "content": [
      "A queue follows First In First Out (FIFO): the first element added is the first to be removed.",
      "Think of a line at a ticket counter - first person in line gets served first.",
      "Core operations: enqueue (add to rear), dequeue (remove from front), peek (view front), isEmpty.",
      "Use cases: Task scheduling, breadth-first search (BFS), printer queue, handling requests in web servers.",
      "Types: Simple queue, circular queue (wraps around), priority queue (elements have priorities), deque (double-ended queue).",
      "Implementation: Can use arrays with two pointers (front and rear) or linked lists."
    ],
    "example": {
      "language": "python",
      "code": "# Queue Implementation\nfrom collections import deque\n\nclass Queue:\n    def __init__(self):\n        self.items = deque()\n    \n    def enqueue(self, item):\n        self.items.append(item)\n        print(f\"Enqueued {item}: {list(self.items)}\")\n    \n    def dequeue(self):\n        if not self.is_empty():\n            item = self.items.popleft()\n            print(f\"Dequeued {item}: {list(self.items)}\")\n            return item\n        return None\n    \n    def peek(self):\n        return self.items[0] if not self.is_empty() else None\n    \n    def is_empty(self):\n        return len(self.items) == 0\n    \n    def size(self):\n        return len(self.items)\n\n# Example usage\nprint(\"Queue Demo:\")\nq = Queue()\nq.enqueue(\"Customer 1\")\nq.enqueue(\"Customer 2\")\nq.enqueue(\"Customer 3\")\n\nprint(\"\\nServing customers:\")\nq.dequeue()  # Customer 1 served first\nq.dequeue()  # Customer 2 served next\n\nprint(\"\\nNext in line:\")\nprint(f\"Front: {q.peek()}\")\n",
      "input": ""
    }
  },
  {
    "slug": "hashmap-basics",
    "title": "Hash Maps: Key-Value Storage",
    "summary": "Master hash tables for O(1) lookups, counting, and frequency problems.",
    "level": "Beginner",
    "category": "Hash Maps",
    "content": [
      "A hash map (or dictionary) stores key-value pairs and allows O(1) average-time insertion, deletion, and lookup.",
      "How it works: A hash function converts keys into array indices. Values are stored at those indices.",
      "Example: Store student grades - {'Alice': 95, 'Bob': 87} - access Alice's grade instantly.",
      "Use cases: Counting frequencies, finding duplicates, caching results, implementing sets.",
      "Common problems: Two Sum (find pairs that sum to target), anagram detection, frequency counter.",
      "Collision handling: When two keys hash to same index, use chaining (linked lists) or open addressing."
    ],
    "example": {
      "language": "python",
      "code": "# Hash Map Basics\n\n# Example 1: Count character frequency\ndef count_chars(s):\n    freq = {}\n    for char in s:\n        freq[char] = freq.get(char, 0) + 1\n    return freq\n\ntext = \"hello world\"\nprint(f\"Character frequencies in '{text}':\")\nprint(count_chars(text))\n\n# Example 2: Find first non-repeating character\ndef first_unique_char(s):\n    freq = count_chars(s)\n    for i, char in enumerate(s):\n        if freq[char] == 1:\n            return i\n    return -1\n\ntest = \"leetcode\"\nprint(f\"\\nFirst unique char in '{test}':\")\nindex = first_unique_char(test)\nprint(f\"Index: {index}, Character: '{test[index]}'\")\n\n# Example 3: Two Sum using hash map\ndef two_sum(nums, target):\n    seen = {}\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:\n            return [seen[complement], i]\n        seen[num] = i\n    return []\n\nnums = [2, 7, 11, 15]\ntarget = 9\nprint(f\"\\nTwo Sum: {nums}, target={target}\")\nprint(f\"Indices: {two_sum(nums, target)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "heap-basics",
    "title": "Heaps: Priority Queue",
    "summary": "Understand min-heap and max-heap for efficient priority-based operations.",
    "level": "Intermediate",
    "category": "Heaps",
    "content": [
      "A heap is a complete binary tree where each parent node has a specific relationship with its children.",
      "Max-Heap: Parent node is always greater than or equal to children. Root is the maximum element.",
      "Min-Heap: Parent node is always less than or equal to children. Root is the minimum element.",
      "Core operations: insert O(log n), extract-min/max O(log n), peek O(1).",
      "Use cases: Priority queues, heap sort, finding kth largest/smallest element, median maintenance.",
      "Array representation: For node at index i, left child at 2i+1, right child at 2i+2, parent at (i-1)/2.",
      "Example: Hospital emergency room - critical patients (high priority) treated before minor cases."
    ],
    "example": {
      "language": "python",
      "code": "# Min-Heap Implementation using Python's heapq\nimport heapq\n\nclass MinHeap:\n    def __init__(self):\n        self.heap = []\n    \n    def push(self, val):\n        heapq.heappush(self.heap, val)\n        print(f\"Inserted {val}: {self.heap}\")\n    \n    def pop(self):\n        if self.heap:\n            val = heapq.heappop(self.heap)\n            print(f\"Extracted min {val}: {self.heap}\")\n            return val\n        return None\n    \n    def peek(self):\n        return self.heap[0] if self.heap else None\n\n# Example: Priority Queue\nprint(\"Min-Heap Demo:\")\nheap = MinHeap()\n\n# Insert elements\nheap.push(10)\nheap.push(5)\nheap.push(20)\nheap.push(1)\n\nprint(\"\\nExtracting minimum elements:\")\nheap.pop()  # Removes 1\nheap.pop()  # Removes 5\n\nprint(f\"\\nCurrent minimum: {heap.peek()}\")\n\n# Example: Find Kth largest\ndef find_kth_largest(nums, k):\n    heap = []\n    for num in nums:\n        heapq.heappush(heap, num)\n        if len(heap) > k:\n            heapq.heappop(heap)\n    return heap[0]\n\nnums = [3, 2, 1, 5, 6, 4]\nk = 2\nprint(f\"\\nFind {k}th largest in {nums}: {find_kth_largest(nums, k)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "array-sliding-window",
    "title": "Sliding Window",
    "summary": "Master the sliding window pattern for subarray problems.",
    "level": "Intermediate",
    "category": "Arrays",
    "content": [
      "Sliding window maintains a window (subarray) and slides it across the array to solve problems efficiently.",
      "Fixed-size window: size k is constant (e.g., max sum of k consecutive elements).",
      "Variable-size window: size changes based on a condition (e.g., longest substring with at most k distinct characters).",
      "Reduces brute-force O(n²) or O(n³) solutions to O(n) by reusing calculations from the previous window."
    ],
    "example": {
      "language": "python",
      "code": "# Fixed sliding window: max sum of k consecutive elements\ndef max_sum_k(arr, k):\n    n = len(arr)\n    if n < k:\n        return -1\n    \n    # Compute sum of first window\n    window_sum = sum(arr[:k])\n    max_sum = window_sum\n    \n    # Slide the window\n    for i in range(n - k):\n        window_sum = window_sum - arr[i] + arr[i + k]\n        max_sum = max(max_sum, window_sum)\n    \n    return max_sum\n\narr = [1, 4, 2, 10, 23, 3, 1, 0, 20]\nk = 4\nprint(max_sum_k(arr, k))  # 39 (10+23+3+1)\n",
      "input": ""
    }
  },
  {
    "slug": "binary-trees-intro",
    "title": "Binary Trees Introduction",
    "summary": "Understanding hierarchical data structures with nodes and children",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "A binary tree is a hierarchical data structure where each node contains a value and can have at most two children: a left child and a right child. Unlike linear structures like arrays or linked lists, trees represent parent-child relationships, making them ideal for hierarchical data like file systems, organizational charts, or expression parsing.",
      "Each binary tree starts with a root node (the topmost node). Nodes with no children are called leaf nodes, while nodes with at least one child are internal nodes. The height of a tree is the longest path from root to any leaf, and the depth of a node is its distance from the root. A tree with only one node has height 0.",
      "Binary trees have several important properties: maximum nodes at level i is 2^i, and a tree of height h can have at most 2^(h+1) - 1 nodes. A perfect binary tree has all levels completely filled. A complete binary tree fills levels left-to-right, with all levels full except possibly the last. A balanced binary tree maintains height of O(log n), ensuring efficient operations.",
      "Common operations include insertion (adding nodes), deletion (removing nodes while maintaining structure), and searching (finding specific values). Traversal methods let us visit all nodes systematically - we'll explore depth-first (preorder, inorder, postorder) and breadth-first (level-order) approaches in upcoming topics.",
      "Time complexity for basic operations depends on tree structure. In a balanced tree, search, insertion, and deletion take O(log n). However, in a skewed tree (resembling a linked list), these degrade to O(n). Space complexity for storing n nodes is always O(n), while recursion depth can reach O(h) where h is tree height."
    ],
    "example": {
      "language": "python",
      "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef count_nodes(root):\n    \"\"\"Count total nodes in binary tree\"\"\"\n    if root is None:\n        return 0\n    return 1 + count_nodes(root.left) + count_nodes(root.right)\n\ndef max_depth(root):\n    \"\"\"Find height of binary tree\"\"\"\n    if root is None:\n        return 0\n    return 1 + max(max_depth(root.left), max_depth(root.right))\n\n# Example usage:\nroot = TreeNode(1)\nroot.left = TreeNode(2)\nroot.right = TreeNode(3)\nroot.left.left = TreeNode(4)\nroot.left.right = TreeNode(5)\n\nprint(f\"Total nodes: {count_nodes(root)}\")  # Output: 5\nprint(f\"Tree height: {max_depth(root)}\")    # Output: 3",
      "input": "Binary Tree:\n       1\n      / \\\n     2   3\n    / \\\n   4   5"
    }
  },
  {
  "slug": "tree-traversals-dfs",
  "title": "Tree Traversals (DFS)",
  "summary": "Master depth-first traversals: inorder, preorder, and postorder",
  "level": "Intermediate",
  "category": "Trees",
  "content": [
    "Depth-First Search (DFS) traversals explore a tree by diving as deep as possible into each branch before backtracking. Unlike breadth-first search which explores level-by-level, DFS uses recursion or a stack to visit nodes in three distinct orders: inorder, preorder, and postorder. Each traversal has specific use cases and produces different node visitation sequences.",
    "Preorder traversal (Root → Left → Right) visits the current node first, then recursively traverses left and right subtrees. This is useful for creating a copy of the tree, serializing tree structure, or prefix notation expressions. For tree [1,2,3,4,5], preorder gives: 1, 2, 4, 5, 3.",
    "Inorder traversal (Left → Root → Right) visits the left subtree first, then the current node, then the right subtree. For Binary Search Trees, inorder traversal produces values in sorted ascending order, making it essential for BST validation and sorted output. For tree [1,2,3,4,5], inorder gives: 4, 2, 5, 1, 3.",
    "Postorder traversal (Left → Right → Root) visits both subtrees before the current node. This is ideal for deleting trees (leaves first), calculating directory sizes, or postfix notation expressions. For tree [1,2,3,4,5], postorder gives: 4, 5, 2, 3, 1. The parent is always processed after its children.",
    "All three DFS traversals have O(n) time complexity visiting each node once, and O(h) space complexity for recursion stack where h is tree height. In balanced trees, space is O(log n), but in skewed trees it becomes O(n). Iterative implementations using explicit stacks can help manage stack overflow for very deep trees."
  ],
  "example": {
    "language": "python",
    "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef inorder_traversal(root, result=[]):\n    \"\"\"Left -> Root -> Right (sorted for BST)\"\"\"\n    if root:\n        inorder_traversal(root.left, result)\n        result.append(root.val)  # Process node\n        inorder_traversal(root.right, result)\n    return result\n\ndef preorder_traversal(root, result=[]):\n    \"\"\"Root -> Left -> Right (copy tree structure)\"\"\"\n    if root:\n        result.append(root.val)  # Process node first\n        preorder_traversal(root.left, result)\n        preorder_traversal(root.right, result)\n    return result\n\ndef postorder_traversal(root, result=[]):\n    \"\"\"Left -> Right -> Root (delete tree, bottom-up)\"\"\"\n    if root:\n        postorder_traversal(root.left, result)\n        postorder_traversal(root.right, result)\n        result.append(root.val)  # Process node last\n    return result\n\n# Example usage:\nroot = TreeNode(1)\nroot.left = TreeNode(2)\nroot.right = TreeNode(3)\nroot.left.left = TreeNode(4)\nroot.left.right = TreeNode(5)\n\nprint(f\"Inorder: {inorder_traversal(root, [])}\")    # [4,2,5,1,3]\nprint(f\"Preorder: {preorder_traversal(root, [])}\")  # [1,2,4,5,3]\nprint(f\"Postorder: {postorder_traversal(root, [])}]\") # [4,5,2,3,1]",
    "input": "Binary Tree:\n       1\n      / \\\n     2   3\n    / \\\n   4   5"
  }
  },
  {
    "slug": "binary-search-trees",
    "title": "Binary Search Trees (BST)",
    "summary": "Learn BST properties and operations: search, insert, and delete",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "A Binary Search Tree (BST) is a specialized binary tree where each node follows a strict ordering property: all values in the left subtree are smaller than the node's value, and all values in the right subtree are greater. This ordering enables efficient searching, insertion, and deletion operations with O(log n) time complexity in balanced trees. For example, in a BST with root 8, left subtree might contain [3,1,6] and right subtree [10,14].",
      "Searching in a BST leverages the ordering property by comparing the target value with the current node. If the target is smaller, search the left subtree; if larger, search the right. This binary decision at each node eliminates half the remaining nodes, similar to binary search on sorted arrays. A search for value 6 in tree [8,3,10,1,6,14] would go: 8→3→6 (found), requiring only 3 comparisons instead of checking all nodes.",
      "Insertion maintains BST property by finding the correct leaf position. Start at root, compare with target: go left if smaller, right if larger. When reaching null, insert the new node there. Inserting 7 into [8,3,10,1,6,14] follows path 8→3→6→right, creating new node. Duplicate values are typically handled by storing frequency counts or rejecting insertion based on requirements.",
      "Deletion is the most complex BST operation with three cases: (1) Deleting a leaf node - simply remove it. (2) Node with one child - replace it with its child. (3) Node with two children - replace with inorder successor (smallest node in right subtree) or inorder predecessor (largest in left subtree), then delete that successor/predecessor node. This maintains BST property throughout the tree.",
      "BST validation requires checking that for every node, all left descendants are smaller and all right descendants are larger. A common mistake is only checking immediate children. Use inorder traversal (which produces sorted output in valid BSTs) or recursive validation with min-max bounds. Time complexity: search, insert, delete are O(h) where h is height - O(log n) for balanced trees but O(n) for skewed trees, highlighting the importance of self-balancing variants like AVL or Red-Black trees."
    ],
    "example": {
      "language": "python",
      "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef search_bst(root, target):\n    \"\"\"Search for target value in BST\"\"\"\n    if not root or root.val == target:\n        return root\n    if target < root.val:\n        return search_bst(root.left, target)\n    return search_bst(root.right, target)\n\ndef insert_bst(root, val):\n    \"\"\"Insert new value into BST\"\"\"\n    if not root:\n        return TreeNode(val)\n    if val < root.val:\n        root.left = insert_bst(root.left, val)\n    elif val > root.val:\n        root.right = insert_bst(root.right, val)\n    return root\n\ndef delete_bst(root, key):\n    \"\"\"Delete node with given key from BST\"\"\"\n    if not root:\n        return None\n    \n    if key < root.val:\n        root.left = delete_bst(root.left, key)\n    elif key > root.val:\n        root.right = delete_bst(root.right, key)\n    else:\n        # Node with one child or no child\n        if not root.left:\n            return root.right\n        if not root.right:\n            return root.left\n        \n        # Node with two children: get inorder successor\n        min_node = root.right\n        while min_node.left:\n            min_node = min_node.left\n        root.val = min_node.val\n        root.right = delete_bst(root.right, min_node.val)\n    \n    return root\n\ndef validate_bst(root, min_val=float('-inf'), max_val=float('inf')):\n    \"\"\"Validate if tree is valid BST\"\"\"\n    if not root:\n        return True\n    if root.val <= min_val or root.val >= max_val:\n        return False\n    return (validate_bst(root.left, min_val, root.val) and\n            validate_bst(root.right, root.val, max_val))",
      "input": "BST Example:\n       8\n      / \\\n     3   10\n    / \\    \\\n   1   6   14\n      / \\\n     4   7"
    }
    },
    {
    "slug": "level-order-traversal",
    "title": "Level Order Traversal (BFS)",
    "summary": "Explore trees level-by-level using breadth-first search",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "Level Order Traversal, also known as Breadth-First Search (BFS) for trees, visits all nodes at each depth level before moving to the next level. Unlike DFS which explores deep into branches, BFS explores horizontally across each level from left to right. For tree [3,9,20,null,null,15,7], level order produces: [[3], [9,20], [15,7]]. This traversal is essential for finding shortest paths, level-based operations, and hierarchical processing.",
      "The BFS algorithm uses a queue (FIFO - First In, First Out) to maintain nodes for processing. Start by enqueuing the root, then repeatedly dequeue a node, process it, and enqueue its children left-to-right. The queue naturally maintains level order because all nodes at depth d are enqueued before any node at depth d+1. This guarantees we process nodes in correct breadth-first order without recursion.",
      "Level-by-level processing tracks nodes at each depth separately. Use a nested loop: outer loop runs while queue is not empty, inner loop processes all nodes at current level (queue size at that moment). This technique enables level-specific operations like finding average of each level, maximum value per level, or creating separate arrays for each depth. The queue size before processing a level tells you exactly how many nodes are at that level.",
      "Variations include Zigzag Level Order (alternating left-to-right and right-to-left directions), Right Side View (only rightmost node per level), and Bottom-Up Level Order (reverse the result). Zigzag uses a flag to reverse alternate levels. Right Side View takes the last element when processing each level. These variations build on the standard BFS template, modifying only how results are collected or ordered.",
      "Time complexity is O(n) as we visit each node exactly once. Space complexity is O(w) where w is maximum width of tree - the queue can hold at most all nodes at the widest level. For complete binary trees, this is O(n/2) = O(n) at the last level. For skewed trees, width is O(1). BFS is ideal when you need shortest path in unweighted trees, level-wise grouping, or finding nodes at specific distances from root."
    ],
    "example": {
      "language": "python",
      "code": "from collections import deque\n\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef level_order_traversal(root):\n    \"\"\"Standard BFS - returns nodes grouped by level\"\"\"\n    if not root:\n        return []\n    \n    result = []\n    queue = deque([root])\n    \n    while queue:\n        level_size = len(queue)  # Nodes at current level\n        current_level = []\n        \n        for _ in range(level_size):\n            node = queue.popleft()\n            current_level.append(node.val)\n            \n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n        \n        result.append(current_level)\n    \n    return result\n\ndef zigzag_traversal(root):\n    \"\"\"Zigzag BFS - alternate left-to-right direction\"\"\"\n    if not root:\n        return []\n    \n    result = []\n    queue = deque([root])\n    left_to_right = True\n    \n    while queue:\n        level_size = len(queue)\n        current_level = deque()\n        \n        for _ in range(level_size):\n            node = queue.popleft()\n            \n            # Add to level based on direction\n            if left_to_right:\n                current_level.append(node.val)\n            else:\n                current_level.appendleft(node.val)\n            \n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n        \n        result.append(list(current_level))\n        left_to_right = not left_to_right\n    \n    return result\n\ndef right_side_view(root):\n    \"\"\"Get rightmost node at each level\"\"\"\n    if not root:\n        return []\n    \n    result = []\n    queue = deque([root])\n    \n    while queue:\n        level_size = len(queue)\n        \n        for i in range(level_size):\n            node = queue.popleft()\n            \n            # Last node in level is rightmost\n            if i == level_size - 1:\n                result.append(node.val)\n            \n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n    \n    return result",
      "input": "Binary Tree:\n       3\n      / \\\n     9  20\n       /  \\\n      15   7\n\nOutput:\n[[3], [9,20], [15,7]]"
    }
    },
    {
    "slug": "tree-properties-paths",
    "title": "Tree Properties & Paths",
    "summary": "Advanced tree algorithms: path sums, LCA, diameter, and construction",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "Tree path problems involve finding or calculating values along paths between nodes. A path is a sequence of connected nodes from one node to another, most commonly from root to leaf. Path Sum problems ask if a root-to-leaf path exists with a specific sum by subtracting each node's value from the target while traversing. For tree [5,4,8,11,null,13,4,7,2], checking path sum 22 would traverse 5→4→11→2, accumulating values to verify the sum matches.",
      "The Diameter of a binary tree measures the longest path between any two nodes, counted as number of edges. Crucially, this path doesn't need to pass through the root. For each node, the potential diameter through it equals the sum of heights of its left and right subtrees. Calculate this for all nodes and return the maximum. For tree [1,2,3,4,5], diameter is 3 (path 4→2→1→3 or 5→2→1→3), even though it passes through root.",
      "Lowest Common Ancestor (LCA) finds the deepest node that is an ancestor of both given nodes. Two approaches exist: (1) Store parent pointers and trace paths from both nodes upward until they meet, or (2) Use recursion - if current node is one of the targets or both targets are found in different subtrees, current node is LCA. For tree [3,5,1,6,2,0,8], LCA(5,1) = 3 (root), but LCA(5,2) = 5 (ancestor can be one of the nodes itself).",
      "Tree construction from traversals recreates the original tree structure given specific orderings. Given preorder and inorder, preorder's first element is root; find it in inorder to split left/right subtrees, then recursively build each subtree. Preorder gives roots in order visited; inorder splits subtrees. Given postorder and inorder, postorder's last element is root. You cannot uniquely construct a tree from preorder and postorder alone without additional information.",
      "Sum of Left Leaves adds only values of leaves that are left children of their parents. Use recursion checking if current node's left child exists and is a leaf (no children). Path problems often use DFS with backtracking, maintaining current path sum or collecting path nodes. Time complexity is typically O(n) visiting each node once. Space complexity is O(h) for recursion stack where h is height. Many path problems use global variables to track maximum values or counts across recursive calls."
    ],
    "example": {
      "language": "python",
      "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef has_path_sum(root, target_sum):\n    \"\"\"Check if root-to-leaf path exists with given sum\"\"\"\n    if not root:\n        return False\n    \n    # Leaf node - check if sum matches\n    if not root.left and not root.right:\n        return target_sum == root.val\n    \n    # Recursively check left and right subtrees\n    remaining = target_sum - root.val\n    return (has_path_sum(root.left, remaining) or \n            has_path_sum(root.right, remaining))\n\ndef diameter_of_tree(root):\n    \"\"\"Find diameter (longest path between any two nodes)\"\"\"\n    diameter = 0\n    \n    def height(node):\n        nonlocal diameter\n        if not node:\n            return 0\n        \n        left_height = height(node.left)\n        right_height = height(node.right)\n        \n        # Update diameter if path through this node is longer\n        diameter = max(diameter, left_height + right_height)\n        \n        # Return height of this subtree\n        return 1 + max(left_height, right_height)\n    \n    height(root)\n    return diameter\n\ndef lowest_common_ancestor(root, p, q):\n    \"\"\"Find LCA of two nodes p and q\"\"\"\n    if not root or root == p or root == q:\n        return root\n    \n    left = lowest_common_ancestor(root.left, p, q)\n    right = lowest_common_ancestor(root.right, p, q)\n    \n    # If both sides return non-null, current node is LCA\n    if left and right:\n        return root\n    \n    # Return whichever side found a target\n    return left if left else right\n\ndef sum_of_left_leaves(root):\n    \"\"\"Calculate sum of all left leaf nodes\"\"\"\n    if not root:\n        return 0\n    \n    total = 0\n    # Check if left child is a leaf\n    if root.left and not root.left.left and not root.left.right:\n        total += root.left.val\n    \n    # Recursively check both subtrees\n    total += sum_of_left_leaves(root.left)\n    total += sum_of_left_leaves(root.right)\n    \n    return total\n\ndef build_tree_preorder_inorder(preorder, inorder):\n    \"\"\"Construct tree from preorder and inorder traversals\"\"\"\n    if not preorder or not inorder:\n        return None\n    \n    # First element of preorder is root\n    root_val = preorder[0]\n    root = TreeNode(root_val)\n    \n    # Find root in inorder to split left/right subtrees\n    mid = inorder.index(root_val)\n    \n    # Recursively build left and right subtrees\n    root.left = build_tree_preorder_inorder(preorder[1:mid+1], inorder[:mid])\n    root.right = build_tree_preorder_inorder(preorder[mid+1:], inorder[mid+1:])\n    \n    return root",
      "input": "Example Tree:\n       5\n      / \\\n     4   8\n    /   / \\\n   11  13  4\n  /  \\      \\\n 7    2      1"
    }
    },
    {
    "slug": "graph-representation",
    "title": "Graph Representation",
    "summary": "Learn how to represent graphs using adjacency lists, matrices, and edge lists",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "A graph is a data structure consisting of nodes (vertices) and edges connecting them. Unlike trees which have hierarchical parent-child relationships, graphs can have arbitrary connections including cycles. Graphs model real-world networks: social connections, road maps, dependencies, and web pages. A graph can be directed (edges have direction) or undirected (connections are bidirectional), and weighted (edges have costs) or unweighted.",
      "Edge List representation stores a graph as a simple list of edges, where each edge is a pair [u, v] indicating nodes u and v are connected. For weighted graphs, edges include weights: [u, v, weight]. This is the most compact representation, using O(E) space where E is number of edges. Edge lists are ideal for algorithms that process all edges like Kruskal's algorithm for minimum spanning trees, but checking if two nodes are connected takes O(E) time.",
      "Adjacency Matrix uses a 2D array of size N×N where N is number of nodes. matrix[i][j] = 1 if edge exists from node i to j, else 0. For weighted graphs, store weights instead of 1. This representation allows O(1) edge lookup - checking if nodes are connected is instant. However, it always uses O(N²) space regardless of edge count, making it inefficient for sparse graphs (few edges). Adjacency matrices work well for dense graphs where most nodes connect to most other nodes.",
      "Adjacency List is the most common representation, using an array or hash map where each node stores a list of its neighbors. For node i, adjacencyList[i] contains all nodes connected to i. This uses O(V + E) space - proportional to actual edges, making it memory-efficient for sparse graphs. Adding edges is O(1), and iterating neighbors of a node is O(degree). Most graph algorithms including DFS, BFS, and Dijkstra's work naturally with adjacency lists.",
      "Choosing the right representation depends on your use case. Use adjacency lists for most problems as they balance space and time efficiency. Use adjacency matrices when you need fast edge lookups or the graph is dense. Use edge lists when you only need to iterate all edges or the graph is very sparse. In coding interviews, adjacency lists are standard - graphs are typically given as arrays where index represents node and value is list of neighbors."
    ],
    "example": {
      "language": "python",
      "code": "from collections import defaultdict\n\n# Graph with nodes 0,1,2,3 and edges [(0,1), (0,2), (1,2), (2,3)]\n\n# 1. Edge List Representation\nedge_list = [[0,1], [0,2], [1,2], [2,3]]\nprint(f\"Edge List: {edge_list}\")\nprint(f\"Space: O(E) = O(4)\")\n\n# 2. Adjacency Matrix Representation\nn = 4  # number of nodes\nadj_matrix = [[0]*n for _ in range(n)]\nfor u, v in edge_list:\n    adj_matrix[u][v] = 1\n    adj_matrix[v][u] = 1  # for undirected graph\n\nprint(f\"\\nAdjacency Matrix:\")\nfor row in adj_matrix:\n    print(row)\nprint(f\"Space: O(N²) = O(16)\")\nprint(f\"Check edge (1,2): O(1) = {adj_matrix[1][2]}\")\n\n# 3. Adjacency List Representation (most common)\nadj_list = defaultdict(list)\nfor u, v in edge_list:\n    adj_list[u].append(v)\n    adj_list[v].append(u)  # for undirected graph\n\nprint(f\"\\nAdjacency List:\")\nfor node in range(n):\n    print(f\"{node}: {adj_list[node]}\")\nprint(f\"Space: O(V+E) = O(4+8) = O(12)\")\n\n# Converting between representations\ndef edge_list_to_adj_list(edges, n):\n    \"\"\"Convert edge list to adjacency list\"\"\"\n    graph = defaultdict(list)\n    for u, v in edges:\n        graph[u].append(v)\n        graph[v].append(u)\n    return graph\n\ndef adj_list_to_matrix(adj_list, n):\n    \"\"\"Convert adjacency list to matrix\"\"\"\n    matrix = [[0]*n for _ in range(n)]\n    for u in adj_list:\n        for v in adj_list[u]:\n            matrix[u][v] = 1\n    return matrix",
      "input": "Graph:\n  0 -- 1\n  |    |\n  2 -- 3\n\nEdges: [[0,1],[0,2],[1,2],[2,3]]"
    }
    },
    {
    "slug": "graph-dfs",
    "title": "Graph DFS (Depth-First Search)",
    "summary": "Master depth-first traversal, cycle detection, and connected components",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Depth-First Search (DFS) is a graph traversal algorithm that explores as far as possible along each branch before backtracking. Starting from a source vertex, DFS visits a neighbor, then recursively explores that neighbor's unvisited neighbors, going deeper until reaching a dead end before backtracking. This contrasts with BFS which explores level-by-level. DFS uses a stack (implicitly via recursion or explicitly) while BFS uses a queue. For graph with edges [0→1, 0→2, 1→3, 2→3], DFS from 0 might visit: 0→1→3→(backtrack)→2.",
      "DFS implementation requires tracking visited nodes to avoid infinite loops in cyclic graphs. Use a boolean visited array or set where visited[node] = true after processing. The recursive approach is most natural: mark current node visited, then recursively call DFS on each unvisited neighbor. Iterative DFS uses an explicit stack, pushing unvisited neighbors and popping to process. Time complexity is O(V + E) where V is vertices and E is edges - we visit each vertex once and explore each edge once. Space complexity is O(V) for the visited array plus O(V) for recursion stack depth.",
      "Cycle detection in undirected graphs uses DFS with parent tracking. A cycle exists if during DFS we encounter a visited node that isn't the parent of current node - this indicates a back edge forming a cycle. For directed graphs, cycle detection requires tracking nodes in the current recursion stack. If we reach a node already in the recursion stack, a cycle exists. Use three states: unvisited (white), in-stack (gray), and completely processed (black). A back edge from gray to gray node indicates a cycle.",
      "Connected components are maximal subgraphs where every pair of vertices has a path between them. In undirected graphs, run DFS from each unvisited node - each DFS call discovers one complete component. Count the number of DFS calls to get component count. For example, graph with edges [0-1, 1-2, 3-4] has 2 components: {0,1,2} and {3,4}. This is useful for network connectivity problems, island counting, and clustering. In directed graphs, use Kosaraju's or Tarjan's algorithm for strongly connected components.",
      "DFS applications include topological sorting (for directed acyclic graphs), maze solving, path finding, detecting deadlocks, and solving puzzles. DFS is preferred over BFS when: (1) solution is far from root, (2) tree is very deep with many branches, (3) checking if path exists, or (4) memory is limited. For shortest paths in unweighted graphs, use BFS instead. DFS can be modified to find all paths between two nodes by removing the permanent visited marking and using backtracking."
    ],
    "example": {
      "language": "python",
      "code": "from collections import defaultdict\n\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v, directed=False):\n        self.graph[u].append(v)\n        if not directed:\n            self.graph[v].append(u)\n    \n    def dfs(self, start, visited=None):\n        \"\"\"Basic DFS traversal\"\"\"\n        if visited is None:\n            visited = set()\n        \n        visited.add(start)\n        print(start, end=' ')\n        \n        for neighbor in self.graph[start]:\n            if neighbor not in visited:\n                self.dfs(neighbor, visited)\n        \n        return visited\n    \n    def has_cycle_undirected(self, node, visited, parent):\n        \"\"\"Detect cycle in undirected graph\"\"\"\n        visited.add(node)\n        \n        for neighbor in self.graph[node]:\n            if neighbor not in visited:\n                if self.has_cycle_undirected(neighbor, visited, node):\n                    return True\n            elif neighbor != parent:\n                # Visited node that's not parent = cycle!\n                return True\n        \n        return False\n    \n    def has_cycle_directed(self, node, visited, rec_stack):\n        \"\"\"Detect cycle in directed graph\"\"\"\n        visited.add(node)\n        rec_stack.add(node)\n        \n        for neighbor in self.graph[node]:\n            if neighbor not in visited:\n                if self.has_cycle_directed(neighbor, visited, rec_stack):\n                    return True\n            elif neighbor in rec_stack:\n                # Back edge to node in recursion stack = cycle!\n                return True\n        \n        rec_stack.remove(node)\n        return False\n    \n    def count_connected_components(self, num_nodes):\n        \"\"\"Count connected components (undirected graph)\"\"\"\n        visited = set()\n        count = 0\n        \n        for node in range(num_nodes):\n            if node not in visited:\n                self.dfs(node, visited)\n                count += 1\n        \n        return count\n    \n    def find_path(self, start, end, visited=None, path=None):\n        \"\"\"Find a path between two nodes\"\"\"\n        if visited is None:\n            visited = set()\n        if path is None:\n            path = []\n        \n        visited.add(start)\n        path.append(start)\n        \n        if start == end:\n            return path\n        \n        for neighbor in self.graph[start]:\n            if neighbor not in visited:\n                result = self.find_path(neighbor, end, visited, path)\n                if result:\n                    return result\n        \n        path.pop()\n        return None",
      "input": "Graph Example:\n    0 --- 1\n    |     |\n    2 --- 3\n\nDFS from 0: 0 → 1 → 3 → 2"
    }
    },
    {
    "slug": "graph-bfs",
    "title": "Graph BFS (Breadth-First Search)",
    "summary": "Learn BFS traversal, shortest paths, and bipartite graph detection",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Breadth-First Search (BFS) is a graph traversal algorithm that explores nodes level by level, visiting all neighbors at the current distance before moving to nodes at the next distance. Starting from a source vertex, BFS visits all nodes at distance 1, then all nodes at distance 2, and so on. This contrasts with DFS which goes deep first. BFS uses a queue (FIFO - First In, First Out) to maintain nodes for processing. For graph with edges [0-1, 0-2, 1-3, 2-3], BFS from 0 visits: 0 → [1,2] → [3], exploring horizontally before going deeper.",
      "BFS implementation uses a queue and visited tracking. Initialize by enqueuing the source node and marking it visited. While queue is not empty: dequeue a node, process it, then enqueue all its unvisited neighbors and mark them visited. The queue ensures nodes are processed in order of increasing distance from source. Time complexity is O(V + E) where V is vertices and E is edges - each vertex is enqueued once and each edge examined once. Space complexity is O(V) for the queue and visited array. The level-by-level nature makes BFS ideal for finding shortest paths.",
      "Shortest path in unweighted graphs is BFS's killer application. Because BFS explores nodes by distance, the first time you reach a node is guaranteed to be via the shortest path. To track paths, maintain a distance array (distance from source) and parent array (previous node in path). When processing node u and discovering unvisited neighbor v, set distance[v] = distance[u] + 1 and parent[v] = u. Reconstruct path by backtracking from destination through parents. For weighted graphs, use Dijkstra's algorithm instead. BFS finds shortest path in O(V + E) time.",
      "Bipartite graphs can be partitioned into two sets where every edge connects vertices from different sets - no two vertices in the same set are adjacent. Examples include matching problems, scheduling, and graph coloring. To check if a graph is bipartite, use BFS with two-coloring: assign source node color 0, then for each node, assign its unvisited neighbors the opposite color (1 if current is 0, vice versa). If you ever find a neighbor with the same color as current node, the graph is not bipartite (contains odd-length cycle). BFS ensures we check all connected components.",
      "BFS applications include shortest path in unweighted graphs, finding connected components, checking bipartiteness, web crawling, GPS navigation (with modifications), social network friend suggestions (friends of friends), and solving puzzles like sliding tile games. Use BFS when: (1) finding shortest path in unweighted graphs, (2) exploring nodes level by level, (3) finding nearest neighbors, or (4) the solution is close to the root. BFS is preferred over DFS for shortest paths because it explores nodes in order of distance. For very deep graphs with limited memory, DFS may be better."
    ],
    "example": {
      "language": "python",
      "code": "from collections import deque, defaultdict\n\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v, directed=False):\n        self.graph[u].append(v)\n        if not directed:\n            self.graph[v].append(u)\n    \n    def bfs(self, start):\n        \"\"\"Basic BFS traversal\"\"\"\n        visited = set()\n        queue = deque([start])\n        visited.add(start)\n        result = []\n        \n        while queue:\n            node = queue.popleft()\n            result.append(node)\n            \n            for neighbor in self.graph[node]:\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    queue.append(neighbor)\n        \n        return result\n    \n    def shortest_path(self, start, end):\n        \"\"\"Find shortest path using BFS\"\"\"\n        if start == end:\n            return [start]\n        \n        visited = set([start])\n        queue = deque([start])\n        parent = {start: None}\n        \n        while queue:\n            node = queue.popleft()\n            \n            if node == end:\n                # Reconstruct path\n                path = []\n                current = end\n                while current is not None:\n                    path.append(current)\n                    current = parent[current]\n                return path[::-1]\n            \n            for neighbor in self.graph[node]:\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    parent[neighbor] = node\n                    queue.append(neighbor)\n        \n        return None  # No path found\n    \n    def shortest_distances(self, start, num_nodes):\n        \"\"\"Find shortest distance from start to all nodes\"\"\"\n        distance = [-1] * num_nodes\n        distance[start] = 0\n        queue = deque([start])\n        \n        while queue:\n            node = queue.popleft()\n            \n            for neighbor in self.graph[node]:\n                if distance[neighbor] == -1:\n                    distance[neighbor] = distance[node] + 1\n                    queue.append(neighbor)\n        \n        return distance\n    \n    def is_bipartite(self, num_nodes):\n        \"\"\"Check if graph is bipartite using 2-coloring\"\"\"\n        color = [-1] * num_nodes\n        \n        # Check all components\n        for start in range(num_nodes):\n            if color[start] == -1:\n                queue = deque([start])\n                color[start] = 0\n                \n                while queue:\n                    node = queue.popleft()\n                    \n                    for neighbor in self.graph[node]:\n                        if color[neighbor] == -1:\n                            # Color with opposite color\n                            color[neighbor] = 1 - color[node]\n                            queue.append(neighbor)\n                        elif color[neighbor] == color[node]:\n                            # Same color = not bipartite\n                            return False\n        \n        return True",
      "input": "Graph Example:\n    0 --- 1\n    |     |\n    2 --- 3\n\nBFS from 0: 0 → 1, 2 → 3\nShortest path 0→3: 0→1→3 (length 2)"
    }
    },
    {
    "slug": "shortest-path-dijkstra",
    "title": "Shortest Path (Dijkstra's Algorithm)",
    "summary": "Find shortest paths in weighted graphs using Dijkstra's algorithm",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Dijkstra's algorithm finds the shortest path from a source vertex to all other vertices in a weighted graph with non-negative edge weights. Unlike BFS which works only for unweighted graphs (or treats all edges as weight 1), Dijkstra handles varying edge costs. The algorithm maintains a set of visited nodes and repeatedly selects the unvisited node with the smallest known distance, then updates distances to its neighbors. For graph with edges [(A,B,4), (A,C,2), (C,B,1)], the shortest path A→B is A→C→B with total weight 3, not the direct edge A→B with weight 4.",
      "The algorithm uses a priority queue (min-heap) to efficiently select the next node to process - always choosing the unvisited node with minimum distance. Initialize all distances to infinity except source (distance 0). While priority queue is not empty: extract node u with minimum distance, mark it visited, then for each neighbor v of u, if distance[u] + weight(u,v) < distance[v], update distance[v] and parent[v]. This relaxation step is key - we constantly try to find better paths. The priority queue ensures we process nodes in order of increasing distance from source.",
      "Time complexity is O((V + E) log V) with a binary heap priority queue, where V is vertices and E is edges. Each vertex is extracted once from the priority queue (O(V log V)), and each edge causes a potential distance update and priority queue operation (O(E log V)). Space complexity is O(V) for distance array, parent array, and priority queue. With a Fibonacci heap, complexity improves to O(E + V log V), but implementation is complex. For dense graphs where E ≈ V², using an array for finding minimum (O(V²)) can be faster than priority queue.",
      "Dijkstra's algorithm has critical requirements and limitations. It requires non-negative edge weights - negative weights break the greedy approach because a shorter path might be found after marking a node as visited. For graphs with negative weights, use Bellman-Ford algorithm instead. Dijkstra works on both directed and undirected graphs. To reconstruct the shortest path, maintain a parent array tracking the previous node in the optimal path. Backtrack from destination through parents to source, then reverse. The algorithm naturally handles disconnected graphs - unreachable nodes retain infinite distance.",
      "Applications include GPS navigation systems, network routing protocols (OSPF uses Dijkstra), social network analysis (degrees of separation), game AI pathfinding, and resource optimization. Dijkstra vs BFS: use BFS for unweighted graphs (faster, simpler), use Dijkstra for weighted graphs with non-negative weights. Dijkstra vs Bellman-Ford: Dijkstra is faster but requires non-negative weights; Bellman-Ford handles negative weights and detects negative cycles. Dijkstra vs A*: A* is Dijkstra with a heuristic for faster goal-directed search, commonly used in games and robotics."
    ],
    "example": {
      "language": "python",
      "code": "import heapq\nfrom collections import defaultdict\n\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v, weight, directed=False):\n        self.graph[u].append((v, weight))\n        if not directed:\n            self.graph[v].append((u, weight))\n    \n    def dijkstra(self, start, num_nodes):\n        \"\"\"Find shortest distances from start to all nodes\"\"\"\n        # Initialize distances to infinity\n        distance = [float('inf')] * num_nodes\n        distance[start] = 0\n        parent = [-1] * num_nodes\n        \n        # Priority queue: (distance, node)\n        pq = [(0, start)]\n        visited = set()\n        \n        while pq:\n            dist, node = heapq.heappop(pq)\n            \n            # Skip if already visited\n            if node in visited:\n                continue\n            \n            visited.add(node)\n            \n            # Check all neighbors\n            for neighbor, weight in self.graph[node]:\n                new_dist = distance[node] + weight\n                \n                # Relaxation: found shorter path?\n                if new_dist < distance[neighbor]:\n                    distance[neighbor] = new_dist\n                    parent[neighbor] = node\n                    heapq.heappush(pq, (new_dist, neighbor))\n        \n        return distance, parent\n    \n    def shortest_path_to(self, start, end, num_nodes):\n        \"\"\"Find shortest path from start to end\"\"\"\n        distance, parent = self.dijkstra(start, num_nodes)\n        \n        if distance[end] == float('inf'):\n            return None  # No path exists\n        \n        # Reconstruct path\n        path = []\n        current = end\n        while current != -1:\n            path.append(current)\n            current = parent[current]\n        \n        path.reverse()\n        return path, distance[end]\n\n# Example usage\ng = Graph()\ng.add_edge(0, 1, 4)\ng.add_edge(0, 2, 2)\ng.add_edge(2, 1, 1)\ng.add_edge(1, 3, 5)\ng.add_edge(2, 3, 8)\n\npath, cost = g.shortest_path_to(0, 3, 4)\nprint(f\"Path: {' -> '.join(map(str, path))}\")\nprint(f\"Cost: {cost}\")\n# Output: Path: 0 -> 2 -> 1 -> 3, Cost: 8",
      "input": "Weighted Graph:\n    0 --4-- 1\n    |  \\    |\n    2   1   5\n    |    \\  |\n    2 --8-- 3\n\nShortest 0→3: 0→2→1→3 (cost: 8)"
    }
    },
    {
    "slug": "topological-sort",
    "title": "Topological Sort",
    "summary": "Order tasks with dependencies using DFS and Kahn's algorithm",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Topological sorting produces a linear ordering of vertices in a directed acyclic graph (DAG) such that for every directed edge u→v, vertex u appears before v in the ordering. This models tasks with dependencies: if task A must be completed before task B, we draw an edge A→B, and topological sort gives a valid task execution order. For courses with prerequisites, topological sort tells you which order to take classes. Example: for edges [0→1, 0→2, 1→3, 2→3], valid orders include [0,1,2,3] or [0,2,1,3]. Topological sort only exists for DAGs - graphs with cycles have no valid ordering.",
      "Two main algorithms solve topological sort: DFS-based and Kahn's algorithm (BFS-based). DFS approach performs depth-first search and adds nodes to result after visiting all descendants, then reverses the result. When exploring a node, recursively visit all neighbors first, then add current node to a stack. The stack naturally contains nodes in reverse topological order - nodes with no outgoing edges appear first, then nodes depending on them. Time complexity is O(V + E) visiting each vertex and edge once. This approach is elegant and naturally detects cycles using recursion stack.",
      "Kahn's algorithm is BFS-based and more intuitive for dependency problems. Calculate in-degree (number of incoming edges) for each vertex. Vertices with in-degree 0 have no dependencies and can be processed first. Initialize queue with all zero in-degree vertices. While queue is not empty: dequeue vertex, add to result, then for each neighbor, decrease its in-degree by 1 (removing the processed dependency). If neighbor's in-degree becomes 0, enqueue it. If result contains all vertices, topological order exists; otherwise graph has a cycle. Time O(V + E), space O(V).",
      "Cycle detection is built into both algorithms. In DFS approach, maintain a recursion stack - if you reach a node already in the current path (gray node), a cycle exists. In Kahn's algorithm, if you process fewer than V vertices, remaining vertices are stuck in a cycle with no zero in-degree vertices. Course Schedule problems on LeetCode are classic topological sort applications: Course Schedule checks if completion is possible (cycle detection), Course Schedule II returns the actual course order (topological ordering).",
      "Applications include course scheduling with prerequisites, build systems (compile files in dependency order), package managers (install packages with dependencies), project management (task scheduling), symbol resolution in compilers, and spreadsheet formula evaluation. Choose DFS for simpler implementation when you just need an ordering. Choose Kahn's when you need to track in-degrees or process nodes level-by-level. Both detect cycles - DFS with recursion stack, Kahn's by checking if all vertices are processed. Multiple valid topological orders can exist for the same DAG."
    ],
    "example": {
      "language": "python",
      "code": "from collections import defaultdict, deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n    \n    def topological_sort_dfs(self):\n        \"\"\"DFS-based topological sort\"\"\"\n        visited = set()\n        rec_stack = set()\n        result = []\n        \n        def dfs(node):\n            if node in rec_stack:\n                return False  # Cycle detected\n            if node in visited:\n                return True\n            \n            visited.add(node)\n            rec_stack.add(node)\n            \n            for neighbor in self.graph[node]:\n                if not dfs(neighbor):\n                    return False\n            \n            rec_stack.remove(node)\n            result.append(node)  # Add after visiting all descendants\n            return True\n        \n        for node in range(self.V):\n            if node not in visited:\n                if not dfs(node):\n                    return None  # Cycle exists\n        \n        return result[::-1]  # Reverse to get correct order\n    \n    def topological_sort_kahn(self):\n        \"\"\"Kahn's algorithm (BFS-based)\"\"\"\n        # Calculate in-degrees\n        in_degree = [0] * self.V\n        for node in range(self.V):\n            for neighbor in self.graph[node]:\n                in_degree[neighbor] += 1\n        \n        # Initialize queue with zero in-degree nodes\n        queue = deque([i for i in range(self.V) if in_degree[i] == 0])\n        result = []\n        \n        while queue:\n            node = queue.popleft()\n            result.append(node)\n            \n            # Reduce in-degree for neighbors\n            for neighbor in self.graph[node]:\n                in_degree[neighbor] -= 1\n                if in_degree[neighbor] == 0:\n                    queue.append(neighbor)\n        \n        # Check if all vertices processed (no cycle)\n        if len(result) != self.V:\n            return None  # Cycle exists\n        \n        return result\n\n# Example: Course prerequisites\ng = Graph(4)\ng.add_edge(0, 1)  # Course 0 before 1\ng.add_edge(0, 2)  # Course 0 before 2\ng.add_edge(1, 3)  # Course 1 before 3\ng.add_edge(2, 3)  # Course 2 before 3\n\nprint(\"DFS:\", g.topological_sort_dfs())   # [0, 1, 2, 3] or [0, 2, 1, 3]\nprint(\"Kahn's:\", g.topological_sort_kahn()) # [0, 1, 2, 3] or [0, 2, 1, 3]",
      "input": "Course Dependencies:\n    0 → 1\n    0 → 2\n    1 → 3\n    2 → 3\n\nValid orders: [0,1,2,3] or [0,2,1,3]"
    }
  },
    {
    "slug": "dp-introduction",
    "title": "Dynamic Programming: Intro, Memoization & Tabulation",
    "summary": "Learn how to turn slow recursive solutions into efficient algorithms using overlapping subproblems and optimal substructure.",
    "level": "Intermediate",
    "category": "Dynamic Programming",
    "content": [
      "Dynamic Programming (DP) is a technique to solve complex problems by breaking them into smaller overlapping subproblems and reusing their solutions instead of recomputing them. It is especially useful in optimization and counting problems where a naive recursive solution repeatedly solves the same subproblems, leading to exponential time. DP usually reduces such solutions to polynomial time by caching or tabulating results.",
      "Two key properties make a problem a good candidate for DP: optimal substructure and overlapping subproblems. Optimal substructure means the optimal solution to the overall problem can be constructed from optimal solutions of its subproblems. Overlapping subproblems means the same subproblems are solved multiple times in a naive recursion, so caching them can save work.",
      "A classic example is the Fibonacci sequence, where fib(n) = fib(n-1) + fib(n-2) with base cases fib(0) = 0 and fib(1) = 1. A naive recursive implementation recomputes the same fib(k) many times, leading to a time complexity of roughly O(2^n). With dynamic programming, we compute each fib(k) once and reuse stored results, reducing the complexity to O(n).",
      "There are two primary approaches to dynamic programming: top-down with memoization and bottom-up with tabulation. In the top-down approach, you start with the original problem and recursively solve subproblems, storing answers in a cache (memo). In the bottom-up approach, you iteratively build a table from the smallest subproblems up to the original one, carefully choosing an order so that dependencies are already computed.",
      "Memoization (top-down DP) wraps a normal recursive solution with a cache like an array or dictionary. When a recursive call is made for a subproblem, you first check if its result is already in the cache; if so, you return it immediately instead of recomputing. This keeps the code close to the mathematical recurrence and is often easier to write and debug when learning DP.",
      "Tabulation (bottom-up DP) builds a table iteratively, usually using loops. For Fibonacci, you can create an array dp where dp[i] stores fib(i), set the base cases like dp[0] and dp[1], and then fill the array from i = 2 to n using the relation dp[i] = dp[i-1] + dp[i-2]. Tabulation avoids recursion and can be more memory-efficient when combined with techniques like only storing the last few states.",
      "Choosing between memoization and tabulation depends on the problem and your goals. Memoization is usually easier to derive from a recursive definition, but it uses the call stack and may have overhead from recursive calls. Tabulation can be faster in practice and gives you more control over iteration order and space optimizations, but it requires you to think explicitly about the correct filling order of the DP table.",
      "Beyond Fibonacci, DP appears in many real-world scenarios such as path counting in grids, minimizing costs in scheduling, and optimizing resource allocation. For example, computing the minimum cost to reach the bottom-right of a grid from the top-left can be solved by defining dp[i][j] as the minimum cost to reach cell (i, j) and building the table row by row or column by column.",
      "When approaching any DP problem, a useful checklist is: identify the state, define the recurrence, choose between memoization and tabulation, set base cases, and determine time and space complexity. The state is what uniquely identifies a subproblem, often an index, pair of indices, or a combination of index and capacity or sum. The recurrence defines how the answer for one state depends on smaller states.",
      "As you practice DP, you will begin to recognize common patterns such as 1D sequence DP, 2D grid DP, knapsack-style subset DP, and DP on strings. Building intuition around these patterns helps you quickly map new problems to familiar templates. In AlgoSort, use the visualizer for this topic to see how subproblems are reused and how a DP table fills step by step for Fibonacci and a small grid path example."
    ],
    "example": {
      "language": "python",
      "code": "from functools import lru_cache\n\n# Top-down: Fibonacci with memoization\n@lru_cache(maxsize=None)\ndef fib_memo(n: int) -> int:\n    if n <= 1:\n        return n\n    return fib_memo(n - 1) + fib_memo(n - 2)\n\n# Bottom-up: Fibonacci with tabulation\ndef fib_tab(n: int) -> int:\n    if n <= 1:\n        return n\n    dp = [0] * (n + 1)\n    dp[0], dp[1] = 0, 1\n    for i in range(2, n + 1):\n        dp[i] = dp[i - 1] + dp[i - 2]\n    return dp[n]\n\nprint(fib_memo(10))  # 55\nprint(fib_tab(10))   # 55",
      "input": "Example: n = 10"
    }
  },
    {
    "slug": "dp-1d-sequence",
    "title": "1D Sequence DP: Climbing Stairs & House Robber",
    "summary": "Master dynamic programming on linear sequences with decision-making at each step: take, skip, or choose optimally.",
    "level": "Intermediate",
    "category": "Dynamic Programming",
    "content": [
      "1D Sequence DP is one of the most fundamental dynamic programming patterns where you process elements in a linear sequence (array) and make optimal decisions at each position. Unlike the basic Fibonacci example, these problems involve real-world choices: should you climb 1 or 2 steps? Should you rob this house or skip it? The key is defining a state that captures your position and building a recurrence based on the choices available at that position.",
      "The general template for 1D sequence DP is: define dp[i] as the optimal solution (max/min value, count, etc.) up to index i, then express dp[i] in terms of previous states like dp[i-1] and dp[i-2]. For example, in Climbing Stairs, dp[i] represents the number of ways to reach step i, and since you can arrive from step i-1 (climb 1) or step i-2 (climb 2), the recurrence is dp[i] = dp[i-1] + dp[i-2], identical to Fibonacci but with a different real-world interpretation.",
      "House Robber introduces a decision-making twist: you cannot rob two adjacent houses. Here dp[i] represents the maximum money you can rob up to house i. At each house, you face a choice: rob it (gaining nums[i] + dp[i-2]) or skip it (keeping dp[i-1]). The recurrence becomes dp[i] = max(dp[i-1], nums[i] + dp[i-2]), demonstrating how DP captures trade-offs between conflicting constraints. This pattern extends to many scheduling and selection problems.",
      "Min Cost Climbing Stairs adds a cost array where each step has a cost, and you can start from step 0 or 1, aiming to reach the top with minimum cost. The recurrence is dp[i] = cost[i] + min(dp[i-1], dp[i-2]), showing that you add the current cost to the cheaper of the two previous paths. This variant teaches how DP handles optimization with weighted choices, common in resource allocation and pathfinding problems.",
      "A powerful optimization technique in 1D DP is space reduction. Notice that dp[i] only depends on dp[i-1] and dp[i-2], not the entire history. Instead of maintaining an array of size n, you can use just two variables (prev1, prev2) and update them in a rolling fashion. This reduces space complexity from O(n) to O(1) while keeping the same O(n) time complexity. Many interview problems expect this optimization as a follow-up question.",
      "These 1D patterns generalize to many scenarios: jump games (can you reach the end?), paint house (choose colors with constraints), delete and earn (pick numbers optimally), and more. The mental model is always: identify what state you need to track, determine the choices at each step, write the recurrence combining those choices, set base cases, and decide between tabulation and memoization. Practicing these core problems builds intuition for recognizing similar structures in new problems.",
      "When debugging 1D DP solutions, trace through small examples manually. For Climbing Stairs with n=5, write out dp[0]=1, dp[1]=1, dp[2]=2, dp[3]=3, dp[4]=5, dp[5]=8 and verify each step follows the recurrence. For House Robber with nums=[2,7,9,3,1], check dp=[2,7,11,11,12] and confirm why dp[2]=11 (rob house 0 and 2) and dp[3]=11 (skipping house 3 is better). Visualizing the decision tree or DP table makes these patterns concrete.",
      "Real-world applications of 1D sequence DP include: stock trading strategies (buy/sell with cooldown), task scheduling (maximize profit with non-overlapping intervals), resource management in games (optimal power-ups collection), and even biology (sequence alignment scoring). The simplicity of 1D DP makes it a building block for more complex 2D grid DP, tree DP, and state machine DP problems you will encounter in advanced topics.",
      "As you practice, pay attention to boundary conditions: what are dp[0] and dp[1]? Can you start from index 0 or 1? Does the problem ask for the value at dp[n] or dp[n-1]? Small off-by-one errors are common in DP, so careful initialization and loop range selection are critical. Use the visualizer in AlgoSort to step through examples and see exactly when each state is computed and how choices propagate through the array.",
      "Moving forward, you will combine this 1D sequence pattern with other dimensions (2D grid DP), additional constraints (knapsack with capacity), or string matching (LCS, edit distance). But the core principle remains: break the problem into subproblems, avoid recomputation, and build up the answer systematically. Mastering 1D DP now makes those advanced topics much easier to understand and implement."
    ],
    "example": {
      "language": "python",
      "code": "# Climbing Stairs - Tabulation\ndef climbStairs(n: int) -> int:\n    if n <= 2:\n        return n\n    dp = [0] * (n + 1)\n    dp[1], dp[2] = 1, 2\n    for i in range(3, n + 1):\n        dp[i] = dp[i - 1] + dp[i - 2]\n    return dp[n]\n\n# House Robber - Tabulation\ndef rob(nums: list[int]) -> int:\n    if not nums:\n        return 0\n    if len(nums) == 1:\n        return nums[0]\n    dp = [0] * len(nums)\n    dp[0] = nums[0]\n    dp[1] = max(nums[0], nums[1])\n    for i in range(2, len(nums)):\n        dp[i] = max(dp[i - 1], nums[i] + dp[i - 2])\n    return dp[-1]\n\n# Space-Optimized House Robber - O(1) space\ndef rob_optimized(nums: list[int]) -> int:\n    if not nums:\n        return 0\n    prev2, prev1 = 0, 0\n    for num in nums:\n        prev2, prev1 = prev1, max(prev1, num + prev2)\n    return prev1\n\nprint(climbStairs(5))  # 8\nprint(rob([2,7,9,3,1]))  # 12\nprint(rob_optimized([2,7,9,3,1]))  # 12",
      "input": "Examples: n=5 for stairs, nums=[2,7,9,3,1] for robber"
    }
  },
    {
    "slug": "dp-knapsack",
    "title": "Knapsack DP: 0/1 Knapsack & Subset Problems",
    "summary": "Master 2D dynamic programming for optimization problems with capacity constraints and subset selection.",
    "level": "Intermediate",
    "category": "Dynamic Programming",
    "content": [
      "Knapsack problems represent a fundamental class of optimization problems where you must make binary decisions (take or leave) for each item while respecting a constraint like weight capacity or target sum. The classic 0/1 Knapsack asks: given items with weights and values and a knapsack with capacity W, what is the maximum value you can carry without exceeding the capacity? Unlike 1D sequence DP, knapsack introduces a second dimension to track both the current item index and remaining capacity.",
      "The state definition for 0/1 Knapsack is dp[i][w] representing the maximum value achievable using the first i items with capacity w. At each item i, you face a choice: skip it (keeping dp[i-1][w]) or take it if it fits (gaining value[i] + dp[i-1][w - weight[i]]). The recurrence is dp[i][w] = max(dp[i-1][w], value[i] + dp[i-1][w - weight[i]] if weight[i] <= w). This creates a 2D table where each cell depends on the row above it, making it a bottom-up tabulation problem.",
      "Building the DP table requires careful initialization: dp[0][w] = 0 for all w (no items means zero value) and dp[i][0] = 0 for all i (zero capacity means nothing fits). You then fill the table row by row from item 1 to n and column by column from capacity 1 to W. The final answer is dp[n][W], representing all items considered with full capacity. Tracing back through the table can reconstruct which items were selected by checking where decisions changed.",
      "Subset Sum is a decision variant of knapsack where all items have weight equal to value, and you ask: can you select items that sum exactly to a target? Here dp[i][sum] is a boolean indicating whether you can achieve sum using the first i items. The recurrence becomes dp[i][sum] = dp[i-1][sum] OR dp[i-1][sum - nums[i]], meaning you can reach sum if either you could reach it without item i, or you could reach sum - nums[i] and then add item i. This pattern extends to Partition Equal Subset Sum and Target Sum problems.",
      "Space optimization for knapsack problems is possible because each row only depends on the previous row. Instead of a 2D array of size n × W, you can use a 1D array of size W and update it in reverse order (from W down to 0) to avoid overwriting values you still need. This reduces space complexity from O(n × W) to O(W) while keeping the same O(n × W) time complexity. The reverse iteration is critical: forward iteration would use already-updated values from the current iteration instead of the previous row.",
      "Unbounded Knapsack is a variant where you can take each item unlimited times. The only change is updating the DP array in forward order instead of reverse, because you want to reuse the current row's values (allowing multiple uses of the same item). The recurrence remains similar but now references dp[w - weight[i]] from the same row. This pattern applies to Coin Change II (counting ways) and Coin Change (minimizing coins) problems, both classic unbounded knapsack applications.",
      "Real-world applications of knapsack DP are everywhere: resource allocation with budget constraints, portfolio optimization selecting investments, cutting stock to minimize waste, project selection maximizing ROI under time limits, and even data compression choosing which data to keep. The binary decision framework (include/exclude) maps naturally to many business and engineering problems where you must optimize under constraints.",
      "When implementing knapsack DP, watch for common pitfalls: forgetting to check if item fits before taking it (weight[i] <= w condition), iterating in the wrong order for space optimization, and off-by-one errors in array indexing since dp often uses 1-indexed items but arrays are 0-indexed. Always validate with small examples like 3 items with known optimal solutions before scaling up. The visualizer in AlgoSort shows exactly how the 2D table fills and how the choice at each cell depends on cells above and to the left.",
      "Advanced knapsack variations include: bounded knapsack (limited quantity per item), multi-dimensional knapsack (multiple constraints like weight and volume), and knapsack with dependencies (some items require others). But mastering 0/1 and unbounded knapsack first gives you the foundation to tackle these extensions. The core idea of tracking state in multiple dimensions and making optimal local choices that compose into a global optimum remains constant across all variants.",
      "As you practice knapsack problems, focus on recognizing the pattern: whenever you see binary choices with cumulative constraints or targets, think knapsack. The items might be numbers to sum, jobs to schedule, or features to select, but the DP structure is the same. Building intuition for how the 2D table represents all possible states and how choices propagate through it will make even complex knapsack variants feel approachable."
    ],
    "example": {
      "language": "python",
      "code": "# 0/1 Knapsack - 2D DP\ndef knapsack_2d(weights: list[int], values: list[int], capacity: int) -> int:\n    n = len(weights)\n    dp = [[0] * (capacity + 1) for _ in range(n + 1)]\n    \n    for i in range(1, n + 1):\n        for w in range(1, capacity + 1):\n            # Skip item i-1\n            dp[i][w] = dp[i-1][w]\n            # Take item i-1 if it fits\n            if weights[i-1] <= w:\n                dp[i][w] = max(dp[i][w], values[i-1] + dp[i-1][w - weights[i-1]])\n    \n    return dp[n][capacity]\n\n# Subset Sum - Space Optimized\ndef canPartition(nums: list[int]) -> bool:\n    total = sum(nums)\n    if total % 2 != 0:\n        return False\n    \n    target = total // 2\n    dp = [False] * (target + 1)\n    dp[0] = True\n    \n    for num in nums:\n        # Iterate backwards to avoid using updated values\n        for s in range(target, num - 1, -1):\n            dp[s] = dp[s] or dp[s - num]\n    \n    return dp[target]\n\n# Examples\nweights = [1, 3, 4, 5]\nvalues = [1, 4, 5, 7]\nprint(knapsack_2d(weights, values, 7))  # 9\nprint(canPartition([1, 5, 11, 5]))  # True",
      "input": "weights=[1,3,4,5], values=[1,4,5,7], capacity=7"
    }
  },
    {
    "slug": "dp-strings",
    "title": "String DP: LCS, Edit Distance & Pattern Matching",
    "summary": "Master 2D dynamic programming on strings for sequence comparison, alignment, and transformation problems.",
    "level": "Intermediate",
    "category": "Dynamic Programming",
    "content": [
      "String dynamic programming deals with problems where you compare, align, or transform two sequences (usually strings). The most fundamental pattern is defining dp[i][j] to represent some property of the first i characters of string A and the first j characters of string B. This creates a 2D table where you build up solutions character by character, making local decisions based on whether characters match or differ. String DP appears in bioinformatics (DNA sequence alignment), natural language processing (spell check, autocorrect), and version control (diff algorithms).",
      "Longest Common Subsequence (LCS) asks: what is the longest sequence of characters that appears in both strings in the same order, but not necessarily consecutively? For example, LCS of 'abcde' and 'ace' is 'ace' with length 3. The state dp[i][j] represents the LCS length of A[0..i-1] and B[0..j-1]. If A[i-1] == B[j-1], they contribute to the LCS, so dp[i][j] = 1 + dp[i-1][j-1]. If they differ, take the best from excluding one character: dp[i][j] = max(dp[i-1][j], dp[i][j-1]). This simple recurrence captures the essence of sequence alignment.",
      "Building the LCS table requires initializing dp[0][j] = 0 and dp[i][0] = 0 (empty string has LCS length 0 with anything). You then fill row by row, and each cell depends only on cells above, to the left, and diagonally above-left. The final answer is dp[m][n] where m and n are the string lengths. Backtracking through the table can reconstruct the actual LCS string by following the path of matching characters, though many problems only ask for the length.",
      "Edit Distance (Levenshtein Distance) measures the minimum number of operations (insert, delete, replace) to transform string A into string B. This is more complex than LCS because you have three choices at each step instead of two. The state dp[i][j] represents the edit distance between A[0..i-1] and B[0..j-1]. If A[i-1] == B[j-1], no operation is needed: dp[i][j] = dp[i-1][j-1]. Otherwise, consider three options: replace (dp[i-1][j-1] + 1), delete from A (dp[i-1][j] + 1), or insert into A (dp[i][j-1] + 1), and take the minimum.",
      "The initialization for Edit Distance is different from LCS: dp[i][0] = i (need i deletions to reduce A[0..i-1] to empty) and dp[0][j] = j (need j insertions to build B[0..j-1] from empty). This reflects the cost of transforming between a string and an empty string. The recurrence carefully considers all three edit operations, making it a classic example of DP with multiple transition choices. Understanding why each operation corresponds to a specific cell (diagonal for replace, left for insert, up for delete) is key to mastering this pattern.",
      "Longest Palindromic Subsequence (LPS) is a variation where you find the longest subsequence of a single string that reads the same forwards and backwards. Interestingly, LPS of string S is equivalent to the LCS of S and its reverse. This shows how string DP problems often reduce to each other. You can also solve LPS directly with dp[i][j] representing the LPS length in substring S[i..j], where the recurrence depends on whether S[i] == S[j]. These transformations between problem formulations are powerful tools for solving new string DP variants.",
      "Space optimization for string DP is similar to knapsack: each row depends only on the previous row, so you can use two 1D arrays and alternate between them, or even a single array with careful updating. For LCS and Edit Distance, you can reduce space from O(m × n) to O(min(m, n)) by making the shorter string define the DP array dimension. This is especially important when comparing very long strings like DNA sequences or large text files, where memory can be a bottleneck.",
      "Real-world applications of string DP are everywhere: spell checkers use Edit Distance to find closest dictionary words, version control systems use LCS to compute diffs between file versions, plagiarism detectors compare document subsequences, and bioinformatics tools align protein or DNA sequences to find evolutionary relationships. The abstraction of comparing two sequences generalizes beyond strings to any ordered data: time series, event logs, or even user behavior sequences.",
      "When implementing string DP, be careful with indexing: dp[i][j] typically represents strings up to index i-1 and j-1 (1-indexed DP array, 0-indexed strings). Always verify base cases and the recurrence with small examples like 'ab' and 'ac' before scaling up. The visualizer in AlgoSort shows how the DP table fills diagonally and how matching characters create paths through the table, making the abstract recurrence concrete and intuitive.",
      "Advanced string DP includes problems like Distinct Subsequences (count occurrences of a pattern), Interleaving Strings (check if one string is formed by interleaving two others), and Wildcard Matching (pattern matching with * and ? wildcards). But mastering LCS and Edit Distance first gives you the mental model for all 2D string DP: define the state for prefixes of both strings, handle character match/mismatch cases, and build the table systematically. Once you internalize this pattern, even complex variants become approachable."
    ],
    "example": {
      "language": "python",
      "code": "# Longest Common Subsequence\ndef longestCommonSubsequence(text1: str, text2: str) -> int:\n    m, n = len(text1), len(text2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if text1[i - 1] == text2[j - 1]:\n                dp[i][j] = 1 + dp[i - 1][j - 1]\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n    \n    return dp[m][n]\n\n# Edit Distance\ndef minDistance(word1: str, word2: str) -> int:\n    m, n = len(word1), len(word2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    # Base cases\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if word1[i - 1] == word2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(\n                    dp[i - 1][j],      # delete\n                    dp[i][j - 1],      # insert\n                    dp[i - 1][j - 1]   # replace\n                )\n    \n    return dp[m][n]\n\n# Examples\nprint(longestCommonSubsequence('abcde', 'ace'))  # 3\nprint(minDistance('horse', 'ros'))  # 3",
      "input": "text1='abcde', text2='ace' for LCS; word1='horse', word2='ros' for Edit Distance"
    }
  },
    {
    "slug": "binary-search-advanced",
    "title": "Binary Search: Advanced Patterns",
    "summary": "Master binary search on answer, rotated arrays, and finding boundaries in complex scenarios.",
    "level": "Intermediate",
    "category": "Binary Search Advanced",
    "content": [
      "Advanced binary search extends beyond finding exact values in sorted arrays to solving optimization problems where you search for the answer itself rather than searching in an array. The key insight is that if you can verify whether a candidate answer works in O(f(n)) time, and the answer space is monotonic (if k works, then k+1 also works or vice versa), you can binary search on the answer space in O(f(n) × log(range)) time. This pattern appears in problems like finding minimum capacity, maximum distance, or threshold values.",
      "Binary search on answer works by defining a range [low, high] for possible answers, then checking the middle value mid. You write a feasibility function that returns true if mid is a valid answer and false otherwise. If mid works, you know the optimal answer is at most mid, so you search [low, mid]. If mid doesn't work, the answer must be larger, so you search [mid+1, high]. The final answer is the smallest (or largest, depending on the problem) value that passes the feasibility check.",
      "Searching in rotated sorted arrays is a classic variation where a sorted array is rotated at an unknown pivot point, like [4,5,6,7,0,1,2]. The array still has sorted portions, and you can identify which half is properly sorted by comparing arr[left] with arr[mid]. If the left half is sorted (arr[left] <= arr[mid]), you check if the target lies within that sorted range. If yes, search there; otherwise, search the right half. This requires careful condition checking to avoid off-by-one errors and handle duplicates.",
      "Finding first and last occurrences of a target in a sorted array with duplicates requires two separate binary searches. For the first occurrence, when you find the target at mid, you don't return immediately but continue searching the left half to see if there's an earlier occurrence: right = mid - 1. For the last occurrence, you search the right half: left = mid + 1. The final answer for first occurrence is left, and for last occurrence is right. This pattern extends to finding lower_bound and upper_bound used in many competitive programming problems.",
      "Peak element problems ask you to find an element that is greater than its neighbors. Even though the array is not fully sorted, binary search works because you can make local decisions: if arr[mid] < arr[mid+1], a peak must exist in the right half (since the sequence is increasing). If arr[mid] > arr[mid+1], a peak must exist in the left half or at mid itself. This works even with multiple peaks because you only need to find one peak, not all of them.",
      "Searching in 2D sorted matrices can be done in O(log(m × n)) by treating the matrix as a flattened sorted array. Map 1D index to 2D coordinates: row = mid / n, col = mid % n. Alternatively, use a staircase search starting from top-right or bottom-left, moving left if the current element is too large or down if it's too small. This takes O(m + n) but is simpler to implement and works even when rows are sorted but columns are not.",
      "Real-world applications of advanced binary search include: allocating server resources (minimize maximum load), cutting materials to minimize waste (maximize minimum piece length), scheduling jobs with deadlines (minimize maximum completion time), and rate limiting (find maximum request rate that doesn't overload). Any optimization problem where you can verify a solution but not directly compute it may be solvable with binary search on the answer.",
      "Common pitfalls include forgetting to handle edge cases like empty arrays, single elements, or all duplicates. Also, be careful with the feasibility function: it must correctly implement the monotonic property. If the function is buggy, binary search will give wrong answers silently. Always test your feasibility function separately on boundary cases before integrating it with binary search. Trace through a small example by hand to verify correctness.",
      "When implementing these patterns, pay attention to whether you need to find minimum or maximum, and whether the condition is greater-than or greater-or-equal. These subtle differences change whether you update left = mid or left = mid + 1. A common template is: if (feasible(mid)) right = mid else left = mid + 1, with final answer being left. But always adapt the template to your specific problem rather than blindly copying it.",
      "Mastering advanced binary search requires practice on diverse problems: Koko eating bananas, capacity to ship packages, split array largest sum, and search in rotated arrays. These problems train you to recognize the binary search pattern even when it's hidden behind problem statements that don't explicitly mention searching. The ability to spot that a problem is 'searchable' is a key competitive programming skill that separates good coders from great ones."
    ],
    "example": {
      "language": "python",
      "code": "# Binary Search on Answer: Koko Eating Bananas\ndef minEatingSpeed(piles: list[int], h: int) -> int:\n    def canFinish(speed: int) -> bool:\n        hours = sum((pile + speed - 1) // speed for pile in piles)\n        return hours <= h\n    \n    left, right = 1, max(piles)\n    while left < right:\n        mid = left + (right - left) // 2\n        if canFinish(mid):\n            right = mid  # Try slower speed\n        else:\n            left = mid + 1  # Need faster speed\n    return left\n\n# Search in Rotated Sorted Array\ndef search_rotated(nums: list[int], target: int) -> int:\n    left, right = 0, len(nums) - 1\n    \n    while left <= right:\n        mid = left + (right - left) // 2\n        if nums[mid] == target:\n            return mid\n        \n        # Check which half is sorted\n        if nums[left] <= nums[mid]:  # Left half sorted\n            if nums[left] <= target < nums[mid]:\n                right = mid - 1\n            else:\n                left = mid + 1\n        else:  # Right half sorted\n            if nums[mid] < target <= nums[right]:\n                left = mid + 1\n            else:\n                right = mid - 1\n    return -1\n\nprint(minEatingSpeed([3,6,7,11], 8))  # 4\nprint(search_rotated([4,5,6,7,0,1,2], 0))  # 4",
      "input": "piles=[3,6,7,11], h=8 for Koko; nums=[4,5,6,7,0,1,2], target=0 for rotated"
    }
  },
  {
    "slug": "greedy-algorithms",
    "title": "Greedy Algorithms: Optimization Patterns",
    "summary": "Learn to solve optimization problems by making locally optimal choices that lead to globally optimal solutions.",
    "level": "Intermediate",
    "category": "Greedy",
    "content": [
      "Greedy algorithms solve optimization problems by making the best local choice at each step, hoping these local choices lead to a global optimum. Unlike dynamic programming which considers all possibilities, greedy algorithms commit to a choice immediately without reconsidering. This makes them fast (often O(n) or O(n log n)) but they only work for problems with specific properties: greedy choice property (local optimum leads to global optimum) and optimal substructure (optimal solution contains optimal solutions to subproblems).",
      "The classic example is the activity selection problem: given start and end times of activities, select the maximum number of non-overlapping activities. The greedy strategy is to always pick the activity that ends earliest, because this leaves the most room for future activities. Sort activities by end time, then iterate through and select each activity whose start time is after the previously selected activity's end time. This simple greedy approach is provably optimal and much faster than trying all subsets.",
      "Interval problems form a major category of greedy problems. In merge intervals, you sort intervals by start time, then merge overlapping ones by comparing the current interval's start with the previous interval's end. In meeting rooms, you check if all meetings can attend by sorting by start time and verifying no overlaps. In meeting rooms II (minimum rooms needed), you can use a greedy approach with a min-heap tracking end times of ongoing meetings, adding a room when a meeting starts before the earliest ending meeting finishes.",
      "Jump game problems test whether you can reach the end of an array where each element represents maximum jump length. The greedy approach maintains the farthest position reachable so far. Iterate through the array, and at each index i, update farthest = max(farthest, i + nums[i]). If at any point i > farthest, you're stuck and return false. If farthest >= last index, return true. For jump game II (minimum jumps), track the current jump boundary and increment jumps when you need to extend the boundary.",
      "Fractional knapsack allows taking fractions of items (unlike 0/1 knapsack which uses DP). The greedy strategy is to sort items by value-to-weight ratio and take items in that order until the capacity is full, possibly taking a fraction of the last item. This works because you can always replace a lower-ratio item fraction with a higher-ratio item fraction to improve the solution. However, this strategy fails for 0/1 knapsack where you can't take fractions, which is why that version requires dynamic programming.",
      "Huffman coding is a greedy algorithm for data compression. It builds an optimal prefix-free binary code by repeatedly combining the two least frequent symbols into a new symbol with their combined frequency, using a min-heap. The resulting binary tree assigns shorter codes to more frequent symbols, minimizing the average code length. This is used in file compression formats like ZIP and JPEG. The greedy choice (combining least frequent symbols) provably leads to the optimal encoding.",
      "Gas station problems ask if you can complete a circular route where each station has gas and the distance to the next station consumes gas. The greedy insight is: if the total gas is less than total cost, it's impossible. Otherwise, start from any station, track your current gas as you move, and if you run out at station i, restart from station i+1 because any station between start and i cannot be the starting point (if you reached i from start and ran out, starting earlier in that range would also fail). The first station where you don't run out is the answer.",
      "Real-world greedy algorithm applications include: task scheduling on processors (minimize idle time), Dijkstra's shortest path (greedily pick nearest unvisited node), Prim's minimum spanning tree (greedily add cheapest edge), load balancing (assign jobs to least loaded server), and caching strategies (evict least recently used item). Many operating system schedulers, network routing protocols, and resource allocators use greedy heuristics for efficiency even when optimal solutions are intractable.",
      "Proving greedy correctness requires showing that the greedy choice is safe (doesn't block future optimal choices) and that after making the greedy choice, the remaining problem has the same structure. Often you prove by exchange argument: assume an optimal solution differs from the greedy choice, then show you can exchange elements to transform it into the greedy solution without worsening it, contradicting optimality. Not all optimization problems have greedy solutions; recognizing when greedy works vs. when DP is needed is a crucial skill.",
      "Common mistakes include applying greedy to problems that need DP (like 0/1 knapsack or longest increasing subsequence), forgetting to sort the input when the greedy strategy depends on order, and choosing the wrong greedy criteria (e.g., picking shortest interval instead of earliest ending interval). Always question: does my greedy choice preserve optimality? Can I construct a counterexample where greedy fails? Test on edge cases like all elements equal, single element, or extreme values to validate your approach."
    ],
    "example": {
      "language": "python",
      "code": "# Activity Selection (Maximum non-overlapping intervals)\ndef max_meetings(start: list[int], end: list[int]) -> int:\n    # Sort by end time\n    meetings = sorted(zip(start, end), key=lambda x: x[1])\n    count = 0\n    last_end = 0\n    \n    for s, e in meetings:\n        if s >= last_end:\n            count += 1\n            last_end = e\n    \n    return count\n\n# Jump Game\ndef canJump(nums: list[int]) -> bool:\n    farthest = 0\n    for i in range(len(nums)):\n        if i > farthest:\n            return False\n        farthest = max(farthest, i + nums[i])\n        if farthest >= len(nums) - 1:\n            return True\n    return True\n\n# Gas Station\ndef canCompleteCircuit(gas: list[int], cost: list[int]) -> int:\n    if sum(gas) < sum(cost):\n        return -1\n    \n    start = 0\n    tank = 0\n    for i in range(len(gas)):\n        tank += gas[i] - cost[i]\n        if tank < 0:\n            start = i + 1\n            tank = 0\n    \n    return start\n\nprint(max_meetings([1,3,0,5,8,5], [2,4,6,7,9,9]))  # 4\nprint(canJump([2,3,1,1,4]))  # True\nprint(canCompleteCircuit([1,2,3,4,5], [3,4,5,1,2]))  # 3",
      "input": "Various greedy problems with optimal local choices"
    }
  },
  {
    "slug": "backtracking-basics",
    "title": "Backtracking: Exploring All Possibilities",
    "summary": "Master the technique of exploring all possible solutions by building candidates incrementally and abandoning them when they fail constraints.",
    "level": "Intermediate",
    "category": "Backtracking",
    "content": [
      "Backtracking is a general algorithmic technique for finding all (or some) solutions to computational problems by incrementally building candidates and abandoning a candidate (backtracking) as soon as it determines the candidate cannot lead to a valid solution. It is essentially a depth-first search of the solution space with pruning. Backtracking is used when you need to explore all possibilities but can eliminate many branches early by checking constraints, making it much faster than brute force.",
      "The template for backtracking involves three components: a choice at each step, constraints that define valid choices, and a goal that defines a complete solution. The recursive structure is: if goal reached, save solution; otherwise, for each valid choice, make the choice, recurse to the next step, then undo the choice (backtrack). This undo step is crucial—it restores the state so you can explore the next choice. Without proper backtracking, you would be exploring with corrupted state.",
      "Subsets (generating all subsets of a set) is the simplest backtracking problem. At each element, you have two choices: include it or exclude it. The recursion tree has 2^n leaves, one for each subset. The template is: at position i, you either add nums[i] to the current subset and recurse to i+1, or skip nums[i] and recurse to i+1. When i reaches n, you've made a choice for all elements, so you save the current subset. This generates all 2^n subsets in O(n × 2^n) time (n to copy each subset).",
      "Permutations (all arrangements of elements) requires tracking which elements have been used. A common approach is to maintain a used array, and at each recursion level, try adding each unused element, mark it as used, recurse, then unmark it. Alternatively, use a swap-based approach: swap the current position with each position from current to end, recurse, then swap back. Both generate all n! permutations in O(n × n!) time. Permutations with duplicates require sorting and skipping duplicate choices to avoid generating the same permutation multiple times.",
      "Combinations (choose k elements from n) can be solved by backtracking with a start index to avoid choosing the same element twice and to ensure combinations rather than permutations. At each level, try adding elements from start to n, recurse with start+1, then remove the element. When the current combination has k elements, save it. This generates C(n, k) combinations. Combination sum problems add a target constraint: only recurse if the current sum doesn't exceed the target, and you can reuse elements by recursing with the same index.",
      "N-Queens places n queens on an n×n chessboard such that no two queens attack each other. For each row, try placing a queen in each column, check if it's safe (no other queen in the same column, diagonal, or anti-diagonal), recurse to the next row, then remove the queen. The constraint checking is the key optimization: maintain sets of occupied columns, diagonals (row - col), and anti-diagonals (row + col) for O(1) conflict checking. Without these, checking safety would be O(n) making the algorithm much slower.",
      "Sudoku solver fills a 9×9 grid following Sudoku rules. For each empty cell, try digits 1-9, check if the digit is valid in the current row, column, and 3×3 box, place it, recurse to the next empty cell, and backtrack if the recursion fails. The key optimization is choosing the order of cells to fill: filling cells with fewer possibilities first (most constrained first) prunes the search tree more effectively. Some implementations precompute possible digits for each cell to speed this up.",
      "Word search in a 2D grid checks if a word exists by trying to form it starting from each cell. For each cell, if it matches the first letter, mark it as visited, recursively search the four neighbors for the next letter, then unmark it. The visited marking prevents using the same cell twice in one path. This is backtracking because you undo the visited mark after the recursion returns, allowing the cell to be used in different paths. Time complexity is O(m × n × 4^L) where L is the word length, but pruning reduces this significantly in practice.",
      "Backtracking optimization techniques include: pruning (stop early when constraints violated), choosing the most constrained variable first, memoization (cache results of subproblems if they repeat), and iterative deepening (limit recursion depth and increase gradually). For combinatorial problems, the search space grows exponentially, so even small optimizations can make the difference between feasible and infeasible runtime. Always look for opportunities to prune: if the current partial solution can't possibly lead to a valid complete solution, backtrack immediately.",
      "Common mistakes in backtracking include forgetting to undo choices (causing incorrect state in later branches), not handling base cases properly (leading to infinite recursion or missing solutions), and inefficient constraint checking (making the algorithm too slow). Always test on small inputs where you can manually verify all solutions. Backtracking is a powerful technique, but it's also easy to get wrong, so careful implementation and testing are essential. Visualizing the recursion tree helps understand the flow and debug issues."
    ],
    "example": {
      "language": "python",
      "code": "# Subsets\ndef subsets(nums: list[int]) -> list[list[int]]:\n    result = []\n    def backtrack(start: int, path: list[int]):\n        result.append(path[:])\n        for i in range(start, len(nums)):\n            path.append(nums[i])\n            backtrack(i + 1, path)\n            path.pop()\n    backtrack(0, [])\n    return result\n\n# Permutations\ndef permute(nums: list[int]) -> list[list[int]]:\n    result = []\n    def backtrack(path: list[int], remaining: list[int]):\n        if not remaining:\n            result.append(path[:])\n            return\n        for i in range(len(remaining)):\n            backtrack(path + [remaining[i]], remaining[:i] + remaining[i+1:])\n    backtrack([], nums)\n    return result\n\n# Combinations\ndef combine(n: int, k: int) -> list[list[int]]:\n    result = []\n    def backtrack(start: int, path: list[int]):\n        if len(path) == k:\n            result.append(path[:])\n            return\n        for i in range(start, n + 1):\n            path.append(i)\n            backtrack(i + 1, path)\n            path.pop()\n    backtrack(1, [])\n    return result\n\nprint(subsets([1,2,3]))  # [[], [1], [2], [1,2], [3], [1,3], [2,3], [1,2,3]]\nprint(permute([1,2,3]))  # [[1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]]\nprint(combine(4, 2))  # [[1,2], [1,3], [1,4], [2,3], [2,4], [3,4]]",
      "input": "nums=[1,2,3] for subsets/permute, n=4, k=2 for combinations"
    }
  },
  {
    "slug": "bit-manipulation-basics",
    "title": "Bit Manipulation: Binary Operations & Tricks",
    "summary": "Master bitwise operators and bit tricks to solve problems efficiently using binary representations.",
    "level": "Intermediate",
    "category": "Bit Manipulation",
    "content": [
      "Bit manipulation uses bitwise operators to perform operations on individual bits of numbers. Computers store data in binary, so bit-level operations are extremely fast. Common bitwise operators are AND (&), OR (|), XOR (^), NOT (~), left shift (<<), and right shift (>>). Understanding these operators and recognizing when to use them can lead to elegant solutions that are faster and use less memory than conventional approaches.",
      "The XOR operator (^) has useful properties: x ^ 0 = x, x ^ x = 0, and XOR is commutative and associative. These properties make XOR perfect for finding elements that appear an odd number of times. For example, to find the single number in an array where every other number appears twice, XOR all numbers together: the pairs cancel out, leaving only the single number. This works in O(n) time and O(1) space, much better than using a hash map.",
      "Bit masks allow you to represent a set using an integer where each bit indicates whether an element is present. For example, the set {0, 2, 5} can be represented as the binary number 100101 (bits 0, 2, and 5 are set). You can add element i with mask |= (1 << i), remove it with mask &= ~(1 << i), check if it's present with (mask & (1 << i)) != 0, and toggle it with mask ^= (1 << i). This is space-efficient for small sets and enables fast subset enumeration.",
      "Generating all subsets using bit manipulation is elegant: for a set of n elements, there are 2^n subsets, which can be represented by integers from 0 to 2^n - 1. For each integer i, the bits of i indicate which elements are in the subset. Iterate i from 0 to 2^n - 1, and for each i, check each bit j to see if element j is included. This is equivalent to backtracking but uses iteration instead of recursion, and it's easier to understand once you grasp the bit pattern concept.",
      "Counting bits (number of 1s in binary representation) is a common operation. The naive approach checks each bit with (n & (1 << i)). A faster trick is Brian Kernighan's algorithm: n & (n - 1) clears the rightmost 1 bit. Repeat until n becomes 0, counting the iterations. For example, 12 (1100) & 11 (1011) = 8 (1000), then 8 & 7 = 0. This takes O(number of 1s) instead of O(log n). Modern CPUs have built-in popcount instructions that count bits in O(1).",
      "Checking if a number is a power of two can be done with a single bit trick: n > 0 && (n & (n - 1)) == 0. Powers of two have exactly one 1 bit in binary (1, 10, 100, 1000, ...). Subtracting 1 flips all bits after that 1 bit, so ANDing them gives 0. For example, 8 (1000) & 7 (0111) = 0. This is much faster than using logarithms or division. Similarly, checking if a number is a power of four can be done with additional checks on bit positions.",
      "Swapping two numbers without a temporary variable uses XOR: a ^= b, b ^= a, a ^= b. After the first operation, a = a ^ b. After the second, b = (a ^ b) ^ b = a. After the third, a = (a ^ b) ^ a = b. This is a classic bit trick, though modern compilers optimize regular swaps well, so it's more of a curiosity than a practical optimization. However, XOR swap is useful in certain scenarios like swapping adjacent elements in an array in-place.",
      "Real-world applications include: network subnet masks (IP address ranges), permission systems (each bit represents a permission), graphics (color manipulation, alpha blending), compression (Huffman coding uses bit streams), cryptography (XOR in stream ciphers), and databases (bitmap indexes for fast queries). Many low-level systems and performance-critical code use bit manipulation extensively. Understanding bits is essential for systems programming and competitive programming.",
      "Advanced bit tricks include: finding the rightmost 1 bit with n & -n (two's complement makes -n flip all bits and add 1), isolating bits with masks, and using bit DP for subset problems (like traveling salesman with bitmask DP). These techniques are not just for puzzles; they appear in real algorithms like hash functions, Bloom filters, and efficient set operations. Mastering bit manipulation opens up a new dimension of problem-solving.",
      "Common mistakes include sign issues (right shift behaves differently for signed vs unsigned), off-by-one errors in bit positions (forgetting bits are 0-indexed), and incorrect precedence (& has lower precedence than ==, so (n & 1 == 0) is parsed as (n & (1 == 0)) which is wrong; use ((n & 1) == 0)). Always use parentheses to clarify bitwise expressions. Test bit manipulation code with small numbers where you can manually verify the binary representation."
    ],
    "example": {
      "language": "python",
      "code": "# Single Number (XOR trick)\ndef singleNumber(nums: list[int]) -> int:\n    result = 0\n    for num in nums:\n        result ^= num\n    return result\n\n# Power of Two\ndef isPowerOfTwo(n: int) -> bool:\n    return n > 0 and (n & (n - 1)) == 0\n\n# Count Bits (Hamming Weight)\ndef hammingWeight(n: int) -> int:\n    count = 0\n    while n:\n        n &= (n - 1)  # Clear rightmost 1 bit\n        count += 1\n    return count\n\n# Subsets using bitmask\ndef subsetsWithBits(nums: list[int]) -> list[list[int]]:\n    n = len(nums)\n    result = []\n    for mask in range(1 << n):  # 2^n subsets\n        subset = []\n        for i in range(n):\n            if mask & (1 << i):  # Check if bit i is set\n                subset.append(nums[i])\n        result.append(subset)\n    return result\n\nprint(singleNumber([4,1,2,1,2]))  # 4\nprint(isPowerOfTwo(16))  # True\nprint(hammingWeight(11))  # 3 (1011 has three 1s)\nprint(subsetsWithBits([1,2]))  # [[], [1], [2], [1,2]]",
      "input": "Various bit manipulation examples"
    }
  }
















]
