[
  {
    "slug": "array-basics",
    "title": "Arrays: Introduction",
    "summary": "Understand arrays, indexing, and basic operations with detailed examples.",
    "level": "Beginner",
    "category": "Arrays",
    "content": [
      "An array is a collection of elements stored at contiguous memory locations. Each element can be accessed directly using its index (position).",
      "Arrays support O(1) random access, meaning you can instantly jump to any position if you know the index.",
      "Common operations: insertion at the end is O(1), insertion/deletion in the middle is O(n) because elements must shift.",
      "Example: Store exam scores [85, 92, 78, 95, 88]. Access the third score with scores[2] = 78.",
      "Key insight: Arrays are fixed-size in many languages (C/C++/Java), but Python lists and JavaScript arrays grow dynamically."
    ],
    "example": {
      "language": "python",
      "code": "# Array basics in Python\nscores = [85, 92, 78, 95, 88]\n\n# Access by index (0-based)\nprint(f\"First score: {scores[0]}\")  # 85\nprint(f\"Third score: {scores[2]}\")  # 78\n\n# Modify element\nscores[1] = 90\nprint(f\"Updated scores: {scores}\")  # [85, 90, 78, 95, 88]\n\n# Append to end (O(1) average)\nscores.append(100)\nprint(f\"After append: {scores}\")\n\n# Insert in middle (O(n))\nscores.insert(2, 88)\nprint(f\"After insert: {scores}\")\n\n# Find length\nprint(f\"Total scores: {len(scores)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "array-two-pointer",
    "title": "Two Pointer Technique",
    "summary": "Learn how to use two pointers to solve array problems efficiently.",
    "level": "Intermediate",
    "category": "Arrays",
    "content": [
      "The two-pointer technique uses two indices that traverse the array from different directions or speeds.",
      "Pattern 1 (Opposite ends): One pointer starts at index 0, another at the last index. Move them toward each other.",
      "Pattern 2 (Same direction): Both pointers start at the beginning but move at different speeds (fast and slow).",
      "Example problem: Reverse an array in-place without extra space. Use left pointer at start, right pointer at end, swap and move inward.",
      "Time complexity: O(n) with O(1) space, making it efficient for in-place solutions."
    ],
    "example": {
      "language": "python",
      "code": "# Example 1: Reverse array in-place\ndef reverse_array(arr):\n    left, right = 0, len(arr) - 1\n    while left < right:\n        # Swap elements\n        arr[left], arr[right] = arr[right], arr[left]\n        left += 1\n        right -= 1\n    return arr\n\ntest1 = [1, 2, 3, 4, 5]\nprint(f\"Original: {test1}\")\nprint(f\"Reversed: {reverse_array(test1)}\")\n\n# Example 2: Check if palindrome\ndef is_palindrome(arr):\n    left, right = 0, len(arr) - 1\n    while left < right:\n        if arr[left] != arr[right]:\n            return False\n        left += 1\n        right -= 1\n    return True\n\ntest2 = [1, 2, 3, 2, 1]\nprint(f\"\\nIs palindrome: {is_palindrome(test2)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "strings-basics",
    "title": "String Manipulation",
    "summary": "Learn string operations, reversal, palindrome checking, and common patterns.",
    "level": "Beginner",
    "category": "Strings",
    "content": [
      "Strings are sequences of characters. In Python and Java, strings are immutable (cannot be changed after creation).",
      "Common operations: length, concatenation, substring extraction, character access, case conversion.",
      "Pattern matching: Check palindromes (reads same forwards/backwards like 'racecar'), anagrams (same letters different order like 'listen' and 'silent').",
      "Example: Check if 'madam' is a palindrome - compare first and last chars, then second and second-last, etc.",
      "Tip: Two-pointer technique works great for string problems too!"
    ],
    "example": {
      "language": "python",
      "code": "# String basics\ntext = \"Hello World\"\n\n# Length and access\nprint(f\"Length: {len(text)}\")\nprint(f\"First char: {text[0]}\")\nprint(f\"Last char: {text[-1]}\")\n\n# Substring\nprint(f\"Substring [0:5]: {text[0:5]}\")  # Hello\n\n# Case conversion\nprint(f\"Upper: {text.upper()}\")\nprint(f\"Lower: {text.lower()}\")\n\n# Check palindrome\ndef is_palindrome(s):\n    # Remove spaces and convert to lowercase\n    s = s.replace(\" \", \"\").lower()\n    left, right = 0, len(s) - 1\n    while left < right:\n        if s[left] != s[right]:\n            return False\n        left += 1\n        right -= 1\n    return True\n\nprint(f\"\\n'racecar' is palindrome: {is_palindrome('racecar')}\")\nprint(f\"'hello' is palindrome: {is_palindrome('hello')}\")\n",
      "input": ""
    }
  },
  {
    "slug": "bubble-sort",
    "title": "Bubble Sort",
    "summary": "Learn the simplest sorting algorithm that repeatedly swaps adjacent elements.",
    "level": "Beginner",
    "category": "Sorting",
    "content": [
      "Bubble Sort repeatedly steps through the list, compares adjacent elements, and swaps them if they're in the wrong order.",
      "The algorithm gets its name because smaller elements 'bubble' to the top (beginning) of the list.",
      "Pass 1: Compare each pair and swap if needed. Largest element reaches the end.",
      "Pass 2: Repeat but ignore the last position (already sorted). Second largest reaches second-last position.",
      "Time Complexity: O(n²) in worst and average cases, O(n) if already sorted with optimization.",
      "Space: O(1) - sorts in place.",
      "Example: [5, 1, 4, 2, 8] → Compare 5&1 (swap) → [1, 5, 4, 2, 8] → Compare 5&4 (swap) → continue..."
    ],
    "example": {
      "language": "python",
      "code": "# Bubble Sort Implementation\ndef bubble_sort(arr):\n    n = len(arr)\n    \n    # Traverse through all array elements\n    for i in range(n):\n        # Track if any swap happened\n        swapped = False\n        \n        # Last i elements are already sorted\n        for j in range(0, n - i - 1):\n            # Swap if current element > next element\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n                swapped = True\n                print(f\"Swapped {arr[j+1]} and {arr[j]}: {arr}\")\n        \n        # If no swaps, array is sorted\n        if not swapped:\n            break\n    \n    return arr\n\n# Example\ntest = [64, 34, 25, 12, 22, 11, 90]\nprint(f\"Original: {test}\")\nprint(\"\\nSorting process:\")\nresult = bubble_sort(test.copy())\nprint(f\"\\nSorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "selection-sort",
    "title": "Selection Sort",
    "summary": "Find the minimum element and place it at the beginning repeatedly.",
    "level": "Beginner",
    "category": "Sorting",
    "content": [
      "Selection Sort divides the array into sorted and unsorted parts. It repeatedly selects the smallest element from the unsorted part and moves it to the sorted part.",
      "Step 1: Find the minimum element in the entire array and swap it with the first element.",
      "Step 2: Find the minimum in the remaining unsorted array and swap with the second position.",
      "Repeat until the entire array is sorted.",
      "Time Complexity: O(n²) in all cases - always makes n² comparisons.",
      "Space: O(1) - in-place sorting.",
      "Example: [29, 10, 14, 37] → Find min=10, swap with 29 → [10, 29, 14, 37] → Find min=14, swap with 29 → [10, 14, 29, 37]"
    ],
    "example": {
      "language": "python",
      "code": "# Selection Sort Implementation\ndef selection_sort(arr):\n    n = len(arr)\n    \n    # Traverse through all array elements\n    for i in range(n):\n        # Find minimum element in remaining unsorted array\n        min_idx = i\n        for j in range(i + 1, n):\n            if arr[j] < arr[min_idx]:\n                min_idx = j\n        \n        # Swap the found minimum with first element\n        if min_idx != i:\n            arr[i], arr[min_idx] = arr[min_idx], arr[i]\n            print(f\"Swapped {arr[i]} with {arr[min_idx]} (position {i}): {arr}\")\n    \n    return arr\n\n# Example\ntest = [64, 25, 12, 22, 11]\nprint(f\"Original: {test}\")\nprint(\"\\nSorting process:\")\nresult = selection_sort(test.copy())\nprint(f\"\\nSorted: {result}\")\n\n# Selection sort makes fewer swaps than bubble sort\nprint(\"\\nNote: Selection sort minimizes the number of swaps!\")\n",
      "input": ""
    }
  },
  {
    "slug": "insertion-sort",
    "title": "Insertion Sort",
    "summary": "Build the sorted array one element at a time by inserting elements in correct position.",
    "level": "Beginner",
    "category": "Sorting",
    "content": [
      "Insertion Sort builds the final sorted array one item at a time. It's similar to how you sort playing cards in your hands.",
      "Start with the second element. Compare it with elements in the sorted part (left side) and insert it at the correct position.",
      "Shift all larger elements one position to the right to make space for the inserted element.",
      "Example: Like arranging cards - pick a card, compare with cards in hand, insert at right spot.",
      "Time Complexity: O(n²) worst case, O(n) best case (already sorted), good for small datasets.",
      "Space: O(1) - in-place sorting.",
      "Practical use: Efficient for small arrays or nearly sorted arrays."
    ],
    "example": {
      "language": "python",
      "code": "# Insertion Sort Implementation\ndef insertion_sort(arr):\n    n = len(arr)\n    \n    # Start from second element\n    for i in range(1, n):\n        key = arr[i]  # Element to be inserted\n        j = i - 1\n        \n        print(f\"\\nInserting {key}:\")\n        \n        # Move elements greater than key one position ahead\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n            print(f\"  Shifted: {arr}\")\n        \n        # Insert key at correct position\n        arr[j + 1] = key\n        print(f\"  Final: {arr}\")\n    \n    return arr\n\n# Example\ntest = [12, 11, 13, 5, 6]\nprint(f\"Original: {test}\")\nresult = insertion_sort(test.copy())\nprint(f\"\\nSorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "merge-sort",
    "title": "Merge Sort",
    "summary": "Efficient divide-and-conquer algorithm that splits, sorts, and merges arrays.",
    "level": "Intermediate",
    "category": "Sorting",
    "content": [
      "Merge Sort uses divide-and-conquer strategy: divide array into halves, sort each half, then merge them.",
      "Step 1 (Divide): Split array into two halves recursively until each subarray has one element.",
      "Step 2 (Conquer): Merge the subarrays back together in sorted order.",
      "Merging: Compare first elements of both subarrays, pick smaller one, repeat until all elements are merged.",
      "Time Complexity: O(n log n) in all cases - much better than O(n²) algorithms!",
      "Space: O(n) - needs extra space for merging.",
      "Example: [38,27,43,3] → [38,27] & [43,3] → [38],[27] & [43],[3] → merge → [27,38] & [3,43] → [3,27,38,43]"
    ],
    "example": {
      "language": "python",
      "code": "# Merge Sort Implementation\ndef merge_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    # Divide array into two halves\n    mid = len(arr) // 2\n    left = arr[:mid]\n    right = arr[mid:]\n    \n    print(f\"Splitting: {arr} → {left} | {right}\")\n    \n    # Recursively sort both halves\n    left = merge_sort(left)\n    right = merge_sort(right)\n    \n    # Merge sorted halves\n    return merge(left, right)\n\ndef merge(left, right):\n    result = []\n    i = j = 0\n    \n    # Compare and merge\n    while i < len(left) and j < len(right):\n        if left[i] <= right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    # Add remaining elements\n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    print(f\"Merging {left} + {right} = {result}\")\n    return result\n\n# Example\ntest = [38, 27, 43, 3, 9, 82, 10]\nprint(f\"Original: {test}\\n\")\nresult = merge_sort(test.copy())\nprint(f\"\\nFinal Sorted: {result}\")\n",
      "input": ""
    }
  },
  {
    "slug": "quick-sort",
    "title": "Quick Sort",
    "summary": "Fast divide-and-conquer algorithm using pivot partitioning.",
    "level": "Intermediate",
    "category": "Sorting",
    "content": [
      "Quick Sort picks a 'pivot' element and partitions the array around it: smaller elements go left, larger go right.",
      "Step 1: Choose a pivot (commonly last element, first element, or random).",
      "Step 2: Partition array so elements < pivot are on left, elements > pivot are on right.",
      "Step 3: Recursively apply quick sort to left and right partitions.",
      "Time Complexity: O(n log n) average case, O(n²) worst case (rare with good pivot selection).",
      "Space: O(log n) for recursion stack.",
      "Example: [10,7,8,9,1,5] pivot=5 → [1] 5 [10,7,8,9] → recursively sort both sides."
    ],
    "example": {
      "language": "python",
      "code": "# Quick Sort Implementation\ndef quick_sort(arr, low=0, high=None):\n    if high is None:\n        high = len(arr) - 1\n    \n    if low < high:\n        # Partition and get pivot index\n        pi = partition(arr, low, high)\n        \n        # Recursively sort elements before and after partition\n        quick_sort(arr, low, pi - 1)\n        quick_sort(arr, pi + 1, high)\n    \n    return arr\n\ndef partition(arr, low, high):\n    # Choose rightmost element as pivot\n    pivot = arr[high]\n    i = low - 1  # Index of smaller element\n    \n    print(f\"\\nPartitioning {arr[low:high+1]} with pivot={pivot}\")\n    \n    for j in range(low, high):\n        # If current element <= pivot\n        if arr[j] <= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    \n    # Place pivot in correct position\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    print(f\"After partition: {arr}\")\n    \n    return i + 1\n\n# Example\ntest = [10, 7, 8, 9, 1, 5]\nprint(f\"Original: {test}\")\nresult = quick_sort(test.copy())\nprint(f\"\\nFinal Sorted: {result}\")\n",
      "input": ""
    }
  },
    {
    "slug": "binary-search-basics",
    "title": "Binary Search: Introduction",
    "summary": "Learn the fundamental technique to search in sorted arrays with O(log n) time complexity.",
    "level": "Beginner",
    "category": "Binary Search",
    "content": [
      "Binary Search is a highly efficient algorithm for finding a target value in a sorted array. Unlike linear search which checks each element one by one taking O(n) time, binary search repeatedly divides the search space in half, achieving O(log n) time complexity. This means searching in an array of 1 million elements takes only about 20 comparisons instead of potentially 1 million. The key requirement is that the array must be sorted beforehand.",
      "The algorithm works by maintaining three pointers: left, right, and mid. Initially, left points to the first index and right to the last. In each iteration, you calculate mid as the middle index between left and right, then compare the element at mid with your target. If they match, you found the answer. If the target is smaller, you know it must be in the left half, so you move right to mid - 1. If the target is larger, it must be in the right half, so you move left to mid + 1. This process continues until you find the target or the search space becomes empty.",
      "A common pitfall is calculating the middle index. Using mid = (left + right) / 2 can cause integer overflow when left and right are large numbers. The safer formula is mid = left + (right - left) / 2, which avoids overflow. Another critical detail is the loop condition: while (left <= right) ensures you don't miss the case where the target is at the exact position where left and right meet. If you use while (left < right), you might exit one iteration too early.",
      "Binary search returns different values based on the problem. For finding exact matches, return the index when found or -1 if not found. For finding insertion positions (where to insert a target in sorted order), return left after the loop ends. For finding first/last occurrence of duplicates, you modify the algorithm to continue searching even after finding a match. These variations make binary search versatile for many problems beyond simple searching.",
      "Real-world applications include searching in databases (indexed columns use binary search trees), dictionaries and phone books (alphabetically sorted), version control systems (finding when a bug was introduced by searching through commits), and even optimizing machine learning hyperparameters (searching for the best learning rate). Any time you have sorted data and need fast lookup, binary search is the go-to technique.",
      "To master binary search, practice on arrays with odd and even lengths, arrays with duplicate elements, and edge cases like single-element arrays or searching for values outside the array bounds. Visualizing the search space shrinking helps build intuition. Always verify your loop invariants: what does left represent? What does right represent? Are they inclusive or exclusive bounds? Clear thinking about these prevents off-by-one errors."
    ],
    "example": {
      "language": "python",
      "code": "def binary_search(arr: list[int], target: int) -> int:\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = left + (right - left) // 2\n        \n        if arr[mid] == target:\n            return mid  # Found target\n        elif arr[mid] < target:\n            left = mid + 1  # Search right half\n        else:\n            right = mid - 1  # Search left half\n    \n    return -1  # Target not found\n\n# Examples\narr = [1, 3, 5, 7, 9, 11, 13, 15]\nprint(binary_search(arr, 7))   # 3\nprint(binary_search(arr, 6))   # -1\nprint(binary_search(arr, 1))   # 0\nprint(binary_search(arr, 15))  # 7",
      "input": "arr = [1, 3, 5, 7, 9, 11, 13, 15], target = 7"
    }
  },

  {
    "slug": "linked-list-basics",
    "title": "Linked Lists: Introduction",
    "summary": "Understand nodes, pointers, and basic linked list operations.",
    "level": "Beginner",
    "category": "Linked Lists",
    "content": [
      "A linked list is a linear data structure where each element (node) contains data and a reference (pointer) to the next node.",
      "Unlike arrays, linked lists don't need contiguous memory. Nodes can be scattered in memory.",
      "Advantages: Dynamic size, efficient insertions/deletions at O(1) if you have the pointer.",
      "Disadvantages: No random access (must traverse from head), extra memory for pointers.",
      "Types: Singly linked list (next pointer only), doubly linked list (next + previous pointers), circular linked list (last node points to first).",
      "Example: Think of a treasure hunt - each clue (node) points to the next location."
    ],
    "example": {
      "language": "python",
      "code": "# Linked List Implementation\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n    \n    def append(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n            return\n        \n        current = self.head\n        while current.next:\n            current = current.next\n        current.next = new_node\n    \n    def display(self):\n        elements = []\n        current = self.head\n        while current:\n            elements.append(str(current.data))\n            current = current.next\n        return \" -> \".join(elements) + \" -> None\"\n\n# Example usage\nll = LinkedList()\nll.append(1)\nll.append(2)\nll.append(3)\nll.append(4)\n\nprint(\"Linked List:\")\nprint(ll.display())\n\nprint(\"\\nEach node points to the next, forming a chain!\")\n",
      "input": ""
    }
  },
  {
    "slug": "stack-basics",
    "title": "Stacks: LIFO Structure",
    "summary": "Master stack operations and solve parentheses matching problems.",
    "level": "Beginner",
    "category": "Stacks",
    "content": [
      "A stack follows Last In First Out (LIFO): the last element added is the first to be removed.",
      "Think of a stack of plates - you can only add or remove from the top.",
      "Core operations: push (add to top), pop (remove from top), peek (view top without removing), isEmpty.",
      "Use cases: Function call stack, undo/redo mechanisms, browser back button, expression evaluation, backtracking algorithms.",
      "Common problem: Check if parentheses are balanced - '(())' is valid, '(()' is not.",
      "Implementation: Can use arrays (Python list) or linked lists."
    ],
    "example": {
      "language": "python",
      "code": "# Stack Implementation\nclass Stack:\n    def __init__(self):\n        self.items = []\n    \n    def push(self, item):\n        self.items.append(item)\n        print(f\"Pushed {item}: {self.items}\")\n    \n    def pop(self):\n        if not self.is_empty():\n            item = self.items.pop()\n            print(f\"Popped {item}: {self.items}\")\n            return item\n        return None\n    \n    def peek(self):\n        return self.items[-1] if not self.is_empty() else None\n    \n    def is_empty(self):\n        return len(self.items) == 0\n\n# Example: Valid Parentheses\ndef is_valid_parentheses(s):\n    stack = []\n    mapping = {')': '(', '}': '{', ']': '['}\n    \n    for char in s:\n        if char in mapping:\n            top = stack.pop() if stack else '#'\n            if mapping[char] != top:\n                return False\n        else:\n            stack.append(char)\n    \n    return not stack\n\nprint(\"Stack Demo:\")\ns = Stack()\ns.push(10)\ns.push(20)\ns.push(30)\ns.pop()\n\nprint(\"\\nValid Parentheses Check:\")\nprint(f\"'()[]{{}}' is valid: {is_valid_parentheses('()[]{}')}\")\nprint(f\"'(]' is valid: {is_valid_parentheses('(]')}\")\n",
      "input": ""
    }
  },
  {
    "slug": "queue-basics",
    "title": "Queues: FIFO Structure",
    "summary": "Learn queue operations and understand First In First Out behavior.",
    "level": "Beginner",
    "category": "Queues",
    "content": [
      "A queue follows First In First Out (FIFO): the first element added is the first to be removed.",
      "Think of a line at a ticket counter - first person in line gets served first.",
      "Core operations: enqueue (add to rear), dequeue (remove from front), peek (view front), isEmpty.",
      "Use cases: Task scheduling, breadth-first search (BFS), printer queue, handling requests in web servers.",
      "Types: Simple queue, circular queue (wraps around), priority queue (elements have priorities), deque (double-ended queue).",
      "Implementation: Can use arrays with two pointers (front and rear) or linked lists."
    ],
    "example": {
      "language": "python",
      "code": "# Queue Implementation\nfrom collections import deque\n\nclass Queue:\n    def __init__(self):\n        self.items = deque()\n    \n    def enqueue(self, item):\n        self.items.append(item)\n        print(f\"Enqueued {item}: {list(self.items)}\")\n    \n    def dequeue(self):\n        if not self.is_empty():\n            item = self.items.popleft()\n            print(f\"Dequeued {item}: {list(self.items)}\")\n            return item\n        return None\n    \n    def peek(self):\n        return self.items[0] if not self.is_empty() else None\n    \n    def is_empty(self):\n        return len(self.items) == 0\n    \n    def size(self):\n        return len(self.items)\n\n# Example usage\nprint(\"Queue Demo:\")\nq = Queue()\nq.enqueue(\"Customer 1\")\nq.enqueue(\"Customer 2\")\nq.enqueue(\"Customer 3\")\n\nprint(\"\\nServing customers:\")\nq.dequeue()  # Customer 1 served first\nq.dequeue()  # Customer 2 served next\n\nprint(\"\\nNext in line:\")\nprint(f\"Front: {q.peek()}\")\n",
      "input": ""
    }
  },
  {
    "slug": "hashmap-basics",
    "title": "Hash Maps: Key-Value Storage",
    "summary": "Master hash tables for O(1) lookups, counting, and frequency problems.",
    "level": "Beginner",
    "category": "Hash Maps",
    "content": [
      "A hash map (or dictionary) stores key-value pairs and allows O(1) average-time insertion, deletion, and lookup.",
      "How it works: A hash function converts keys into array indices. Values are stored at those indices.",
      "Example: Store student grades - {'Alice': 95, 'Bob': 87} - access Alice's grade instantly.",
      "Use cases: Counting frequencies, finding duplicates, caching results, implementing sets.",
      "Common problems: Two Sum (find pairs that sum to target), anagram detection, frequency counter.",
      "Collision handling: When two keys hash to same index, use chaining (linked lists) or open addressing."
    ],
    "example": {
      "language": "python",
      "code": "# Hash Map Basics\n\n# Example 1: Count character frequency\ndef count_chars(s):\n    freq = {}\n    for char in s:\n        freq[char] = freq.get(char, 0) + 1\n    return freq\n\ntext = \"hello world\"\nprint(f\"Character frequencies in '{text}':\")\nprint(count_chars(text))\n\n# Example 2: Find first non-repeating character\ndef first_unique_char(s):\n    freq = count_chars(s)\n    for i, char in enumerate(s):\n        if freq[char] == 1:\n            return i\n    return -1\n\ntest = \"leetcode\"\nprint(f\"\\nFirst unique char in '{test}':\")\nindex = first_unique_char(test)\nprint(f\"Index: {index}, Character: '{test[index]}'\")\n\n# Example 3: Two Sum using hash map\ndef two_sum(nums, target):\n    seen = {}\n    for i, num in enumerate(nums):\n        complement = target - num\n        if complement in seen:\n            return [seen[complement], i]\n        seen[num] = i\n    return []\n\nnums = [2, 7, 11, 15]\ntarget = 9\nprint(f\"\\nTwo Sum: {nums}, target={target}\")\nprint(f\"Indices: {two_sum(nums, target)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "heap-basics",
    "title": "Heaps: Priority Queue",
    "summary": "Understand min-heap and max-heap for efficient priority-based operations.",
    "level": "Intermediate",
    "category": "Heaps",
    "content": [
      "A heap is a complete binary tree where each parent node has a specific relationship with its children.",
      "Max-Heap: Parent node is always greater than or equal to children. Root is the maximum element.",
      "Min-Heap: Parent node is always less than or equal to children. Root is the minimum element.",
      "Core operations: insert O(log n), extract-min/max O(log n), peek O(1).",
      "Use cases: Priority queues, heap sort, finding kth largest/smallest element, median maintenance.",
      "Array representation: For node at index i, left child at 2i+1, right child at 2i+2, parent at (i-1)/2.",
      "Example: Hospital emergency room - critical patients (high priority) treated before minor cases."
    ],
    "example": {
      "language": "python",
      "code": "# Min-Heap Implementation using Python's heapq\nimport heapq\n\nclass MinHeap:\n    def __init__(self):\n        self.heap = []\n    \n    def push(self, val):\n        heapq.heappush(self.heap, val)\n        print(f\"Inserted {val}: {self.heap}\")\n    \n    def pop(self):\n        if self.heap:\n            val = heapq.heappop(self.heap)\n            print(f\"Extracted min {val}: {self.heap}\")\n            return val\n        return None\n    \n    def peek(self):\n        return self.heap[0] if self.heap else None\n\n# Example: Priority Queue\nprint(\"Min-Heap Demo:\")\nheap = MinHeap()\n\n# Insert elements\nheap.push(10)\nheap.push(5)\nheap.push(20)\nheap.push(1)\n\nprint(\"\\nExtracting minimum elements:\")\nheap.pop()  # Removes 1\nheap.pop()  # Removes 5\n\nprint(f\"\\nCurrent minimum: {heap.peek()}\")\n\n# Example: Find Kth largest\ndef find_kth_largest(nums, k):\n    heap = []\n    for num in nums:\n        heapq.heappush(heap, num)\n        if len(heap) > k:\n            heapq.heappop(heap)\n    return heap[0]\n\nnums = [3, 2, 1, 5, 6, 4]\nk = 2\nprint(f\"\\nFind {k}th largest in {nums}: {find_kth_largest(nums, k)}\")\n",
      "input": ""
    }
  },
  {
    "slug": "array-sliding-window",
    "title": "Sliding Window",
    "summary": "Master the sliding window pattern for subarray problems.",
    "level": "Intermediate",
    "category": "Arrays",
    "content": [
      "Sliding window maintains a window (subarray) and slides it across the array to solve problems efficiently.",
      "Fixed-size window: size k is constant (e.g., max sum of k consecutive elements).",
      "Variable-size window: size changes based on a condition (e.g., longest substring with at most k distinct characters).",
      "Reduces brute-force O(n²) or O(n³) solutions to O(n) by reusing calculations from the previous window."
    ],
    "example": {
      "language": "python",
      "code": "# Fixed sliding window: max sum of k consecutive elements\ndef max_sum_k(arr, k):\n    n = len(arr)\n    if n < k:\n        return -1\n    \n    # Compute sum of first window\n    window_sum = sum(arr[:k])\n    max_sum = window_sum\n    \n    # Slide the window\n    for i in range(n - k):\n        window_sum = window_sum - arr[i] + arr[i + k]\n        max_sum = max(max_sum, window_sum)\n    \n    return max_sum\n\narr = [1, 4, 2, 10, 23, 3, 1, 0, 20]\nk = 4\nprint(max_sum_k(arr, k))  # 39 (10+23+3+1)\n",
      "input": ""
    }
  },
  {
    "slug": "binary-trees-intro",
    "title": "Binary Trees Introduction",
    "summary": "Understanding hierarchical data structures with nodes and children",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "A binary tree is a hierarchical data structure where each node contains a value and can have at most two children: a left child and a right child. Unlike linear structures like arrays or linked lists, trees represent parent-child relationships, making them ideal for hierarchical data like file systems, organizational charts, or expression parsing.",
      "Each binary tree starts with a root node (the topmost node). Nodes with no children are called leaf nodes, while nodes with at least one child are internal nodes. The height of a tree is the longest path from root to any leaf, and the depth of a node is its distance from the root. A tree with only one node has height 0.",
      "Binary trees have several important properties: maximum nodes at level i is 2^i, and a tree of height h can have at most 2^(h+1) - 1 nodes. A perfect binary tree has all levels completely filled. A complete binary tree fills levels left-to-right, with all levels full except possibly the last. A balanced binary tree maintains height of O(log n), ensuring efficient operations.",
      "Common operations include insertion (adding nodes), deletion (removing nodes while maintaining structure), and searching (finding specific values). Traversal methods let us visit all nodes systematically - we'll explore depth-first (preorder, inorder, postorder) and breadth-first (level-order) approaches in upcoming topics.",
      "Time complexity for basic operations depends on tree structure. In a balanced tree, search, insertion, and deletion take O(log n). However, in a skewed tree (resembling a linked list), these degrade to O(n). Space complexity for storing n nodes is always O(n), while recursion depth can reach O(h) where h is tree height."
    ],
    "example": {
      "language": "python",
      "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef count_nodes(root):\n    \"\"\"Count total nodes in binary tree\"\"\"\n    if root is None:\n        return 0\n    return 1 + count_nodes(root.left) + count_nodes(root.right)\n\ndef max_depth(root):\n    \"\"\"Find height of binary tree\"\"\"\n    if root is None:\n        return 0\n    return 1 + max(max_depth(root.left), max_depth(root.right))\n\n# Example usage:\nroot = TreeNode(1)\nroot.left = TreeNode(2)\nroot.right = TreeNode(3)\nroot.left.left = TreeNode(4)\nroot.left.right = TreeNode(5)\n\nprint(f\"Total nodes: {count_nodes(root)}\")  # Output: 5\nprint(f\"Tree height: {max_depth(root)}\")    # Output: 3",
      "input": "Binary Tree:\n       1\n      / \\\n     2   3\n    / \\\n   4   5"
    }
  },
  {
  "slug": "tree-traversals-dfs",
  "title": "Tree Traversals (DFS)",
  "summary": "Master depth-first traversals: inorder, preorder, and postorder",
  "level": "Intermediate",
  "category": "Trees",
  "content": [
    "Depth-First Search (DFS) traversals explore a tree by diving as deep as possible into each branch before backtracking. Unlike breadth-first search which explores level-by-level, DFS uses recursion or a stack to visit nodes in three distinct orders: inorder, preorder, and postorder. Each traversal has specific use cases and produces different node visitation sequences.",
    "Preorder traversal (Root → Left → Right) visits the current node first, then recursively traverses left and right subtrees. This is useful for creating a copy of the tree, serializing tree structure, or prefix notation expressions. For tree [1,2,3,4,5], preorder gives: 1, 2, 4, 5, 3.",
    "Inorder traversal (Left → Root → Right) visits the left subtree first, then the current node, then the right subtree. For Binary Search Trees, inorder traversal produces values in sorted ascending order, making it essential for BST validation and sorted output. For tree [1,2,3,4,5], inorder gives: 4, 2, 5, 1, 3.",
    "Postorder traversal (Left → Right → Root) visits both subtrees before the current node. This is ideal for deleting trees (leaves first), calculating directory sizes, or postfix notation expressions. For tree [1,2,3,4,5], postorder gives: 4, 5, 2, 3, 1. The parent is always processed after its children.",
    "All three DFS traversals have O(n) time complexity visiting each node once, and O(h) space complexity for recursion stack where h is tree height. In balanced trees, space is O(log n), but in skewed trees it becomes O(n). Iterative implementations using explicit stacks can help manage stack overflow for very deep trees."
  ],
  "example": {
    "language": "python",
    "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef inorder_traversal(root, result=[]):\n    \"\"\"Left -> Root -> Right (sorted for BST)\"\"\"\n    if root:\n        inorder_traversal(root.left, result)\n        result.append(root.val)  # Process node\n        inorder_traversal(root.right, result)\n    return result\n\ndef preorder_traversal(root, result=[]):\n    \"\"\"Root -> Left -> Right (copy tree structure)\"\"\"\n    if root:\n        result.append(root.val)  # Process node first\n        preorder_traversal(root.left, result)\n        preorder_traversal(root.right, result)\n    return result\n\ndef postorder_traversal(root, result=[]):\n    \"\"\"Left -> Right -> Root (delete tree, bottom-up)\"\"\"\n    if root:\n        postorder_traversal(root.left, result)\n        postorder_traversal(root.right, result)\n        result.append(root.val)  # Process node last\n    return result\n\n# Example usage:\nroot = TreeNode(1)\nroot.left = TreeNode(2)\nroot.right = TreeNode(3)\nroot.left.left = TreeNode(4)\nroot.left.right = TreeNode(5)\n\nprint(f\"Inorder: {inorder_traversal(root, [])}\")    # [4,2,5,1,3]\nprint(f\"Preorder: {preorder_traversal(root, [])}\")  # [1,2,4,5,3]\nprint(f\"Postorder: {postorder_traversal(root, [])}]\") # [4,5,2,3,1]",
    "input": "Binary Tree:\n       1\n      / \\\n     2   3\n    / \\\n   4   5"
  }
  },
  {
    "slug": "binary-search-trees",
    "title": "Binary Search Trees (BST)",
    "summary": "Learn BST properties and operations: search, insert, and delete",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "A Binary Search Tree (BST) is a specialized binary tree where each node follows a strict ordering property: all values in the left subtree are smaller than the node's value, and all values in the right subtree are greater. This ordering enables efficient searching, insertion, and deletion operations with O(log n) time complexity in balanced trees. For example, in a BST with root 8, left subtree might contain [3,1,6] and right subtree [10,14].",
      "Searching in a BST leverages the ordering property by comparing the target value with the current node. If the target is smaller, search the left subtree; if larger, search the right. This binary decision at each node eliminates half the remaining nodes, similar to binary search on sorted arrays. A search for value 6 in tree [8,3,10,1,6,14] would go: 8→3→6 (found), requiring only 3 comparisons instead of checking all nodes.",
      "Insertion maintains BST property by finding the correct leaf position. Start at root, compare with target: go left if smaller, right if larger. When reaching null, insert the new node there. Inserting 7 into [8,3,10,1,6,14] follows path 8→3→6→right, creating new node. Duplicate values are typically handled by storing frequency counts or rejecting insertion based on requirements.",
      "Deletion is the most complex BST operation with three cases: (1) Deleting a leaf node - simply remove it. (2) Node with one child - replace it with its child. (3) Node with two children - replace with inorder successor (smallest node in right subtree) or inorder predecessor (largest in left subtree), then delete that successor/predecessor node. This maintains BST property throughout the tree.",
      "BST validation requires checking that for every node, all left descendants are smaller and all right descendants are larger. A common mistake is only checking immediate children. Use inorder traversal (which produces sorted output in valid BSTs) or recursive validation with min-max bounds. Time complexity: search, insert, delete are O(h) where h is height - O(log n) for balanced trees but O(n) for skewed trees, highlighting the importance of self-balancing variants like AVL or Red-Black trees."
    ],
    "example": {
      "language": "python",
      "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef search_bst(root, target):\n    \"\"\"Search for target value in BST\"\"\"\n    if not root or root.val == target:\n        return root\n    if target < root.val:\n        return search_bst(root.left, target)\n    return search_bst(root.right, target)\n\ndef insert_bst(root, val):\n    \"\"\"Insert new value into BST\"\"\"\n    if not root:\n        return TreeNode(val)\n    if val < root.val:\n        root.left = insert_bst(root.left, val)\n    elif val > root.val:\n        root.right = insert_bst(root.right, val)\n    return root\n\ndef delete_bst(root, key):\n    \"\"\"Delete node with given key from BST\"\"\"\n    if not root:\n        return None\n    \n    if key < root.val:\n        root.left = delete_bst(root.left, key)\n    elif key > root.val:\n        root.right = delete_bst(root.right, key)\n    else:\n        # Node with one child or no child\n        if not root.left:\n            return root.right\n        if not root.right:\n            return root.left\n        \n        # Node with two children: get inorder successor\n        min_node = root.right\n        while min_node.left:\n            min_node = min_node.left\n        root.val = min_node.val\n        root.right = delete_bst(root.right, min_node.val)\n    \n    return root\n\ndef validate_bst(root, min_val=float('-inf'), max_val=float('inf')):\n    \"\"\"Validate if tree is valid BST\"\"\"\n    if not root:\n        return True\n    if root.val <= min_val or root.val >= max_val:\n        return False\n    return (validate_bst(root.left, min_val, root.val) and\n            validate_bst(root.right, root.val, max_val))",
      "input": "BST Example:\n       8\n      / \\\n     3   10\n    / \\    \\\n   1   6   14\n      / \\\n     4   7"
    }
    },
    {
    "slug": "level-order-traversal",
    "title": "Level Order Traversal (BFS)",
    "summary": "Explore trees level-by-level using breadth-first search",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "Level Order Traversal, also known as Breadth-First Search (BFS) for trees, visits all nodes at each depth level before moving to the next level. Unlike DFS which explores deep into branches, BFS explores horizontally across each level from left to right. For tree [3,9,20,null,null,15,7], level order produces: [[3], [9,20], [15,7]]. This traversal is essential for finding shortest paths, level-based operations, and hierarchical processing.",
      "The BFS algorithm uses a queue (FIFO - First In, First Out) to maintain nodes for processing. Start by enqueuing the root, then repeatedly dequeue a node, process it, and enqueue its children left-to-right. The queue naturally maintains level order because all nodes at depth d are enqueued before any node at depth d+1. This guarantees we process nodes in correct breadth-first order without recursion.",
      "Level-by-level processing tracks nodes at each depth separately. Use a nested loop: outer loop runs while queue is not empty, inner loop processes all nodes at current level (queue size at that moment). This technique enables level-specific operations like finding average of each level, maximum value per level, or creating separate arrays for each depth. The queue size before processing a level tells you exactly how many nodes are at that level.",
      "Variations include Zigzag Level Order (alternating left-to-right and right-to-left directions), Right Side View (only rightmost node per level), and Bottom-Up Level Order (reverse the result). Zigzag uses a flag to reverse alternate levels. Right Side View takes the last element when processing each level. These variations build on the standard BFS template, modifying only how results are collected or ordered.",
      "Time complexity is O(n) as we visit each node exactly once. Space complexity is O(w) where w is maximum width of tree - the queue can hold at most all nodes at the widest level. For complete binary trees, this is O(n/2) = O(n) at the last level. For skewed trees, width is O(1). BFS is ideal when you need shortest path in unweighted trees, level-wise grouping, or finding nodes at specific distances from root."
    ],
    "example": {
      "language": "python",
      "code": "from collections import deque\n\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef level_order_traversal(root):\n    \"\"\"Standard BFS - returns nodes grouped by level\"\"\"\n    if not root:\n        return []\n    \n    result = []\n    queue = deque([root])\n    \n    while queue:\n        level_size = len(queue)  # Nodes at current level\n        current_level = []\n        \n        for _ in range(level_size):\n            node = queue.popleft()\n            current_level.append(node.val)\n            \n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n        \n        result.append(current_level)\n    \n    return result\n\ndef zigzag_traversal(root):\n    \"\"\"Zigzag BFS - alternate left-to-right direction\"\"\"\n    if not root:\n        return []\n    \n    result = []\n    queue = deque([root])\n    left_to_right = True\n    \n    while queue:\n        level_size = len(queue)\n        current_level = deque()\n        \n        for _ in range(level_size):\n            node = queue.popleft()\n            \n            # Add to level based on direction\n            if left_to_right:\n                current_level.append(node.val)\n            else:\n                current_level.appendleft(node.val)\n            \n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n        \n        result.append(list(current_level))\n        left_to_right = not left_to_right\n    \n    return result\n\ndef right_side_view(root):\n    \"\"\"Get rightmost node at each level\"\"\"\n    if not root:\n        return []\n    \n    result = []\n    queue = deque([root])\n    \n    while queue:\n        level_size = len(queue)\n        \n        for i in range(level_size):\n            node = queue.popleft()\n            \n            # Last node in level is rightmost\n            if i == level_size - 1:\n                result.append(node.val)\n            \n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n    \n    return result",
      "input": "Binary Tree:\n       3\n      / \\\n     9  20\n       /  \\\n      15   7\n\nOutput:\n[[3], [9,20], [15,7]]"
    }
    },
    {
    "slug": "tree-properties-paths",
    "title": "Tree Properties & Paths",
    "summary": "Advanced tree algorithms: path sums, LCA, diameter, and construction",
    "level": "Intermediate",
    "category": "Trees",
    "content": [
      "Tree path problems involve finding or calculating values along paths between nodes. A path is a sequence of connected nodes from one node to another, most commonly from root to leaf. Path Sum problems ask if a root-to-leaf path exists with a specific sum by subtracting each node's value from the target while traversing. For tree [5,4,8,11,null,13,4,7,2], checking path sum 22 would traverse 5→4→11→2, accumulating values to verify the sum matches.",
      "The Diameter of a binary tree measures the longest path between any two nodes, counted as number of edges. Crucially, this path doesn't need to pass through the root. For each node, the potential diameter through it equals the sum of heights of its left and right subtrees. Calculate this for all nodes and return the maximum. For tree [1,2,3,4,5], diameter is 3 (path 4→2→1→3 or 5→2→1→3), even though it passes through root.",
      "Lowest Common Ancestor (LCA) finds the deepest node that is an ancestor of both given nodes. Two approaches exist: (1) Store parent pointers and trace paths from both nodes upward until they meet, or (2) Use recursion - if current node is one of the targets or both targets are found in different subtrees, current node is LCA. For tree [3,5,1,6,2,0,8], LCA(5,1) = 3 (root), but LCA(5,2) = 5 (ancestor can be one of the nodes itself).",
      "Tree construction from traversals recreates the original tree structure given specific orderings. Given preorder and inorder, preorder's first element is root; find it in inorder to split left/right subtrees, then recursively build each subtree. Preorder gives roots in order visited; inorder splits subtrees. Given postorder and inorder, postorder's last element is root. You cannot uniquely construct a tree from preorder and postorder alone without additional information.",
      "Sum of Left Leaves adds only values of leaves that are left children of their parents. Use recursion checking if current node's left child exists and is a leaf (no children). Path problems often use DFS with backtracking, maintaining current path sum or collecting path nodes. Time complexity is typically O(n) visiting each node once. Space complexity is O(h) for recursion stack where h is height. Many path problems use global variables to track maximum values or counts across recursive calls."
    ],
    "example": {
      "language": "python",
      "code": "class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef has_path_sum(root, target_sum):\n    \"\"\"Check if root-to-leaf path exists with given sum\"\"\"\n    if not root:\n        return False\n    \n    # Leaf node - check if sum matches\n    if not root.left and not root.right:\n        return target_sum == root.val\n    \n    # Recursively check left and right subtrees\n    remaining = target_sum - root.val\n    return (has_path_sum(root.left, remaining) or \n            has_path_sum(root.right, remaining))\n\ndef diameter_of_tree(root):\n    \"\"\"Find diameter (longest path between any two nodes)\"\"\"\n    diameter = 0\n    \n    def height(node):\n        nonlocal diameter\n        if not node:\n            return 0\n        \n        left_height = height(node.left)\n        right_height = height(node.right)\n        \n        # Update diameter if path through this node is longer\n        diameter = max(diameter, left_height + right_height)\n        \n        # Return height of this subtree\n        return 1 + max(left_height, right_height)\n    \n    height(root)\n    return diameter\n\ndef lowest_common_ancestor(root, p, q):\n    \"\"\"Find LCA of two nodes p and q\"\"\"\n    if not root or root == p or root == q:\n        return root\n    \n    left = lowest_common_ancestor(root.left, p, q)\n    right = lowest_common_ancestor(root.right, p, q)\n    \n    # If both sides return non-null, current node is LCA\n    if left and right:\n        return root\n    \n    # Return whichever side found a target\n    return left if left else right\n\ndef sum_of_left_leaves(root):\n    \"\"\"Calculate sum of all left leaf nodes\"\"\"\n    if not root:\n        return 0\n    \n    total = 0\n    # Check if left child is a leaf\n    if root.left and not root.left.left and not root.left.right:\n        total += root.left.val\n    \n    # Recursively check both subtrees\n    total += sum_of_left_leaves(root.left)\n    total += sum_of_left_leaves(root.right)\n    \n    return total\n\ndef build_tree_preorder_inorder(preorder, inorder):\n    \"\"\"Construct tree from preorder and inorder traversals\"\"\"\n    if not preorder or not inorder:\n        return None\n    \n    # First element of preorder is root\n    root_val = preorder[0]\n    root = TreeNode(root_val)\n    \n    # Find root in inorder to split left/right subtrees\n    mid = inorder.index(root_val)\n    \n    # Recursively build left and right subtrees\n    root.left = build_tree_preorder_inorder(preorder[1:mid+1], inorder[:mid])\n    root.right = build_tree_preorder_inorder(preorder[mid+1:], inorder[mid+1:])\n    \n    return root",
      "input": "Example Tree:\n       5\n      / \\\n     4   8\n    /   / \\\n   11  13  4\n  /  \\      \\\n 7    2      1"
    }
    },
    {
    "slug": "graph-representation",
    "title": "Graph Representation",
    "summary": "Learn how to represent graphs using adjacency lists, matrices, and edge lists",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "A graph is a data structure consisting of nodes (vertices) and edges connecting them. Unlike trees which have hierarchical parent-child relationships, graphs can have arbitrary connections including cycles. Graphs model real-world networks: social connections, road maps, dependencies, and web pages. A graph can be directed (edges have direction) or undirected (connections are bidirectional), and weighted (edges have costs) or unweighted.",
      "Edge List representation stores a graph as a simple list of edges, where each edge is a pair [u, v] indicating nodes u and v are connected. For weighted graphs, edges include weights: [u, v, weight]. This is the most compact representation, using O(E) space where E is number of edges. Edge lists are ideal for algorithms that process all edges like Kruskal's algorithm for minimum spanning trees, but checking if two nodes are connected takes O(E) time.",
      "Adjacency Matrix uses a 2D array of size N×N where N is number of nodes. matrix[i][j] = 1 if edge exists from node i to j, else 0. For weighted graphs, store weights instead of 1. This representation allows O(1) edge lookup - checking if nodes are connected is instant. However, it always uses O(N²) space regardless of edge count, making it inefficient for sparse graphs (few edges). Adjacency matrices work well for dense graphs where most nodes connect to most other nodes.",
      "Adjacency List is the most common representation, using an array or hash map where each node stores a list of its neighbors. For node i, adjacencyList[i] contains all nodes connected to i. This uses O(V + E) space - proportional to actual edges, making it memory-efficient for sparse graphs. Adding edges is O(1), and iterating neighbors of a node is O(degree). Most graph algorithms including DFS, BFS, and Dijkstra's work naturally with adjacency lists.",
      "Choosing the right representation depends on your use case. Use adjacency lists for most problems as they balance space and time efficiency. Use adjacency matrices when you need fast edge lookups or the graph is dense. Use edge lists when you only need to iterate all edges or the graph is very sparse. In coding interviews, adjacency lists are standard - graphs are typically given as arrays where index represents node and value is list of neighbors."
    ],
    "example": {
      "language": "python",
      "code": "from collections import defaultdict\n\n# Graph with nodes 0,1,2,3 and edges [(0,1), (0,2), (1,2), (2,3)]\n\n# 1. Edge List Representation\nedge_list = [[0,1], [0,2], [1,2], [2,3]]\nprint(f\"Edge List: {edge_list}\")\nprint(f\"Space: O(E) = O(4)\")\n\n# 2. Adjacency Matrix Representation\nn = 4  # number of nodes\nadj_matrix = [[0]*n for _ in range(n)]\nfor u, v in edge_list:\n    adj_matrix[u][v] = 1\n    adj_matrix[v][u] = 1  # for undirected graph\n\nprint(f\"\\nAdjacency Matrix:\")\nfor row in adj_matrix:\n    print(row)\nprint(f\"Space: O(N²) = O(16)\")\nprint(f\"Check edge (1,2): O(1) = {adj_matrix[1][2]}\")\n\n# 3. Adjacency List Representation (most common)\nadj_list = defaultdict(list)\nfor u, v in edge_list:\n    adj_list[u].append(v)\n    adj_list[v].append(u)  # for undirected graph\n\nprint(f\"\\nAdjacency List:\")\nfor node in range(n):\n    print(f\"{node}: {adj_list[node]}\")\nprint(f\"Space: O(V+E) = O(4+8) = O(12)\")\n\n# Converting between representations\ndef edge_list_to_adj_list(edges, n):\n    \"\"\"Convert edge list to adjacency list\"\"\"\n    graph = defaultdict(list)\n    for u, v in edges:\n        graph[u].append(v)\n        graph[v].append(u)\n    return graph\n\ndef adj_list_to_matrix(adj_list, n):\n    \"\"\"Convert adjacency list to matrix\"\"\"\n    matrix = [[0]*n for _ in range(n)]\n    for u in adj_list:\n        for v in adj_list[u]:\n            matrix[u][v] = 1\n    return matrix",
      "input": "Graph:\n  0 -- 1\n  |    |\n  2 -- 3\n\nEdges: [[0,1],[0,2],[1,2],[2,3]]"
    }
    },
    {
    "slug": "graph-dfs",
    "title": "Graph DFS (Depth-First Search)",
    "summary": "Master depth-first traversal, cycle detection, and connected components",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Depth-First Search (DFS) is a graph traversal algorithm that explores as far as possible along each branch before backtracking. Starting from a source vertex, DFS visits a neighbor, then recursively explores that neighbor's unvisited neighbors, going deeper until reaching a dead end before backtracking. This contrasts with BFS which explores level-by-level. DFS uses a stack (implicitly via recursion or explicitly) while BFS uses a queue. For graph with edges [0→1, 0→2, 1→3, 2→3], DFS from 0 might visit: 0→1→3→(backtrack)→2.",
      "DFS implementation requires tracking visited nodes to avoid infinite loops in cyclic graphs. Use a boolean visited array or set where visited[node] = true after processing. The recursive approach is most natural: mark current node visited, then recursively call DFS on each unvisited neighbor. Iterative DFS uses an explicit stack, pushing unvisited neighbors and popping to process. Time complexity is O(V + E) where V is vertices and E is edges - we visit each vertex once and explore each edge once. Space complexity is O(V) for the visited array plus O(V) for recursion stack depth.",
      "Cycle detection in undirected graphs uses DFS with parent tracking. A cycle exists if during DFS we encounter a visited node that isn't the parent of current node - this indicates a back edge forming a cycle. For directed graphs, cycle detection requires tracking nodes in the current recursion stack. If we reach a node already in the recursion stack, a cycle exists. Use three states: unvisited (white), in-stack (gray), and completely processed (black). A back edge from gray to gray node indicates a cycle.",
      "Connected components are maximal subgraphs where every pair of vertices has a path between them. In undirected graphs, run DFS from each unvisited node - each DFS call discovers one complete component. Count the number of DFS calls to get component count. For example, graph with edges [0-1, 1-2, 3-4] has 2 components: {0,1,2} and {3,4}. This is useful for network connectivity problems, island counting, and clustering. In directed graphs, use Kosaraju's or Tarjan's algorithm for strongly connected components.",
      "DFS applications include topological sorting (for directed acyclic graphs), maze solving, path finding, detecting deadlocks, and solving puzzles. DFS is preferred over BFS when: (1) solution is far from root, (2) tree is very deep with many branches, (3) checking if path exists, or (4) memory is limited. For shortest paths in unweighted graphs, use BFS instead. DFS can be modified to find all paths between two nodes by removing the permanent visited marking and using backtracking."
    ],
    "example": {
      "language": "python",
      "code": "from collections import defaultdict\n\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v, directed=False):\n        self.graph[u].append(v)\n        if not directed:\n            self.graph[v].append(u)\n    \n    def dfs(self, start, visited=None):\n        \"\"\"Basic DFS traversal\"\"\"\n        if visited is None:\n            visited = set()\n        \n        visited.add(start)\n        print(start, end=' ')\n        \n        for neighbor in self.graph[start]:\n            if neighbor not in visited:\n                self.dfs(neighbor, visited)\n        \n        return visited\n    \n    def has_cycle_undirected(self, node, visited, parent):\n        \"\"\"Detect cycle in undirected graph\"\"\"\n        visited.add(node)\n        \n        for neighbor in self.graph[node]:\n            if neighbor not in visited:\n                if self.has_cycle_undirected(neighbor, visited, node):\n                    return True\n            elif neighbor != parent:\n                # Visited node that's not parent = cycle!\n                return True\n        \n        return False\n    \n    def has_cycle_directed(self, node, visited, rec_stack):\n        \"\"\"Detect cycle in directed graph\"\"\"\n        visited.add(node)\n        rec_stack.add(node)\n        \n        for neighbor in self.graph[node]:\n            if neighbor not in visited:\n                if self.has_cycle_directed(neighbor, visited, rec_stack):\n                    return True\n            elif neighbor in rec_stack:\n                # Back edge to node in recursion stack = cycle!\n                return True\n        \n        rec_stack.remove(node)\n        return False\n    \n    def count_connected_components(self, num_nodes):\n        \"\"\"Count connected components (undirected graph)\"\"\"\n        visited = set()\n        count = 0\n        \n        for node in range(num_nodes):\n            if node not in visited:\n                self.dfs(node, visited)\n                count += 1\n        \n        return count\n    \n    def find_path(self, start, end, visited=None, path=None):\n        \"\"\"Find a path between two nodes\"\"\"\n        if visited is None:\n            visited = set()\n        if path is None:\n            path = []\n        \n        visited.add(start)\n        path.append(start)\n        \n        if start == end:\n            return path\n        \n        for neighbor in self.graph[start]:\n            if neighbor not in visited:\n                result = self.find_path(neighbor, end, visited, path)\n                if result:\n                    return result\n        \n        path.pop()\n        return None",
      "input": "Graph Example:\n    0 --- 1\n    |     |\n    2 --- 3\n\nDFS from 0: 0 → 1 → 3 → 2"
    }
    },
    {
    "slug": "graph-bfs",
    "title": "Graph BFS (Breadth-First Search)",
    "summary": "Learn BFS traversal, shortest paths, and bipartite graph detection",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Breadth-First Search (BFS) is a graph traversal algorithm that explores nodes level by level, visiting all neighbors at the current distance before moving to nodes at the next distance. Starting from a source vertex, BFS visits all nodes at distance 1, then all nodes at distance 2, and so on. This contrasts with DFS which goes deep first. BFS uses a queue (FIFO - First In, First Out) to maintain nodes for processing. For graph with edges [0-1, 0-2, 1-3, 2-3], BFS from 0 visits: 0 → [1,2] → [3], exploring horizontally before going deeper.",
      "BFS implementation uses a queue and visited tracking. Initialize by enqueuing the source node and marking it visited. While queue is not empty: dequeue a node, process it, then enqueue all its unvisited neighbors and mark them visited. The queue ensures nodes are processed in order of increasing distance from source. Time complexity is O(V + E) where V is vertices and E is edges - each vertex is enqueued once and each edge examined once. Space complexity is O(V) for the queue and visited array. The level-by-level nature makes BFS ideal for finding shortest paths.",
      "Shortest path in unweighted graphs is BFS's killer application. Because BFS explores nodes by distance, the first time you reach a node is guaranteed to be via the shortest path. To track paths, maintain a distance array (distance from source) and parent array (previous node in path). When processing node u and discovering unvisited neighbor v, set distance[v] = distance[u] + 1 and parent[v] = u. Reconstruct path by backtracking from destination through parents. For weighted graphs, use Dijkstra's algorithm instead. BFS finds shortest path in O(V + E) time.",
      "Bipartite graphs can be partitioned into two sets where every edge connects vertices from different sets - no two vertices in the same set are adjacent. Examples include matching problems, scheduling, and graph coloring. To check if a graph is bipartite, use BFS with two-coloring: assign source node color 0, then for each node, assign its unvisited neighbors the opposite color (1 if current is 0, vice versa). If you ever find a neighbor with the same color as current node, the graph is not bipartite (contains odd-length cycle). BFS ensures we check all connected components.",
      "BFS applications include shortest path in unweighted graphs, finding connected components, checking bipartiteness, web crawling, GPS navigation (with modifications), social network friend suggestions (friends of friends), and solving puzzles like sliding tile games. Use BFS when: (1) finding shortest path in unweighted graphs, (2) exploring nodes level by level, (3) finding nearest neighbors, or (4) the solution is close to the root. BFS is preferred over DFS for shortest paths because it explores nodes in order of distance. For very deep graphs with limited memory, DFS may be better."
    ],
    "example": {
      "language": "python",
      "code": "from collections import deque, defaultdict\n\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v, directed=False):\n        self.graph[u].append(v)\n        if not directed:\n            self.graph[v].append(u)\n    \n    def bfs(self, start):\n        \"\"\"Basic BFS traversal\"\"\"\n        visited = set()\n        queue = deque([start])\n        visited.add(start)\n        result = []\n        \n        while queue:\n            node = queue.popleft()\n            result.append(node)\n            \n            for neighbor in self.graph[node]:\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    queue.append(neighbor)\n        \n        return result\n    \n    def shortest_path(self, start, end):\n        \"\"\"Find shortest path using BFS\"\"\"\n        if start == end:\n            return [start]\n        \n        visited = set([start])\n        queue = deque([start])\n        parent = {start: None}\n        \n        while queue:\n            node = queue.popleft()\n            \n            if node == end:\n                # Reconstruct path\n                path = []\n                current = end\n                while current is not None:\n                    path.append(current)\n                    current = parent[current]\n                return path[::-1]\n            \n            for neighbor in self.graph[node]:\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    parent[neighbor] = node\n                    queue.append(neighbor)\n        \n        return None  # No path found\n    \n    def shortest_distances(self, start, num_nodes):\n        \"\"\"Find shortest distance from start to all nodes\"\"\"\n        distance = [-1] * num_nodes\n        distance[start] = 0\n        queue = deque([start])\n        \n        while queue:\n            node = queue.popleft()\n            \n            for neighbor in self.graph[node]:\n                if distance[neighbor] == -1:\n                    distance[neighbor] = distance[node] + 1\n                    queue.append(neighbor)\n        \n        return distance\n    \n    def is_bipartite(self, num_nodes):\n        \"\"\"Check if graph is bipartite using 2-coloring\"\"\"\n        color = [-1] * num_nodes\n        \n        # Check all components\n        for start in range(num_nodes):\n            if color[start] == -1:\n                queue = deque([start])\n                color[start] = 0\n                \n                while queue:\n                    node = queue.popleft()\n                    \n                    for neighbor in self.graph[node]:\n                        if color[neighbor] == -1:\n                            # Color with opposite color\n                            color[neighbor] = 1 - color[node]\n                            queue.append(neighbor)\n                        elif color[neighbor] == color[node]:\n                            # Same color = not bipartite\n                            return False\n        \n        return True",
      "input": "Graph Example:\n    0 --- 1\n    |     |\n    2 --- 3\n\nBFS from 0: 0 → 1, 2 → 3\nShortest path 0→3: 0→1→3 (length 2)"
    }
    },
    {
    "slug": "shortest-path-dijkstra",
    "title": "Shortest Path (Dijkstra's Algorithm)",
    "summary": "Find shortest paths in weighted graphs using Dijkstra's algorithm",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Dijkstra's algorithm finds the shortest path from a source vertex to all other vertices in a weighted graph with non-negative edge weights. Unlike BFS which works only for unweighted graphs (or treats all edges as weight 1), Dijkstra handles varying edge costs. The algorithm maintains a set of visited nodes and repeatedly selects the unvisited node with the smallest known distance, then updates distances to its neighbors. For graph with edges [(A,B,4), (A,C,2), (C,B,1)], the shortest path A→B is A→C→B with total weight 3, not the direct edge A→B with weight 4.",
      "The algorithm uses a priority queue (min-heap) to efficiently select the next node to process - always choosing the unvisited node with minimum distance. Initialize all distances to infinity except source (distance 0). While priority queue is not empty: extract node u with minimum distance, mark it visited, then for each neighbor v of u, if distance[u] + weight(u,v) < distance[v], update distance[v] and parent[v]. This relaxation step is key - we constantly try to find better paths. The priority queue ensures we process nodes in order of increasing distance from source.",
      "Time complexity is O((V + E) log V) with a binary heap priority queue, where V is vertices and E is edges. Each vertex is extracted once from the priority queue (O(V log V)), and each edge causes a potential distance update and priority queue operation (O(E log V)). Space complexity is O(V) for distance array, parent array, and priority queue. With a Fibonacci heap, complexity improves to O(E + V log V), but implementation is complex. For dense graphs where E ≈ V², using an array for finding minimum (O(V²)) can be faster than priority queue.",
      "Dijkstra's algorithm has critical requirements and limitations. It requires non-negative edge weights - negative weights break the greedy approach because a shorter path might be found after marking a node as visited. For graphs with negative weights, use Bellman-Ford algorithm instead. Dijkstra works on both directed and undirected graphs. To reconstruct the shortest path, maintain a parent array tracking the previous node in the optimal path. Backtrack from destination through parents to source, then reverse. The algorithm naturally handles disconnected graphs - unreachable nodes retain infinite distance.",
      "Applications include GPS navigation systems, network routing protocols (OSPF uses Dijkstra), social network analysis (degrees of separation), game AI pathfinding, and resource optimization. Dijkstra vs BFS: use BFS for unweighted graphs (faster, simpler), use Dijkstra for weighted graphs with non-negative weights. Dijkstra vs Bellman-Ford: Dijkstra is faster but requires non-negative weights; Bellman-Ford handles negative weights and detects negative cycles. Dijkstra vs A*: A* is Dijkstra with a heuristic for faster goal-directed search, commonly used in games and robotics."
    ],
    "example": {
      "language": "python",
      "code": "import heapq\nfrom collections import defaultdict\n\nclass Graph:\n    def __init__(self):\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v, weight, directed=False):\n        self.graph[u].append((v, weight))\n        if not directed:\n            self.graph[v].append((u, weight))\n    \n    def dijkstra(self, start, num_nodes):\n        \"\"\"Find shortest distances from start to all nodes\"\"\"\n        # Initialize distances to infinity\n        distance = [float('inf')] * num_nodes\n        distance[start] = 0\n        parent = [-1] * num_nodes\n        \n        # Priority queue: (distance, node)\n        pq = [(0, start)]\n        visited = set()\n        \n        while pq:\n            dist, node = heapq.heappop(pq)\n            \n            # Skip if already visited\n            if node in visited:\n                continue\n            \n            visited.add(node)\n            \n            # Check all neighbors\n            for neighbor, weight in self.graph[node]:\n                new_dist = distance[node] + weight\n                \n                # Relaxation: found shorter path?\n                if new_dist < distance[neighbor]:\n                    distance[neighbor] = new_dist\n                    parent[neighbor] = node\n                    heapq.heappush(pq, (new_dist, neighbor))\n        \n        return distance, parent\n    \n    def shortest_path_to(self, start, end, num_nodes):\n        \"\"\"Find shortest path from start to end\"\"\"\n        distance, parent = self.dijkstra(start, num_nodes)\n        \n        if distance[end] == float('inf'):\n            return None  # No path exists\n        \n        # Reconstruct path\n        path = []\n        current = end\n        while current != -1:\n            path.append(current)\n            current = parent[current]\n        \n        path.reverse()\n        return path, distance[end]\n\n# Example usage\ng = Graph()\ng.add_edge(0, 1, 4)\ng.add_edge(0, 2, 2)\ng.add_edge(2, 1, 1)\ng.add_edge(1, 3, 5)\ng.add_edge(2, 3, 8)\n\npath, cost = g.shortest_path_to(0, 3, 4)\nprint(f\"Path: {' -> '.join(map(str, path))}\")\nprint(f\"Cost: {cost}\")\n# Output: Path: 0 -> 2 -> 1 -> 3, Cost: 8",
      "input": "Weighted Graph:\n    0 --4-- 1\n    |  \\    |\n    2   1   5\n    |    \\  |\n    2 --8-- 3\n\nShortest 0→3: 0→2→1→3 (cost: 8)"
    }
    },
    {
    "slug": "topological-sort",
    "title": "Topological Sort",
    "summary": "Order tasks with dependencies using DFS and Kahn's algorithm",
    "level": "Intermediate",
    "category": "Graphs",
    "content": [
      "Topological sorting produces a linear ordering of vertices in a directed acyclic graph (DAG) such that for every directed edge u→v, vertex u appears before v in the ordering. This models tasks with dependencies: if task A must be completed before task B, we draw an edge A→B, and topological sort gives a valid task execution order. For courses with prerequisites, topological sort tells you which order to take classes. Example: for edges [0→1, 0→2, 1→3, 2→3], valid orders include [0,1,2,3] or [0,2,1,3]. Topological sort only exists for DAGs - graphs with cycles have no valid ordering.",
      "Two main algorithms solve topological sort: DFS-based and Kahn's algorithm (BFS-based). DFS approach performs depth-first search and adds nodes to result after visiting all descendants, then reverses the result. When exploring a node, recursively visit all neighbors first, then add current node to a stack. The stack naturally contains nodes in reverse topological order - nodes with no outgoing edges appear first, then nodes depending on them. Time complexity is O(V + E) visiting each vertex and edge once. This approach is elegant and naturally detects cycles using recursion stack.",
      "Kahn's algorithm is BFS-based and more intuitive for dependency problems. Calculate in-degree (number of incoming edges) for each vertex. Vertices with in-degree 0 have no dependencies and can be processed first. Initialize queue with all zero in-degree vertices. While queue is not empty: dequeue vertex, add to result, then for each neighbor, decrease its in-degree by 1 (removing the processed dependency). If neighbor's in-degree becomes 0, enqueue it. If result contains all vertices, topological order exists; otherwise graph has a cycle. Time O(V + E), space O(V).",
      "Cycle detection is built into both algorithms. In DFS approach, maintain a recursion stack - if you reach a node already in the current path (gray node), a cycle exists. In Kahn's algorithm, if you process fewer than V vertices, remaining vertices are stuck in a cycle with no zero in-degree vertices. Course Schedule problems on LeetCode are classic topological sort applications: Course Schedule checks if completion is possible (cycle detection), Course Schedule II returns the actual course order (topological ordering).",
      "Applications include course scheduling with prerequisites, build systems (compile files in dependency order), package managers (install packages with dependencies), project management (task scheduling), symbol resolution in compilers, and spreadsheet formula evaluation. Choose DFS for simpler implementation when you just need an ordering. Choose Kahn's when you need to track in-degrees or process nodes level-by-level. Both detect cycles - DFS with recursion stack, Kahn's by checking if all vertices are processed. Multiple valid topological orders can exist for the same DAG."
    ],
    "example": {
      "language": "python",
      "code": "from collections import defaultdict, deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = defaultdict(list)\n    \n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n    \n    def topological_sort_dfs(self):\n        \"\"\"DFS-based topological sort\"\"\"\n        visited = set()\n        rec_stack = set()\n        result = []\n        \n        def dfs(node):\n            if node in rec_stack:\n                return False  # Cycle detected\n            if node in visited:\n                return True\n            \n            visited.add(node)\n            rec_stack.add(node)\n            \n            for neighbor in self.graph[node]:\n                if not dfs(neighbor):\n                    return False\n            \n            rec_stack.remove(node)\n            result.append(node)  # Add after visiting all descendants\n            return True\n        \n        for node in range(self.V):\n            if node not in visited:\n                if not dfs(node):\n                    return None  # Cycle exists\n        \n        return result[::-1]  # Reverse to get correct order\n    \n    def topological_sort_kahn(self):\n        \"\"\"Kahn's algorithm (BFS-based)\"\"\"\n        # Calculate in-degrees\n        in_degree = [0] * self.V\n        for node in range(self.V):\n            for neighbor in self.graph[node]:\n                in_degree[neighbor] += 1\n        \n        # Initialize queue with zero in-degree nodes\n        queue = deque([i for i in range(self.V) if in_degree[i] == 0])\n        result = []\n        \n        while queue:\n            node = queue.popleft()\n            result.append(node)\n            \n            # Reduce in-degree for neighbors\n            for neighbor in self.graph[node]:\n                in_degree[neighbor] -= 1\n                if in_degree[neighbor] == 0:\n                    queue.append(neighbor)\n        \n        # Check if all vertices processed (no cycle)\n        if len(result) != self.V:\n            return None  # Cycle exists\n        \n        return result\n\n# Example: Course prerequisites\ng = Graph(4)\ng.add_edge(0, 1)  # Course 0 before 1\ng.add_edge(0, 2)  # Course 0 before 2\ng.add_edge(1, 3)  # Course 1 before 3\ng.add_edge(2, 3)  # Course 2 before 3\n\nprint(\"DFS:\", g.topological_sort_dfs())   # [0, 1, 2, 3] or [0, 2, 1, 3]\nprint(\"Kahn's:\", g.topological_sort_kahn()) # [0, 1, 2, 3] or [0, 2, 1, 3]",
      "input": "Course Dependencies:\n    0 → 1\n    0 → 2\n    1 → 3\n    2 → 3\n\nValid orders: [0,1,2,3] or [0,2,1,3]"
    }
  },
    {
    "slug": "dp-introduction",
    "title": "Dynamic Programming: Intro, Memoization & Tabulation",
    "summary": "Learn how to turn slow recursive solutions into efficient algorithms using overlapping subproblems and optimal substructure.",
    "level": "Intermediate",
    "category": "Dynamic Programming",
    "content": [
      "Dynamic Programming (DP) is a technique to solve complex problems by breaking them into smaller overlapping subproblems and reusing their solutions instead of recomputing them. It is especially useful in optimization and counting problems where a naive recursive solution repeatedly solves the same subproblems, leading to exponential time. DP usually reduces such solutions to polynomial time by caching or tabulating results.",
      "Two key properties make a problem a good candidate for DP: optimal substructure and overlapping subproblems. Optimal substructure means the optimal solution to the overall problem can be constructed from optimal solutions of its subproblems. Overlapping subproblems means the same subproblems are solved multiple times in a naive recursion, so caching them can save work.",
      "A classic example is the Fibonacci sequence, where fib(n) = fib(n-1) + fib(n-2) with base cases fib(0) = 0 and fib(1) = 1. A naive recursive implementation recomputes the same fib(k) many times, leading to a time complexity of roughly O(2^n). With dynamic programming, we compute each fib(k) once and reuse stored results, reducing the complexity to O(n).",
      "There are two primary approaches to dynamic programming: top-down with memoization and bottom-up with tabulation. In the top-down approach, you start with the original problem and recursively solve subproblems, storing answers in a cache (memo). In the bottom-up approach, you iteratively build a table from the smallest subproblems up to the original one, carefully choosing an order so that dependencies are already computed.",
      "Memoization (top-down DP) wraps a normal recursive solution with a cache like an array or dictionary. When a recursive call is made for a subproblem, you first check if its result is already in the cache; if so, you return it immediately instead of recomputing. This keeps the code close to the mathematical recurrence and is often easier to write and debug when learning DP.",
      "Tabulation (bottom-up DP) builds a table iteratively, usually using loops. For Fibonacci, you can create an array dp where dp[i] stores fib(i), set the base cases like dp[0] and dp[1], and then fill the array from i = 2 to n using the relation dp[i] = dp[i-1] + dp[i-2]. Tabulation avoids recursion and can be more memory-efficient when combined with techniques like only storing the last few states.",
      "Choosing between memoization and tabulation depends on the problem and your goals. Memoization is usually easier to derive from a recursive definition, but it uses the call stack and may have overhead from recursive calls. Tabulation can be faster in practice and gives you more control over iteration order and space optimizations, but it requires you to think explicitly about the correct filling order of the DP table.",
      "Beyond Fibonacci, DP appears in many real-world scenarios such as path counting in grids, minimizing costs in scheduling, and optimizing resource allocation. For example, computing the minimum cost to reach the bottom-right of a grid from the top-left can be solved by defining dp[i][j] as the minimum cost to reach cell (i, j) and building the table row by row or column by column.",
      "When approaching any DP problem, a useful checklist is: identify the state, define the recurrence, choose between memoization and tabulation, set base cases, and determine time and space complexity. The state is what uniquely identifies a subproblem, often an index, pair of indices, or a combination of index and capacity or sum. The recurrence defines how the answer for one state depends on smaller states.",
      "As you practice DP, you will begin to recognize common patterns such as 1D sequence DP, 2D grid DP, knapsack-style subset DP, and DP on strings. Building intuition around these patterns helps you quickly map new problems to familiar templates. In AlgoSort, use the visualizer for this topic to see how subproblems are reused and how a DP table fills step by step for Fibonacci and a small grid path example."
    ],
    "example": {
      "language": "python",
      "code": "from functools import lru_cache\n\n# Top-down: Fibonacci with memoization\n@lru_cache(maxsize=None)\ndef fib_memo(n: int) -> int:\n    if n <= 1:\n        return n\n    return fib_memo(n - 1) + fib_memo(n - 2)\n\n# Bottom-up: Fibonacci with tabulation\ndef fib_tab(n: int) -> int:\n    if n <= 1:\n        return n\n    dp = [0] * (n + 1)\n    dp[0], dp[1] = 0, 1\n    for i in range(2, n + 1):\n        dp[i] = dp[i - 1] + dp[i - 2]\n    return dp[n]\n\nprint(fib_memo(10))  # 55\nprint(fib_tab(10))   # 55",
      "input": "Example: n = 10"
    }
  },
    {
    "slug": "dp-1d-sequence",
    "title": "1D Sequence DP: Climbing Stairs & House Robber",
    "summary": "Master dynamic programming on linear sequences with decision-making at each step: take, skip, or choose optimally.",
    "level": "Intermediate",
    "category": "Dynamic Programming",
    "content": [
      "1D Sequence DP is one of the most fundamental dynamic programming patterns where you process elements in a linear sequence (array) and make optimal decisions at each position. Unlike the basic Fibonacci example, these problems involve real-world choices: should you climb 1 or 2 steps? Should you rob this house or skip it? The key is defining a state that captures your position and building a recurrence based on the choices available at that position.",
      "The general template for 1D sequence DP is: define dp[i] as the optimal solution (max/min value, count, etc.) up to index i, then express dp[i] in terms of previous states like dp[i-1] and dp[i-2]. For example, in Climbing Stairs, dp[i] represents the number of ways to reach step i, and since you can arrive from step i-1 (climb 1) or step i-2 (climb 2), the recurrence is dp[i] = dp[i-1] + dp[i-2], identical to Fibonacci but with a different real-world interpretation.",
      "House Robber introduces a decision-making twist: you cannot rob two adjacent houses. Here dp[i] represents the maximum money you can rob up to house i. At each house, you face a choice: rob it (gaining nums[i] + dp[i-2]) or skip it (keeping dp[i-1]). The recurrence becomes dp[i] = max(dp[i-1], nums[i] + dp[i-2]), demonstrating how DP captures trade-offs between conflicting constraints. This pattern extends to many scheduling and selection problems.",
      "Min Cost Climbing Stairs adds a cost array where each step has a cost, and you can start from step 0 or 1, aiming to reach the top with minimum cost. The recurrence is dp[i] = cost[i] + min(dp[i-1], dp[i-2]), showing that you add the current cost to the cheaper of the two previous paths. This variant teaches how DP handles optimization with weighted choices, common in resource allocation and pathfinding problems.",
      "A powerful optimization technique in 1D DP is space reduction. Notice that dp[i] only depends on dp[i-1] and dp[i-2], not the entire history. Instead of maintaining an array of size n, you can use just two variables (prev1, prev2) and update them in a rolling fashion. This reduces space complexity from O(n) to O(1) while keeping the same O(n) time complexity. Many interview problems expect this optimization as a follow-up question.",
      "These 1D patterns generalize to many scenarios: jump games (can you reach the end?), paint house (choose colors with constraints), delete and earn (pick numbers optimally), and more. The mental model is always: identify what state you need to track, determine the choices at each step, write the recurrence combining those choices, set base cases, and decide between tabulation and memoization. Practicing these core problems builds intuition for recognizing similar structures in new problems.",
      "When debugging 1D DP solutions, trace through small examples manually. For Climbing Stairs with n=5, write out dp[0]=1, dp[1]=1, dp[2]=2, dp[3]=3, dp[4]=5, dp[5]=8 and verify each step follows the recurrence. For House Robber with nums=[2,7,9,3,1], check dp=[2,7,11,11,12] and confirm why dp[2]=11 (rob house 0 and 2) and dp[3]=11 (skipping house 3 is better). Visualizing the decision tree or DP table makes these patterns concrete.",
      "Real-world applications of 1D sequence DP include: stock trading strategies (buy/sell with cooldown), task scheduling (maximize profit with non-overlapping intervals), resource management in games (optimal power-ups collection), and even biology (sequence alignment scoring). The simplicity of 1D DP makes it a building block for more complex 2D grid DP, tree DP, and state machine DP problems you will encounter in advanced topics.",
      "As you practice, pay attention to boundary conditions: what are dp[0] and dp[1]? Can you start from index 0 or 1? Does the problem ask for the value at dp[n] or dp[n-1]? Small off-by-one errors are common in DP, so careful initialization and loop range selection are critical. Use the visualizer in AlgoSort to step through examples and see exactly when each state is computed and how choices propagate through the array.",
      "Moving forward, you will combine this 1D sequence pattern with other dimensions (2D grid DP), additional constraints (knapsack with capacity), or string matching (LCS, edit distance). But the core principle remains: break the problem into subproblems, avoid recomputation, and build up the answer systematically. Mastering 1D DP now makes those advanced topics much easier to understand and implement."
    ],
    "example": {
      "language": "python",
      "code": "# Climbing Stairs - Tabulation\ndef climbStairs(n: int) -> int:\n    if n <= 2:\n        return n\n    dp = [0] * (n + 1)\n    dp[1], dp[2] = 1, 2\n    for i in range(3, n + 1):\n        dp[i] = dp[i - 1] + dp[i - 2]\n    return dp[n]\n\n# House Robber - Tabulation\ndef rob(nums: list[int]) -> int:\n    if not nums:\n        return 0\n    if len(nums) == 1:\n        return nums[0]\n    dp = [0] * len(nums)\n    dp[0] = nums[0]\n    dp[1] = max(nums[0], nums[1])\n    for i in range(2, len(nums)):\n        dp[i] = max(dp[i - 1], nums[i] + dp[i - 2])\n    return dp[-1]\n\n# Space-Optimized House Robber - O(1) space\ndef rob_optimized(nums: list[int]) -> int:\n    if not nums:\n        return 0\n    prev2, prev1 = 0, 0\n    for num in nums:\n        prev2, prev1 = prev1, max(prev1, num + prev2)\n    return prev1\n\nprint(climbStairs(5))  # 8\nprint(rob([2,7,9,3,1]))  # 12\nprint(rob_optimized([2,7,9,3,1]))  # 12",
      "input": "Examples: n=5 for stairs, nums=[2,7,9,3,1] for robber"
    }
  },
    {
    "slug": "dp-knapsack",
    "title": "Knapsack DP: 0/1 Knapsack & Subset Problems",
    "summary": "Master 2D dynamic programming for optimization problems with capacity constraints and subset selection.",
    "level": "Intermediate",
    "category": "Dynamic Programming",
    "content": [
      "Knapsack problems represent a fundamental class of optimization problems where you must make binary decisions (take or leave) for each item while respecting a constraint like weight capacity or target sum. The classic 0/1 Knapsack asks: given items with weights and values and a knapsack with capacity W, what is the maximum value you can carry without exceeding the capacity? Unlike 1D sequence DP, knapsack introduces a second dimension to track both the current item index and remaining capacity.",
      "The state definition for 0/1 Knapsack is dp[i][w] representing the maximum value achievable using the first i items with capacity w. At each item i, you face a choice: skip it (keeping dp[i-1][w]) or take it if it fits (gaining value[i] + dp[i-1][w - weight[i]]). The recurrence is dp[i][w] = max(dp[i-1][w], value[i] + dp[i-1][w - weight[i]] if weight[i] <= w). This creates a 2D table where each cell depends on the row above it, making it a bottom-up tabulation problem.",
      "Building the DP table requires careful initialization: dp[0][w] = 0 for all w (no items means zero value) and dp[i][0] = 0 for all i (zero capacity means nothing fits). You then fill the table row by row from item 1 to n and column by column from capacity 1 to W. The final answer is dp[n][W], representing all items considered with full capacity. Tracing back through the table can reconstruct which items were selected by checking where decisions changed.",
      "Subset Sum is a decision variant of knapsack where all items have weight equal to value, and you ask: can you select items that sum exactly to a target? Here dp[i][sum] is a boolean indicating whether you can achieve sum using the first i items. The recurrence becomes dp[i][sum] = dp[i-1][sum] OR dp[i-1][sum - nums[i]], meaning you can reach sum if either you could reach it without item i, or you could reach sum - nums[i] and then add item i. This pattern extends to Partition Equal Subset Sum and Target Sum problems.",
      "Space optimization for knapsack problems is possible because each row only depends on the previous row. Instead of a 2D array of size n × W, you can use a 1D array of size W and update it in reverse order (from W down to 0) to avoid overwriting values you still need. This reduces space complexity from O(n × W) to O(W) while keeping the same O(n × W) time complexity. The reverse iteration is critical: forward iteration would use already-updated values from the current iteration instead of the previous row.",
      "Unbounded Knapsack is a variant where you can take each item unlimited times. The only change is updating the DP array in forward order instead of reverse, because you want to reuse the current row's values (allowing multiple uses of the same item). The recurrence remains similar but now references dp[w - weight[i]] from the same row. This pattern applies to Coin Change II (counting ways) and Coin Change (minimizing coins) problems, both classic unbounded knapsack applications.",
      "Real-world applications of knapsack DP are everywhere: resource allocation with budget constraints, portfolio optimization selecting investments, cutting stock to minimize waste, project selection maximizing ROI under time limits, and even data compression choosing which data to keep. The binary decision framework (include/exclude) maps naturally to many business and engineering problems where you must optimize under constraints.",
      "When implementing knapsack DP, watch for common pitfalls: forgetting to check if item fits before taking it (weight[i] <= w condition), iterating in the wrong order for space optimization, and off-by-one errors in array indexing since dp often uses 1-indexed items but arrays are 0-indexed. Always validate with small examples like 3 items with known optimal solutions before scaling up. The visualizer in AlgoSort shows exactly how the 2D table fills and how the choice at each cell depends on cells above and to the left.",
      "Advanced knapsack variations include: bounded knapsack (limited quantity per item), multi-dimensional knapsack (multiple constraints like weight and volume), and knapsack with dependencies (some items require others). But mastering 0/1 and unbounded knapsack first gives you the foundation to tackle these extensions. The core idea of tracking state in multiple dimensions and making optimal local choices that compose into a global optimum remains constant across all variants.",
      "As you practice knapsack problems, focus on recognizing the pattern: whenever you see binary choices with cumulative constraints or targets, think knapsack. The items might be numbers to sum, jobs to schedule, or features to select, but the DP structure is the same. Building intuition for how the 2D table represents all possible states and how choices propagate through it will make even complex knapsack variants feel approachable."
    ],
    "example": {
      "language": "python",
      "code": "# 0/1 Knapsack - 2D DP\ndef knapsack_2d(weights: list[int], values: list[int], capacity: int) -> int:\n    n = len(weights)\n    dp = [[0] * (capacity + 1) for _ in range(n + 1)]\n    \n    for i in range(1, n + 1):\n        for w in range(1, capacity + 1):\n            # Skip item i-1\n            dp[i][w] = dp[i-1][w]\n            # Take item i-1 if it fits\n            if weights[i-1] <= w:\n                dp[i][w] = max(dp[i][w], values[i-1] + dp[i-1][w - weights[i-1]])\n    \n    return dp[n][capacity]\n\n# Subset Sum - Space Optimized\ndef canPartition(nums: list[int]) -> bool:\n    total = sum(nums)\n    if total % 2 != 0:\n        return False\n    \n    target = total // 2\n    dp = [False] * (target + 1)\n    dp[0] = True\n    \n    for num in nums:\n        # Iterate backwards to avoid using updated values\n        for s in range(target, num - 1, -1):\n            dp[s] = dp[s] or dp[s - num]\n    \n    return dp[target]\n\n# Examples\nweights = [1, 3, 4, 5]\nvalues = [1, 4, 5, 7]\nprint(knapsack_2d(weights, values, 7))  # 9\nprint(canPartition([1, 5, 11, 5]))  # True",
      "input": "weights=[1,3,4,5], values=[1,4,5,7], capacity=7"
    }
  },
    {
    "slug": "dp-strings",
    "title": "String DP: LCS, Edit Distance & Pattern Matching",
    "summary": "Master 2D dynamic programming on strings for sequence comparison, alignment, and transformation problems.",
    "level": "Intermediate",
    "category": "Dynamic Programming",
    "content": [
      "String dynamic programming deals with problems where you compare, align, or transform two sequences (usually strings). The most fundamental pattern is defining dp[i][j] to represent some property of the first i characters of string A and the first j characters of string B. This creates a 2D table where you build up solutions character by character, making local decisions based on whether characters match or differ. String DP appears in bioinformatics (DNA sequence alignment), natural language processing (spell check, autocorrect), and version control (diff algorithms).",
      "Longest Common Subsequence (LCS) asks: what is the longest sequence of characters that appears in both strings in the same order, but not necessarily consecutively? For example, LCS of 'abcde' and 'ace' is 'ace' with length 3. The state dp[i][j] represents the LCS length of A[0..i-1] and B[0..j-1]. If A[i-1] == B[j-1], they contribute to the LCS, so dp[i][j] = 1 + dp[i-1][j-1]. If they differ, take the best from excluding one character: dp[i][j] = max(dp[i-1][j], dp[i][j-1]). This simple recurrence captures the essence of sequence alignment.",
      "Building the LCS table requires initializing dp[0][j] = 0 and dp[i][0] = 0 (empty string has LCS length 0 with anything). You then fill row by row, and each cell depends only on cells above, to the left, and diagonally above-left. The final answer is dp[m][n] where m and n are the string lengths. Backtracking through the table can reconstruct the actual LCS string by following the path of matching characters, though many problems only ask for the length.",
      "Edit Distance (Levenshtein Distance) measures the minimum number of operations (insert, delete, replace) to transform string A into string B. This is more complex than LCS because you have three choices at each step instead of two. The state dp[i][j] represents the edit distance between A[0..i-1] and B[0..j-1]. If A[i-1] == B[j-1], no operation is needed: dp[i][j] = dp[i-1][j-1]. Otherwise, consider three options: replace (dp[i-1][j-1] + 1), delete from A (dp[i-1][j] + 1), or insert into A (dp[i][j-1] + 1), and take the minimum.",
      "The initialization for Edit Distance is different from LCS: dp[i][0] = i (need i deletions to reduce A[0..i-1] to empty) and dp[0][j] = j (need j insertions to build B[0..j-1] from empty). This reflects the cost of transforming between a string and an empty string. The recurrence carefully considers all three edit operations, making it a classic example of DP with multiple transition choices. Understanding why each operation corresponds to a specific cell (diagonal for replace, left for insert, up for delete) is key to mastering this pattern.",
      "Longest Palindromic Subsequence (LPS) is a variation where you find the longest subsequence of a single string that reads the same forwards and backwards. Interestingly, LPS of string S is equivalent to the LCS of S and its reverse. This shows how string DP problems often reduce to each other. You can also solve LPS directly with dp[i][j] representing the LPS length in substring S[i..j], where the recurrence depends on whether S[i] == S[j]. These transformations between problem formulations are powerful tools for solving new string DP variants.",
      "Space optimization for string DP is similar to knapsack: each row depends only on the previous row, so you can use two 1D arrays and alternate between them, or even a single array with careful updating. For LCS and Edit Distance, you can reduce space from O(m × n) to O(min(m, n)) by making the shorter string define the DP array dimension. This is especially important when comparing very long strings like DNA sequences or large text files, where memory can be a bottleneck.",
      "Real-world applications of string DP are everywhere: spell checkers use Edit Distance to find closest dictionary words, version control systems use LCS to compute diffs between file versions, plagiarism detectors compare document subsequences, and bioinformatics tools align protein or DNA sequences to find evolutionary relationships. The abstraction of comparing two sequences generalizes beyond strings to any ordered data: time series, event logs, or even user behavior sequences.",
      "When implementing string DP, be careful with indexing: dp[i][j] typically represents strings up to index i-1 and j-1 (1-indexed DP array, 0-indexed strings). Always verify base cases and the recurrence with small examples like 'ab' and 'ac' before scaling up. The visualizer in AlgoSort shows how the DP table fills diagonally and how matching characters create paths through the table, making the abstract recurrence concrete and intuitive.",
      "Advanced string DP includes problems like Distinct Subsequences (count occurrences of a pattern), Interleaving Strings (check if one string is formed by interleaving two others), and Wildcard Matching (pattern matching with * and ? wildcards). But mastering LCS and Edit Distance first gives you the mental model for all 2D string DP: define the state for prefixes of both strings, handle character match/mismatch cases, and build the table systematically. Once you internalize this pattern, even complex variants become approachable."
    ],
    "example": {
      "language": "python",
      "code": "# Longest Common Subsequence\ndef longestCommonSubsequence(text1: str, text2: str) -> int:\n    m, n = len(text1), len(text2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if text1[i - 1] == text2[j - 1]:\n                dp[i][j] = 1 + dp[i - 1][j - 1]\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n    \n    return dp[m][n]\n\n# Edit Distance\ndef minDistance(word1: str, word2: str) -> int:\n    m, n = len(word1), len(word2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    # Base cases\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if word1[i - 1] == word2[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(\n                    dp[i - 1][j],      # delete\n                    dp[i][j - 1],      # insert\n                    dp[i - 1][j - 1]   # replace\n                )\n    \n    return dp[m][n]\n\n# Examples\nprint(longestCommonSubsequence('abcde', 'ace'))  # 3\nprint(minDistance('horse', 'ros'))  # 3",
      "input": "text1='abcde', text2='ace' for LCS; word1='horse', word2='ros' for Edit Distance"
    }
  },
    {
    "slug": "binary-search-advanced",
    "title": "Binary Search: Advanced Patterns",
    "summary": "Master binary search on answer, rotated arrays, and finding boundaries in complex scenarios.",
    "level": "Intermediate",
    "category": "Binary Search Advanced",
    "content": [
      "Advanced binary search extends beyond finding exact values in sorted arrays to solving optimization problems where you search for the answer itself rather than searching in an array. The key insight is that if you can verify whether a candidate answer works in O(f(n)) time, and the answer space is monotonic (if k works, then k+1 also works or vice versa), you can binary search on the answer space in O(f(n) × log(range)) time. This pattern appears in problems like finding minimum capacity, maximum distance, or threshold values.",
      "Binary search on answer works by defining a range [low, high] for possible answers, then checking the middle value mid. You write a feasibility function that returns true if mid is a valid answer and false otherwise. If mid works, you know the optimal answer is at most mid, so you search [low, mid]. If mid doesn't work, the answer must be larger, so you search [mid+1, high]. The final answer is the smallest (or largest, depending on the problem) value that passes the feasibility check.",
      "Searching in rotated sorted arrays is a classic variation where a sorted array is rotated at an unknown pivot point, like [4,5,6,7,0,1,2]. The array still has sorted portions, and you can identify which half is properly sorted by comparing arr[left] with arr[mid]. If the left half is sorted (arr[left] <= arr[mid]), you check if the target lies within that sorted range. If yes, search there; otherwise, search the right half. This requires careful condition checking to avoid off-by-one errors and handle duplicates.",
      "Finding first and last occurrences of a target in a sorted array with duplicates requires two separate binary searches. For the first occurrence, when you find the target at mid, you don't return immediately but continue searching the left half to see if there's an earlier occurrence: right = mid - 1. For the last occurrence, you search the right half: left = mid + 1. The final answer for first occurrence is left, and for last occurrence is right. This pattern extends to finding lower_bound and upper_bound used in many competitive programming problems.",
      "Peak element problems ask you to find an element that is greater than its neighbors. Even though the array is not fully sorted, binary search works because you can make local decisions: if arr[mid] < arr[mid+1], a peak must exist in the right half (since the sequence is increasing). If arr[mid] > arr[mid+1], a peak must exist in the left half or at mid itself. This works even with multiple peaks because you only need to find one peak, not all of them.",
      "Searching in 2D sorted matrices can be done in O(log(m × n)) by treating the matrix as a flattened sorted array. Map 1D index to 2D coordinates: row = mid / n, col = mid % n. Alternatively, use a staircase search starting from top-right or bottom-left, moving left if the current element is too large or down if it's too small. This takes O(m + n) but is simpler to implement and works even when rows are sorted but columns are not.",
      "Real-world applications of advanced binary search include: allocating server resources (minimize maximum load), cutting materials to minimize waste (maximize minimum piece length), scheduling jobs with deadlines (minimize maximum completion time), and rate limiting (find maximum request rate that doesn't overload). Any optimization problem where you can verify a solution but not directly compute it may be solvable with binary search on the answer.",
      "Common pitfalls include forgetting to handle edge cases like empty arrays, single elements, or all duplicates. Also, be careful with the feasibility function: it must correctly implement the monotonic property. If the function is buggy, binary search will give wrong answers silently. Always test your feasibility function separately on boundary cases before integrating it with binary search. Trace through a small example by hand to verify correctness.",
      "When implementing these patterns, pay attention to whether you need to find minimum or maximum, and whether the condition is greater-than or greater-or-equal. These subtle differences change whether you update left = mid or left = mid + 1. A common template is: if (feasible(mid)) right = mid else left = mid + 1, with final answer being left. But always adapt the template to your specific problem rather than blindly copying it.",
      "Mastering advanced binary search requires practice on diverse problems: Koko eating bananas, capacity to ship packages, split array largest sum, and search in rotated arrays. These problems train you to recognize the binary search pattern even when it's hidden behind problem statements that don't explicitly mention searching. The ability to spot that a problem is 'searchable' is a key competitive programming skill that separates good coders from great ones."
    ],
    "example": {
      "language": "python",
      "code": "# Binary Search on Answer: Koko Eating Bananas\ndef minEatingSpeed(piles: list[int], h: int) -> int:\n    def canFinish(speed: int) -> bool:\n        hours = sum((pile + speed - 1) // speed for pile in piles)\n        return hours <= h\n    \n    left, right = 1, max(piles)\n    while left < right:\n        mid = left + (right - left) // 2\n        if canFinish(mid):\n            right = mid  # Try slower speed\n        else:\n            left = mid + 1  # Need faster speed\n    return left\n\n# Search in Rotated Sorted Array\ndef search_rotated(nums: list[int], target: int) -> int:\n    left, right = 0, len(nums) - 1\n    \n    while left <= right:\n        mid = left + (right - left) // 2\n        if nums[mid] == target:\n            return mid\n        \n        # Check which half is sorted\n        if nums[left] <= nums[mid]:  # Left half sorted\n            if nums[left] <= target < nums[mid]:\n                right = mid - 1\n            else:\n                left = mid + 1\n        else:  # Right half sorted\n            if nums[mid] < target <= nums[right]:\n                left = mid + 1\n            else:\n                right = mid - 1\n    return -1\n\nprint(minEatingSpeed([3,6,7,11], 8))  # 4\nprint(search_rotated([4,5,6,7,0,1,2], 0))  # 4",
      "input": "piles=[3,6,7,11], h=8 for Koko; nums=[4,5,6,7,0,1,2], target=0 for rotated"
    }
  },
  {
    "slug": "greedy-algorithms",
    "title": "Greedy Algorithms: Optimization Patterns",
    "summary": "Learn to solve optimization problems by making locally optimal choices that lead to globally optimal solutions.",
    "level": "Intermediate",
    "category": "Greedy",
    "content": [
      "Greedy algorithms solve optimization problems by making the best local choice at each step, hoping these local choices lead to a global optimum. Unlike dynamic programming which considers all possibilities, greedy algorithms commit to a choice immediately without reconsidering. This makes them fast (often O(n) or O(n log n)) but they only work for problems with specific properties: greedy choice property (local optimum leads to global optimum) and optimal substructure (optimal solution contains optimal solutions to subproblems).",
      "The classic example is the activity selection problem: given start and end times of activities, select the maximum number of non-overlapping activities. The greedy strategy is to always pick the activity that ends earliest, because this leaves the most room for future activities. Sort activities by end time, then iterate through and select each activity whose start time is after the previously selected activity's end time. This simple greedy approach is provably optimal and much faster than trying all subsets.",
      "Interval problems form a major category of greedy problems. In merge intervals, you sort intervals by start time, then merge overlapping ones by comparing the current interval's start with the previous interval's end. In meeting rooms, you check if all meetings can attend by sorting by start time and verifying no overlaps. In meeting rooms II (minimum rooms needed), you can use a greedy approach with a min-heap tracking end times of ongoing meetings, adding a room when a meeting starts before the earliest ending meeting finishes.",
      "Jump game problems test whether you can reach the end of an array where each element represents maximum jump length. The greedy approach maintains the farthest position reachable so far. Iterate through the array, and at each index i, update farthest = max(farthest, i + nums[i]). If at any point i > farthest, you're stuck and return false. If farthest >= last index, return true. For jump game II (minimum jumps), track the current jump boundary and increment jumps when you need to extend the boundary.",
      "Fractional knapsack allows taking fractions of items (unlike 0/1 knapsack which uses DP). The greedy strategy is to sort items by value-to-weight ratio and take items in that order until the capacity is full, possibly taking a fraction of the last item. This works because you can always replace a lower-ratio item fraction with a higher-ratio item fraction to improve the solution. However, this strategy fails for 0/1 knapsack where you can't take fractions, which is why that version requires dynamic programming.",
      "Huffman coding is a greedy algorithm for data compression. It builds an optimal prefix-free binary code by repeatedly combining the two least frequent symbols into a new symbol with their combined frequency, using a min-heap. The resulting binary tree assigns shorter codes to more frequent symbols, minimizing the average code length. This is used in file compression formats like ZIP and JPEG. The greedy choice (combining least frequent symbols) provably leads to the optimal encoding.",
      "Gas station problems ask if you can complete a circular route where each station has gas and the distance to the next station consumes gas. The greedy insight is: if the total gas is less than total cost, it's impossible. Otherwise, start from any station, track your current gas as you move, and if you run out at station i, restart from station i+1 because any station between start and i cannot be the starting point (if you reached i from start and ran out, starting earlier in that range would also fail). The first station where you don't run out is the answer.",
      "Real-world greedy algorithm applications include: task scheduling on processors (minimize idle time), Dijkstra's shortest path (greedily pick nearest unvisited node), Prim's minimum spanning tree (greedily add cheapest edge), load balancing (assign jobs to least loaded server), and caching strategies (evict least recently used item). Many operating system schedulers, network routing protocols, and resource allocators use greedy heuristics for efficiency even when optimal solutions are intractable.",
      "Proving greedy correctness requires showing that the greedy choice is safe (doesn't block future optimal choices) and that after making the greedy choice, the remaining problem has the same structure. Often you prove by exchange argument: assume an optimal solution differs from the greedy choice, then show you can exchange elements to transform it into the greedy solution without worsening it, contradicting optimality. Not all optimization problems have greedy solutions; recognizing when greedy works vs. when DP is needed is a crucial skill.",
      "Common mistakes include applying greedy to problems that need DP (like 0/1 knapsack or longest increasing subsequence), forgetting to sort the input when the greedy strategy depends on order, and choosing the wrong greedy criteria (e.g., picking shortest interval instead of earliest ending interval). Always question: does my greedy choice preserve optimality? Can I construct a counterexample where greedy fails? Test on edge cases like all elements equal, single element, or extreme values to validate your approach."
    ],
    "example": {
      "language": "python",
      "code": "# Activity Selection (Maximum non-overlapping intervals)\ndef max_meetings(start: list[int], end: list[int]) -> int:\n    # Sort by end time\n    meetings = sorted(zip(start, end), key=lambda x: x[1])\n    count = 0\n    last_end = 0\n    \n    for s, e in meetings:\n        if s >= last_end:\n            count += 1\n            last_end = e\n    \n    return count\n\n# Jump Game\ndef canJump(nums: list[int]) -> bool:\n    farthest = 0\n    for i in range(len(nums)):\n        if i > farthest:\n            return False\n        farthest = max(farthest, i + nums[i])\n        if farthest >= len(nums) - 1:\n            return True\n    return True\n\n# Gas Station\ndef canCompleteCircuit(gas: list[int], cost: list[int]) -> int:\n    if sum(gas) < sum(cost):\n        return -1\n    \n    start = 0\n    tank = 0\n    for i in range(len(gas)):\n        tank += gas[i] - cost[i]\n        if tank < 0:\n            start = i + 1\n            tank = 0\n    \n    return start\n\nprint(max_meetings([1,3,0,5,8,5], [2,4,6,7,9,9]))  # 4\nprint(canJump([2,3,1,1,4]))  # True\nprint(canCompleteCircuit([1,2,3,4,5], [3,4,5,1,2]))  # 3",
      "input": "Various greedy problems with optimal local choices"
    }
  },
  {
    "slug": "backtracking-basics",
    "title": "Backtracking: Exploring All Possibilities",
    "summary": "Master the technique of exploring all possible solutions by building candidates incrementally and abandoning them when they fail constraints.",
    "level": "Intermediate",
    "category": "Backtracking",
    "content": [
      "Backtracking is a general algorithmic technique for finding all (or some) solutions to computational problems by incrementally building candidates and abandoning a candidate (backtracking) as soon as it determines the candidate cannot lead to a valid solution. It is essentially a depth-first search of the solution space with pruning. Backtracking is used when you need to explore all possibilities but can eliminate many branches early by checking constraints, making it much faster than brute force.",
      "The template for backtracking involves three components: a choice at each step, constraints that define valid choices, and a goal that defines a complete solution. The recursive structure is: if goal reached, save solution; otherwise, for each valid choice, make the choice, recurse to the next step, then undo the choice (backtrack). This undo step is crucial—it restores the state so you can explore the next choice. Without proper backtracking, you would be exploring with corrupted state.",
      "Subsets (generating all subsets of a set) is the simplest backtracking problem. At each element, you have two choices: include it or exclude it. The recursion tree has 2^n leaves, one for each subset. The template is: at position i, you either add nums[i] to the current subset and recurse to i+1, or skip nums[i] and recurse to i+1. When i reaches n, you've made a choice for all elements, so you save the current subset. This generates all 2^n subsets in O(n × 2^n) time (n to copy each subset).",
      "Permutations (all arrangements of elements) requires tracking which elements have been used. A common approach is to maintain a used array, and at each recursion level, try adding each unused element, mark it as used, recurse, then unmark it. Alternatively, use a swap-based approach: swap the current position with each position from current to end, recurse, then swap back. Both generate all n! permutations in O(n × n!) time. Permutations with duplicates require sorting and skipping duplicate choices to avoid generating the same permutation multiple times.",
      "Combinations (choose k elements from n) can be solved by backtracking with a start index to avoid choosing the same element twice and to ensure combinations rather than permutations. At each level, try adding elements from start to n, recurse with start+1, then remove the element. When the current combination has k elements, save it. This generates C(n, k) combinations. Combination sum problems add a target constraint: only recurse if the current sum doesn't exceed the target, and you can reuse elements by recursing with the same index.",
      "N-Queens places n queens on an n×n chessboard such that no two queens attack each other. For each row, try placing a queen in each column, check if it's safe (no other queen in the same column, diagonal, or anti-diagonal), recurse to the next row, then remove the queen. The constraint checking is the key optimization: maintain sets of occupied columns, diagonals (row - col), and anti-diagonals (row + col) for O(1) conflict checking. Without these, checking safety would be O(n) making the algorithm much slower.",
      "Sudoku solver fills a 9×9 grid following Sudoku rules. For each empty cell, try digits 1-9, check if the digit is valid in the current row, column, and 3×3 box, place it, recurse to the next empty cell, and backtrack if the recursion fails. The key optimization is choosing the order of cells to fill: filling cells with fewer possibilities first (most constrained first) prunes the search tree more effectively. Some implementations precompute possible digits for each cell to speed this up.",
      "Word search in a 2D grid checks if a word exists by trying to form it starting from each cell. For each cell, if it matches the first letter, mark it as visited, recursively search the four neighbors for the next letter, then unmark it. The visited marking prevents using the same cell twice in one path. This is backtracking because you undo the visited mark after the recursion returns, allowing the cell to be used in different paths. Time complexity is O(m × n × 4^L) where L is the word length, but pruning reduces this significantly in practice.",
      "Backtracking optimization techniques include: pruning (stop early when constraints violated), choosing the most constrained variable first, memoization (cache results of subproblems if they repeat), and iterative deepening (limit recursion depth and increase gradually). For combinatorial problems, the search space grows exponentially, so even small optimizations can make the difference between feasible and infeasible runtime. Always look for opportunities to prune: if the current partial solution can't possibly lead to a valid complete solution, backtrack immediately.",
      "Common mistakes in backtracking include forgetting to undo choices (causing incorrect state in later branches), not handling base cases properly (leading to infinite recursion or missing solutions), and inefficient constraint checking (making the algorithm too slow). Always test on small inputs where you can manually verify all solutions. Backtracking is a powerful technique, but it's also easy to get wrong, so careful implementation and testing are essential. Visualizing the recursion tree helps understand the flow and debug issues."
    ],
    "example": {
      "language": "python",
      "code": "# Subsets\ndef subsets(nums: list[int]) -> list[list[int]]:\n    result = []\n    def backtrack(start: int, path: list[int]):\n        result.append(path[:])\n        for i in range(start, len(nums)):\n            path.append(nums[i])\n            backtrack(i + 1, path)\n            path.pop()\n    backtrack(0, [])\n    return result\n\n# Permutations\ndef permute(nums: list[int]) -> list[list[int]]:\n    result = []\n    def backtrack(path: list[int], remaining: list[int]):\n        if not remaining:\n            result.append(path[:])\n            return\n        for i in range(len(remaining)):\n            backtrack(path + [remaining[i]], remaining[:i] + remaining[i+1:])\n    backtrack([], nums)\n    return result\n\n# Combinations\ndef combine(n: int, k: int) -> list[list[int]]:\n    result = []\n    def backtrack(start: int, path: list[int]):\n        if len(path) == k:\n            result.append(path[:])\n            return\n        for i in range(start, n + 1):\n            path.append(i)\n            backtrack(i + 1, path)\n            path.pop()\n    backtrack(1, [])\n    return result\n\nprint(subsets([1,2,3]))  # [[], [1], [2], [1,2], [3], [1,3], [2,3], [1,2,3]]\nprint(permute([1,2,3]))  # [[1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]]\nprint(combine(4, 2))  # [[1,2], [1,3], [1,4], [2,3], [2,4], [3,4]]",
      "input": "nums=[1,2,3] for subsets/permute, n=4, k=2 for combinations"
    }
  },
  {
    "slug": "bit-manipulation-basics",
    "title": "Bit Manipulation: Binary Operations & Tricks",
    "summary": "Master bitwise operators and bit tricks to solve problems efficiently using binary representations.",
    "level": "Intermediate",
    "category": "Bit Manipulation",
    "content": [
      "Bit manipulation uses bitwise operators to perform operations on individual bits of numbers. Computers store data in binary, so bit-level operations are extremely fast. Common bitwise operators are AND (&), OR (|), XOR (^), NOT (~), left shift (<<), and right shift (>>). Understanding these operators and recognizing when to use them can lead to elegant solutions that are faster and use less memory than conventional approaches.",
      "The XOR operator (^) has useful properties: x ^ 0 = x, x ^ x = 0, and XOR is commutative and associative. These properties make XOR perfect for finding elements that appear an odd number of times. For example, to find the single number in an array where every other number appears twice, XOR all numbers together: the pairs cancel out, leaving only the single number. This works in O(n) time and O(1) space, much better than using a hash map.",
      "Bit masks allow you to represent a set using an integer where each bit indicates whether an element is present. For example, the set {0, 2, 5} can be represented as the binary number 100101 (bits 0, 2, and 5 are set). You can add element i with mask |= (1 << i), remove it with mask &= ~(1 << i), check if it's present with (mask & (1 << i)) != 0, and toggle it with mask ^= (1 << i). This is space-efficient for small sets and enables fast subset enumeration.",
      "Generating all subsets using bit manipulation is elegant: for a set of n elements, there are 2^n subsets, which can be represented by integers from 0 to 2^n - 1. For each integer i, the bits of i indicate which elements are in the subset. Iterate i from 0 to 2^n - 1, and for each i, check each bit j to see if element j is included. This is equivalent to backtracking but uses iteration instead of recursion, and it's easier to understand once you grasp the bit pattern concept.",
      "Counting bits (number of 1s in binary representation) is a common operation. The naive approach checks each bit with (n & (1 << i)). A faster trick is Brian Kernighan's algorithm: n & (n - 1) clears the rightmost 1 bit. Repeat until n becomes 0, counting the iterations. For example, 12 (1100) & 11 (1011) = 8 (1000), then 8 & 7 = 0. This takes O(number of 1s) instead of O(log n). Modern CPUs have built-in popcount instructions that count bits in O(1).",
      "Checking if a number is a power of two can be done with a single bit trick: n > 0 && (n & (n - 1)) == 0. Powers of two have exactly one 1 bit in binary (1, 10, 100, 1000, ...). Subtracting 1 flips all bits after that 1 bit, so ANDing them gives 0. For example, 8 (1000) & 7 (0111) = 0. This is much faster than using logarithms or division. Similarly, checking if a number is a power of four can be done with additional checks on bit positions.",
      "Swapping two numbers without a temporary variable uses XOR: a ^= b, b ^= a, a ^= b. After the first operation, a = a ^ b. After the second, b = (a ^ b) ^ b = a. After the third, a = (a ^ b) ^ a = b. This is a classic bit trick, though modern compilers optimize regular swaps well, so it's more of a curiosity than a practical optimization. However, XOR swap is useful in certain scenarios like swapping adjacent elements in an array in-place.",
      "Real-world applications include: network subnet masks (IP address ranges), permission systems (each bit represents a permission), graphics (color manipulation, alpha blending), compression (Huffman coding uses bit streams), cryptography (XOR in stream ciphers), and databases (bitmap indexes for fast queries). Many low-level systems and performance-critical code use bit manipulation extensively. Understanding bits is essential for systems programming and competitive programming.",
      "Advanced bit tricks include: finding the rightmost 1 bit with n & -n (two's complement makes -n flip all bits and add 1), isolating bits with masks, and using bit DP for subset problems (like traveling salesman with bitmask DP). These techniques are not just for puzzles; they appear in real algorithms like hash functions, Bloom filters, and efficient set operations. Mastering bit manipulation opens up a new dimension of problem-solving.",
      "Common mistakes include sign issues (right shift behaves differently for signed vs unsigned), off-by-one errors in bit positions (forgetting bits are 0-indexed), and incorrect precedence (& has lower precedence than ==, so (n & 1 == 0) is parsed as (n & (1 == 0)) which is wrong; use ((n & 1) == 0)). Always use parentheses to clarify bitwise expressions. Test bit manipulation code with small numbers where you can manually verify the binary representation."
    ],
    "example": {
      "language": "python",
      "code": "# Single Number (XOR trick)\ndef singleNumber(nums: list[int]) -> int:\n    result = 0\n    for num in nums:\n        result ^= num\n    return result\n\n# Power of Two\ndef isPowerOfTwo(n: int) -> bool:\n    return n > 0 and (n & (n - 1)) == 0\n\n# Count Bits (Hamming Weight)\ndef hammingWeight(n: int) -> int:\n    count = 0\n    while n:\n        n &= (n - 1)  # Clear rightmost 1 bit\n        count += 1\n    return count\n\n# Subsets using bitmask\ndef subsetsWithBits(nums: list[int]) -> list[list[int]]:\n    n = len(nums)\n    result = []\n    for mask in range(1 << n):  # 2^n subsets\n        subset = []\n        for i in range(n):\n            if mask & (1 << i):  # Check if bit i is set\n                subset.append(nums[i])\n        result.append(subset)\n    return result\n\nprint(singleNumber([4,1,2,1,2]))  # 4\nprint(isPowerOfTwo(16))  # True\nprint(hammingWeight(11))  # 3 (1011 has three 1s)\nprint(subsetsWithBits([1,2]))  # [[], [1], [2], [1,2]]",
      "input": "Various bit manipulation examples"
    }
  },
    {
    "slug": "trie-prefix-tree",
    "title": "Trie (Prefix Tree)",
    "summary": "Master trie data structure for efficient string storage, prefix matching, and autocomplete systems.",
    "level": "Advanced",
    "category": "Advanced Data Structures",
    "content": [
      "A Trie (pronounced 'try') is a tree where each path from root to node spells a string. Root is empty, each edge labeled with a character. Used for efficient prefix operations.",
      "Insert operation: Start at root. For each character in word, check if edge with that character exists. If yes, follow it. If no, create new child node. Mark last node as 'end of word'.",
      "Example insert 'cat': Root → 'c' (create node) → 'a' (create node) → 't' (create node, mark as end). Now insert 'car': Root → 'c' (exists!) → 'a' (exists!) → 'r' (create node, mark as end).",
      "Search operation: Start at root. For each character, try to follow edge with that character. If edge doesn't exist, word not in trie (return false). If reach end, check 'end of word' flag.",
      "Prefix search (startsWith): Same as search, but don't check 'end of word' flag. Just verify path exists. Example: searching 'ca' in trie with 'cat', 'car' returns true.",
      "Time complexity: Insert O(m), Search O(m), Prefix O(m) where m = word length. Independent of how many words stored! Space: O(ALPHABET_SIZE × total_chars) worst case.",
      "Use cases: Autocomplete (find all words starting with prefix), spell checker, IP routing, T9 phone typing, dictionary with fast prefix queries.",
      "Trie vs HashMap: HashMap finds exact word in O(1) but can't do 'find all words starting with X'. Trie excels at prefix-based queries and can iterate words in sorted order."
    ],


    "example": {
      "language": "python",
      "code": "# Trie (Prefix Tree) Implementation\n\nclass TrieNode:\n    def __init__(self):\n        self.children = {}  # character -> TrieNode\n        self.is_end_of_word = False\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()\n    \n    def insert(self, word: str) -> None:\n        \"\"\"Insert a word into the trie. O(m) time.\"\"\"\n        node = self.root\n        for char in word:\n            if char not in node.children:\n                node.children[char] = TrieNode()\n            node = node.children[char]\n        node.is_end_of_word = True\n    \n    def search(self, word: str) -> bool:\n        \"\"\"Search if word exists in trie. O(m) time.\"\"\"\n        node = self.root\n        for char in word:\n            if char not in node.children:\n                return False\n            node = node.children[char]\n        return node.is_end_of_word\n    \n    def startsWith(self, prefix: str) -> bool:\n        \"\"\"Check if any word starts with prefix. O(m) time.\"\"\"\n        node = self.root\n        for char in prefix:\n            if char not in node.children:\n                return False\n            node = node.children[char]\n        return True\n    \n    def findWordsWithPrefix(self, prefix: str) -> list:\n        \"\"\"Find all words starting with given prefix.\"\"\"\n        node = self.root\n        # Navigate to prefix end\n        for char in prefix:\n            if char not in node.children:\n                return []\n            node = node.children[char]\n        \n        # DFS to collect all words from this point\n        result = []\n        self._dfs(node, prefix, result)\n        return result\n    \n    def _dfs(self, node, path, result):\n        if node.is_end_of_word:\n            result.append(path)\n        for char, child in node.children.items():\n            self._dfs(child, path + char, result)\n\n# Example Usage\ntrie = Trie()\nwords = [\"apple\", \"app\", \"application\", \"apply\", \"banana\", \"band\"]\n\nprint(\"Inserting words:\", words)\nfor word in words:\n    trie.insert(word)\n\nprint(\"\\nSearch Results:\")\nprint(f\"Search 'apple': {trie.search('apple')}\")  # True\nprint(f\"Search 'app': {trie.search('app')}\")      # True\nprint(f\"Search 'appl': {trie.search('appl')}\")    # False (prefix only)\n\nprint(\"\\nPrefix Search:\")\nprint(f\"StartsWith 'app': {trie.startsWith('app')}\")  # True\nprint(f\"StartsWith 'ban': {trie.startsWith('ban')}\")  # True\nprint(f\"StartsWith 'cat': {trie.startsWith('cat')}\")  # False\n\nprint(\"\\nAutocomplete (words with prefix 'app'):\")\nprint(trie.findWordsWithPrefix('app'))  # ['apple', 'app', 'application', 'apply']\n",
      "input": ""
    }
  },
  {
    "slug": "union-find-dsu",
    "title": "Union Find (Disjoint Set Union)",
    "summary": "Learn Union-Find for efficient connectivity queries, cycle detection, and dynamic graph components.",
    "level": "Advanced",
    "category": "Advanced Data Structures",
    "content": [
      "Union-Find tracks elements partitioned into disjoint sets. Supports two operations: Find(x) - which set is x in? Union(x,y) - merge x's set with y's set. Both nearly O(1)!",
      "Structure: Each element has a parent pointer. Initially, each element is its own parent (separate sets). Root of tree is representative - elements with parent[i] = i.",
      "Find operation: Follow parent pointers until reaching root. Example: Find(3) where parent[3]=1, parent[1]=0, parent[0]=0 → follow 3→1→0, return 0 (the root).",
      "Path compression (optimization): During Find(3), make all nodes on path point directly to root. Set parent[3]=0, parent[1]=0. Next Find(3) is instant! This flattens tree.",
      "Union operation: Find roots of both elements. Make one root point to the other. Example: Union(3,5) where root(3)=0, root(5)=2 → set parent[2]=0. Now 0 is root of merged set.",
      "Union by rank (optimization): Track tree height (rank). When merging, attach smaller tree under larger tree's root. Keeps trees shallow. If equal rank, pick either and increment rank.",
      "With both optimizations: Find and Union both O(α(n)) amortized time, where α is inverse Ackermann function ≈ 4 for any practical n. Effectively constant time!",
      "Use cases: Kruskal's MST algorithm, cycle detection in undirected graphs, connected components, dynamic connectivity queries, 'are these in same group?' problems."
    ],


    "example": {
      "language": "python",
      "code": "# Union Find (Disjoint Set Union) Implementation\n\nclass UnionFind:\n    def __init__(self, n: int):\n        \"\"\"Initialize n disjoint sets (0 to n-1).\"\"\"\n        self.parent = list(range(n))  # Each element is its own parent initially\n        self.rank = [0] * n           # Rank for union by rank optimization\n        self.components = n           # Track number of connected components\n    \n    def find(self, x: int) -> int:\n        \"\"\"Find root of set containing x with path compression.\"\"\"\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])  # Path compression\n        return self.parent[x]\n    \n    def union(self, x: int, y: int) -> bool:\n        \"\"\"Union sets containing x and y. Returns True if they were in different sets.\"\"\"\n        root_x = self.find(x)\n        root_y = self.find(y)\n        \n        if root_x == root_y:\n            return False  # Already in same set\n        \n        # Union by rank: attach smaller tree under larger tree\n        if self.rank[root_x] < self.rank[root_y]:\n            self.parent[root_x] = root_y\n        elif self.rank[root_x] > self.rank[root_y]:\n            self.parent[root_y] = root_x\n        else:\n            self.parent[root_y] = root_x\n            self.rank[root_x] += 1\n        \n        self.components -= 1\n        return True\n    \n    def connected(self, x: int, y: int) -> bool:\n        \"\"\"Check if x and y are in same connected component.\"\"\"\n        return self.find(x) == self.find(y)\n    \n    def count_components(self) -> int:\n        \"\"\"Return number of disjoint sets.\"\"\"\n        return self.components\n\n# Example 1: Network Connectivity\nprint(\"=== Network Connectivity ===\")\nuf = UnionFind(6)  # 6 nodes: 0,1,2,3,4,5\nprint(f\"Initial components: {uf.count_components()}\")\n\n# Connect some nodes\nconnections = [(0, 1), (1, 2), (3, 4)]\nfor x, y in connections:\n    uf.union(x, y)\n    print(f\"Connected {x}-{y}, Components: {uf.count_components()}\")\n\nprint(f\"\\nAre 0 and 2 connected? {uf.connected(0, 2)}\")  # True (via 1)\nprint(f\"Are 0 and 3 connected? {uf.connected(0, 3)}\")    # False\nprint(f\"Are 3 and 4 connected? {uf.connected(3, 4)}\")    # True\n\n# Example 2: Cycle Detection\nprint(\"\\n=== Cycle Detection ===\")\nedges = [(0, 1), (1, 2), (2, 3), (3, 1)]  # Last edge creates cycle\nuf2 = UnionFind(4)\n\nfor u, v in edges:\n    if uf2.connected(u, v):\n        print(f\"Cycle detected! Edge ({u}, {v}) connects already connected nodes.\")\n        break\n    else:\n        uf2.union(u, v)\n        print(f\"Added edge ({u}, {v})\")\n\n# Example 3: Connected Components Count\nprint(\"\\n=== Island Problem Simulation ===\")\n# Imagine 5 islands, some get connected by bridges\nislands = UnionFind(5)\nprint(f\"Initial islands: {islands.count_components()}\")\n\nbridges = [(0, 1), (2, 3)]  # Connect island 0-1 and 2-3\nfor i, j in bridges:\n    islands.union(i, j)\nprint(f\"After bridges {bridges}: {islands.count_components()} island groups\")\nprint(\"Groups: {0,1}, {2,3}, {4}\")\n",
      "input": ""
    }
  },
  {
    "slug": "segment-tree",
    "title": "Segment Tree",
    "summary": "Master segment trees for efficient range queries and updates on arrays with logarithmic complexity.",
    "level": "Advanced",
    "category": "Advanced Data Structures",
    "content": [
      "Segment Tree enables O(log n) range queries (sum/min/max) AND O(log n) updates on arrays. Brute force is O(n) per query. Useful when many queries + updates needed.",
      "Structure: Binary tree where each leaf = one array element, each internal node = merge of children's segments. Root represents entire array [0, n-1]. Tree stored in array like heap.",
      "Build operation: Start at leaves (copy array values), then work upward. Each parent = sum (or min/max) of its two children. Build takes O(n) time. Tree size is ~4n.",
      "Range query [L, R]: Start at root. At each node, check segment overlap. No overlap? Return 0. Complete overlap? Return node value. Partial overlap? Query both children and combine.",
      "Query example: Query [2,5] in tree for array [1,3,5,7,9,11]. Root splits into left [0,2] and right [3,5]. Left has partial overlap → query deeper. Right complete overlap → use directly.",
      "Point update: Change single element arr[i]. Follow path from leaf i to root, updating each ancestor. At leaf, set new value. At each parent, recalculate as left_child + right_child.",
      "Update example: Change arr[2]=5 to arr[2]=10. Update leaf node. Then update parent covering [2,3], then grandparent covering [0,3], all the way to root. Only O(log n) nodes affected.",
      "When to use: Frequent range queries + updates on fixed-size array. If only queries (no updates), use prefix sum. If only updates at end, simpler structures work."
    ],


    "example": {
      "language": "python",
      "code": "# Segment Tree Implementation (Range Sum Query)\n\nclass SegmentTree:\n    def __init__(self, arr: list):\n        \"\"\"Build segment tree from array.\"\"\"\n        self.n = len(arr)\n        self.tree = [0] * (4 * self.n)  # Allocate space for tree\n        if self.n > 0:\n            self._build(arr, 0, 0, self.n - 1)\n    \n    def _build(self, arr: list, node: int, start: int, end: int):\n        \"\"\"Recursively build tree. node is current tree node index.\"\"\"\n        if start == end:\n            # Leaf node - store array element\n            self.tree[node] = arr[start]\n        else:\n            mid = (start + end) // 2\n            left_child = 2 * node + 1\n            right_child = 2 * node + 2\n            \n            # Build left and right subtrees\n            self._build(arr, left_child, start, mid)\n            self._build(arr, right_child, mid + 1, end)\n            \n            # Internal node stores sum of children\n            self.tree[node] = self.tree[left_child] + self.tree[right_child]\n    \n    def update(self, idx: int, value: int):\n        \"\"\"Update element at index idx to value.\"\"\"\n        self._update(0, 0, self.n - 1, idx, value)\n    \n    def _update(self, node: int, start: int, end: int, idx: int, value: int):\n        \"\"\"Recursively update tree.\"\"\"\n        if start == end:\n            # Leaf node - update value\n            self.tree[node] = value\n        else:\n            mid = (start + end) // 2\n            left_child = 2 * node + 1\n            right_child = 2 * node + 2\n            \n            if idx <= mid:\n                self._update(left_child, start, mid, idx, value)\n            else:\n                self._update(right_child, mid + 1, end, idx, value)\n            \n            # Update current node\n            self.tree[node] = self.tree[left_child] + self.tree[right_child]\n    \n    def query(self, L: int, R: int) -> int:\n        \"\"\"Get sum of elements in range [L, R].\"\"\"\n        return self._query(0, 0, self.n - 1, L, R)\n    \n    def _query(self, node: int, start: int, end: int, L: int, R: int) -> int:\n        \"\"\"Recursively compute range sum.\"\"\"\n        # No overlap\n        if R < start or L > end:\n            return 0\n        \n        # Complete overlap\n        if L <= start and end <= R:\n            return self.tree[node]\n        \n        # Partial overlap - query both children\n        mid = (start + end) // 2\n        left_sum = self._query(2 * node + 1, start, mid, L, R)\n        right_sum = self._query(2 * node + 2, mid + 1, end, L, R)\n        return left_sum + right_sum\n\n# Example Usage\narr = [1, 3, 5, 7, 9, 11]\nprint(f\"Array: {arr}\")\nseg_tree = SegmentTree(arr)\n\nprint(\"\\n=== Range Sum Queries ===\")\nprint(f\"Sum of range [1, 3]: {seg_tree.query(1, 3)}\")  # 3+5+7 = 15\nprint(f\"Sum of range [0, 5]: {seg_tree.query(0, 5)}\")  # 1+3+5+7+9+11 = 36\nprint(f\"Sum of range [2, 4]: {seg_tree.query(2, 4)}\")  # 5+7+9 = 21\n\nprint(\"\\n=== Update and Query ===\")\nprint(f\"Original arr[2] = {arr[2]}\")\nseg_tree.update(2, 10)  # Change arr[2] from 5 to 10\nprint(f\"After update arr[2] = 10\")\nprint(f\"Sum of range [1, 3]: {seg_tree.query(1, 3)}\")  # 3+10+7 = 20\nprint(f\"Sum of range [0, 5]: {seg_tree.query(0, 5)}\")  # 1+3+10+7+9+11 = 41\n\n# Performance comparison\nprint(\"\\n=== Complexity Comparison ===\")\nprint(\"Naive approach: Range sum O(n), Update O(1)\")\nprint(\"Segment Tree: Range sum O(log n), Update O(log n)\")\nprint(\"Best for: Frequent queries + updates on static-size arrays\")\n",
      "input": ""
    }
  },
  {
    "slug": "fenwick-tree-bit",
    "title": "Binary Indexed Tree (Fenwick Tree)",
    "summary": "Learn BIT/Fenwick Tree for space-efficient prefix sums and range updates with elegant bit manipulation.",
    "level": "Advanced",
    "category": "Advanced Data Structures",
    "content": [
      "Problem solved: 'Find sum from start to any index' (prefix sum) + 'change any element' - both need to be fast. Fenwick Tree does both in O(log n)!",
      "Simpler than Segment Tree: Just an array! But clever bit manipulation determines which ranges each index stores. Half the code, same power (for sum queries).",
      "Key insight: Index i stores sum of last (i & -i) elements. Example: index 6 stores sum of 2 elements, index 8 stores sum of 8 elements.",
      "Bit trick explained: (i & -i) extracts last 1-bit. If i = 6 = 110₂, then i&-i = 2 = 010₂. This determines how many elements this position covers.",
      "Update operation: Change arr[i] by adding delta. Then update parent nodes by adding (i & -i) repeatedly until exceed array size.",
      "Query operation: Sum[0..i] is found by adding values at i, then i - (i&-i), then repeat until reach 0. Visits only O(log n) positions!",
      "Limitation: Only works for operations that are reversible (sum, XOR). Cannot do min/max queries - use Segment Tree for that.",
      "When to use: Counting problems (inversions, frequency), prefix sums with updates, competitive programming (fast to code). 2D version handles matrix range sums!"
    ],

    "example": {
      "language": "python",
      "code": "# Binary Indexed Tree (Fenwick Tree) Implementation\n\nclass FenwickTree:\n    def __init__(self, n: int):\n        \"\"\"Initialize BIT for array of size n (1-indexed internally).\"\"\"\n        self.n = n\n        self.tree = [0] * (n + 1)  # 1-indexed\n    \n    def update(self, idx: int, delta: int):\n        \"\"\"Add delta to element at index idx (0-indexed input).\"\"\"\n        idx += 1  # Convert to 1-indexed\n        while idx <= self.n:\n            self.tree[idx] += delta\n            idx += idx & -idx  # Move to parent (add last set bit)\n    \n    def prefix_sum(self, idx: int) -> int:\n        \"\"\"Get sum of elements from index 0 to idx (0-indexed input).\"\"\"\n        idx += 1  # Convert to 1-indexed\n        result = 0\n        while idx > 0:\n            result += self.tree[idx]\n            idx -= idx & -idx  # Move to previous range (remove last set bit)\n        return result\n    \n    def range_sum(self, left: int, right: int) -> int:\n        \"\"\"Get sum of elements from index left to right (both inclusive, 0-indexed).\"\"\"\n        if left == 0:\n            return self.prefix_sum(right)\n        return self.prefix_sum(right) - self.prefix_sum(left - 1)\n\n# Helper to build from existing array\ndef build_fenwick_tree(arr: list) -> FenwickTree:\n    \"\"\"Build Fenwick Tree from array.\"\"\"\n    bit = FenwickTree(len(arr))\n    for i, val in enumerate(arr):\n        bit.update(i, val)\n    return bit\n\n# Example 1: Basic Operations\nprint(\"=== Basic Fenwick Tree Operations ===\")\narr = [3, 2, -1, 6, 5, 4, -3, 3, 7, 2]\nprint(f\"Array: {arr}\")\nbit = build_fenwick_tree(arr)\n\nprint(\"\\nPrefix Sums:\")\nfor i in range(len(arr)):\n    print(f\"Sum[0..{i}] = {bit.prefix_sum(i)}\")\n\nprint(\"\\nRange Sum Queries:\")\nprint(f\"Sum[2..5] = {bit.range_sum(2, 5)}\")  # -1+6+5+4 = 14\nprint(f\"Sum[0..3] = {bit.range_sum(0, 3)}\")  # 3+2-1+6 = 10\nprint(f\"Sum[5..9] = {bit.range_sum(5, 9)}\")  # 4-3+3+7+2 = 13\n\nprint(\"\\n=== Update Operation ===\")\nprint(f\"Original arr[3] = {arr[3]}\")\nbit.update(3, 4)  # Add 4 to index 3 (6 becomes 10)\nprint(f\"After adding 4 to index 3:\")\nprint(f\"Sum[2..5] = {bit.range_sum(2, 5)}\")  # Should increase by 4: 18\n\n# Example 2: Inversion Count Problem\nprint(\"\\n=== Counting Inversions ===\")\ndef count_inversions(arr: list) -> int:\n    \"\"\"Count pairs (i,j) where i<j but arr[i]>arr[j] using BIT.\"\"\"\n    # Coordinate compression\n    sorted_vals = sorted(set(arr))\n    rank = {v: i for i, v in enumerate(sorted_vals)}\n    \n    bit = FenwickTree(len(sorted_vals))\n    inversions = 0\n    \n    for i, val in enumerate(arr):\n        # Count how many elements seen so far are greater than current\n        r = rank[val]\n        inversions += i - bit.prefix_sum(r)\n        bit.update(r, 1)\n    \n    return inversions\n\ntest_arr = [5, 2, 6, 1]\nprint(f\"Array: {test_arr}\")\nprint(f\"Number of inversions: {count_inversions(test_arr)}\")\nprint(\"Inversions: (5,2), (5,1), (2,1), (6,1) = 4 pairs\")\n\n# Example 3: Bit Manipulation Insight\nprint(\"\\n=== Understanding i & -i (Last Set Bit) ===\")\nfor i in range(1, 9):\n    last_bit = i & -i\n    print(f\"i={i:2d}, binary={i:04b}, i&-i={last_bit}, covers range of {last_bit} elements\")\n",
      "input": ""
    }
  },
    {
    "slug": "monotonic-stack-queue",
    "title": "Monotonic Stack & Queue",
    "summary": "Master monotonic data structures for efficient next greater/smaller element and sliding window problems.",
    "level": "Advanced",
    "category": "Advanced Data Structures",
    "content": [
      "Problem: Find next greater element for every element in array, or maintain min/max in sliding window. Brute force is O(n²) - too slow!",
      "Monotonic Stack: Stack that maintains elements in increasing or decreasing order. When pushing, pop all elements that violate the order.",
      "Key insight: Each element pushed/popped at most once, so total time is O(n) despite seeming like nested loops!",
      "Use cases: Next/Previous Greater/Smaller Element, largest rectangle in histogram, stock span problems, trapping rain water.",
      "Monotonic Queue: Similar concept but uses deque (double-ended queue) to efficiently maintain min/max in sliding window.",
      "Pattern recognition: When problem asks 'for each element, find first element to left/right that is greater/smaller', think monotonic stack.",
      "Implementation tip: Store indices instead of values in stack/queue to track positions and calculate distances.",
      "Real-world analogy: Stack of books where you only keep taller books. When new book arrives, remove all shorter books on top, then add it."
    ],
    "example": {
      "language": "python",
      "code": "# Monotonic Stack & Queue Examples\n\nfrom collections import deque\n\n# Example 1: Next Greater Element using Monotonic Stack\ndef next_greater_elements(nums):\n    \"\"\"For each element, find next greater element to the right.\"\"\"\n    n = len(nums)\n    result = [-1] * n\n    stack = []  # Monotonic decreasing stack (stores indices)\n    \n    for i in range(n):\n        # While stack not empty and current element is greater\n        while stack and nums[i] > nums[stack[-1]]:\n            idx = stack.pop()\n            result[idx] = nums[i]\n        stack.append(i)\n    \n    return result\n\nprint(\"=== Next Greater Element ===\")\narr = [4, 5, 2, 10, 8]\nprint(f\"Array: {arr}\")\nprint(f\"Next Greater: {next_greater_elements(arr)}\")\nprint(\"Explanation: [5, 10, 10, -1, -1]\")\n\n# Example 2: Previous Smaller Element\ndef previous_smaller_elements(nums):\n    \"\"\"For each element, find previous smaller element to the left.\"\"\"\n    n = len(nums)\n    result = [-1] * n\n    stack = []  # Monotonic increasing stack\n    \n    for i in range(n):\n        # While stack not empty and current element is smaller/equal\n        while stack and nums[i] <= nums[stack[-1]]:\n            stack.pop()\n        \n        if stack:\n            result[i] = nums[stack[-1]]\n        stack.append(i)\n    \n    return result\n\nprint(\"\\n=== Previous Smaller Element ===\")\narr2 = [4, 5, 2, 10, 8]\nprint(f\"Array: {arr2}\")\nprint(f\"Previous Smaller: {previous_smaller_elements(arr2)}\")\nprint(\"Explanation: [-1, 4, -1, 2, 2]\")\n\n# Example 3: Largest Rectangle in Histogram\ndef largest_rectangle_area(heights):\n    \"\"\"Find largest rectangle area in histogram using monotonic stack.\"\"\"\n    stack = []\n    max_area = 0\n    heights = heights + [0]  # Add sentinel to handle remaining elements\n    \n    for i, h in enumerate(heights):\n        # Pop bars taller than current height\n        while stack and heights[stack[-1]] > h:\n            height_idx = stack.pop()\n            height = heights[height_idx]\n            width = i if not stack else i - stack[-1] - 1\n            max_area = max(max_area, height * width)\n        stack.append(i)\n    \n    return max_area\n\nprint(\"\\n=== Largest Rectangle in Histogram ===\")\nhistogram = [2, 1, 5, 6, 2, 3]\nprint(f\"Heights: {histogram}\")\nprint(f\"Largest area: {largest_rectangle_area(histogram)}\")\nprint(\"Explanation: Rectangle with height 5-6 has area 10\")\n\n# Example 4: Sliding Window Maximum using Monotonic Queue\ndef sliding_window_maximum(nums, k):\n    \"\"\"Find maximum in each sliding window of size k.\"\"\"\n    dq = deque()  # Stores indices in decreasing order of values\n    result = []\n    \n    for i in range(len(nums)):\n        # Remove elements outside current window\n        while dq and dq[0] < i - k + 1:\n            dq.popleft()\n        \n        # Remove smaller elements (they'll never be max)\n        while dq and nums[i] > nums[dq[-1]]:\n            dq.pop()\n        \n        dq.append(i)\n        \n        # Add to result once window is full\n        if i >= k - 1:\n            result.append(nums[dq[0]])\n    \n    return result\n\nprint(\"\\n=== Sliding Window Maximum ===\")\narr3 = [1, 3, -1, -3, 5, 3, 6, 7]\nk = 3\nprint(f\"Array: {arr3}, Window size: {k}\")\nprint(f\"Window maximums: {sliding_window_maximum(arr3, k)}\")\nprint(\"Explanation: [3, 3, 5, 5, 6, 7]\")\n\n# Example 5: Stock Span Problem\ndef stock_span(prices):\n    \"\"\"For each day, find how many consecutive days before had price <= current.\"\"\"\n    n = len(prices)\n    span = [1] * n\n    stack = []  # Monotonic decreasing stack\n    \n    for i in range(n):\n        while stack and prices[i] >= prices[stack[-1]]:\n            stack.pop()\n        \n        span[i] = i + 1 if not stack else i - stack[-1]\n        stack.append(i)\n    \n    return span\n\nprint(\"\\n=== Stock Span Problem ===\")\nprices = [100, 80, 60, 70, 60, 75, 85]\nprint(f\"Prices: {prices}\")\nprint(f\"Span: {stock_span(prices)}\")\nprint(\"Explanation: [1, 1, 1, 2, 1, 4, 6]\")\nprint(\"Day 6: Price 85 >= last 6 days (75,60,70,60,80,100? No, stops at 100)\")\n\nprint(\"\\n=== Time Complexity ===\")\nprint(\"All operations: O(n) - each element pushed/popped at most once\")\nprint(\"Space: O(n) for stack/deque\")\n",
      "input": ""
    }
  },
  {
    "slug": "sparse-table",
    "title": "Sparse Table for Range Queries",
    "summary": "Learn sparse table for O(1) range minimum/maximum queries on static arrays using precomputation.",
    "level": "Advanced",
    "category": "Advanced Data Structures",
    "content": [
      "Problem: Need to answer many range min/max queries on an array that doesn't change. Segment Tree works but Sparse Table can query in O(1)!",
      "Key idea: Precompute answers for all ranges of length 2^k. Any range [L, R] can be covered by at most 2 overlapping power-of-2 ranges.",
      "Preprocessing: Build 2D table where table[i][j] = min/max of range [i, i+2^j-1]. Takes O(n log n) time and space.",
      "Query: For range [L, R], find largest k where 2^k <= (R-L+1). Answer is min(table[L][k], table[R-2^k+1][k]). Just O(1) time!",
      "Why overlapping works: For idempotent operations (min, max, GCD) where f(x,x)=x, overlapping doesn't affect result. Doesn't work for sum!",
      "Use cases: Range minimum/maximum query (RMQ), lowest common ancestor (LCA) in trees, static array queries without updates.",
      "Sparse Table vs Segment Tree: Sparse Table - O(n log n) build, O(1) query, no updates. Segment Tree - O(n) build, O(log n) query, O(log n) update.",
      "When to use: Static array with many queries. If updates needed, use Segment Tree or Fenwick Tree instead."
    ],
    "example": {
      "language": "python",
      "code": "# Sparse Table Implementation\n\nimport math\n\nclass SparseTable:\n    def __init__(self, arr: list, operation='min'):\n        \"\"\"Build sparse table for range queries.\n        operation: 'min' or 'max' (idempotent operations only)\n        \"\"\"\n        self.n = len(arr)\n        self.operation = operation\n        self.op_func = min if operation == 'min' else max\n        \n        # Calculate max power of 2 needed\n        self.max_log = math.floor(math.log2(self.n)) + 1\n        \n        # table[i][j] = op of range [i, i + 2^j - 1]\n        self.table = [[0] * self.max_log for _ in range(self.n)]\n        \n        # Build table\n        self._build(arr)\n    \n    def _build(self, arr: list):\n        \"\"\"Precompute all power-of-2 range queries.\"\"\"\n        # Base case: ranges of length 1\n        for i in range(self.n):\n            self.table[i][0] = arr[i]\n        \n        # Fill table for increasing powers of 2\n        j = 1\n        while (1 << j) <= self.n:  # 2^j <= n\n            i = 0\n            while (i + (1 << j) - 1) < self.n:\n                # Combine two halves of length 2^(j-1)\n                left = self.table[i][j - 1]\n                right = self.table[i + (1 << (j - 1))][j - 1]\n                self.table[i][j] = self.op_func(left, right)\n                i += 1\n            j += 1\n    \n    def query(self, L: int, R: int):\n        \"\"\"Query operation over range [L, R] in O(1) time.\"\"\"\n        if L < 0 or R >= self.n or L > R:\n            raise ValueError(f\"Invalid range [{L}, {R}]\")\n        \n        # Find largest power of 2 that fits in range\n        length = R - L + 1\n        k = math.floor(math.log2(length))\n        \n        # Query two (possibly overlapping) ranges of length 2^k\n        left_result = self.table[L][k]\n        right_result = self.table[R - (1 << k) + 1][k]\n        \n        return self.op_func(left_result, right_result)\n\n# Example 1: Range Minimum Query\nprint(\"=== Range Minimum Query (RMQ) ===\")\narr = [2, 5, 1, 7, 3, 9, 4, 6]\nprint(f\"Array: {arr}\")\n\nsparse_min = SparseTable(arr, operation='min')\n\nqueries = [(0, 3), (2, 5), (1, 7), (4, 6)]\nprint(\"\\nQueries (0-indexed):\")\nfor L, R in queries:\n    result = sparse_min.query(L, R)\n    print(f\"  Min in range [{L}, {R}]: {result} (subarray: {arr[L:R+1]})\")\n\n# Example 2: Range Maximum Query\nprint(\"\\n=== Range Maximum Query ===\")\narr2 = [3, 1, 4, 1, 5, 9, 2, 6]\nprint(f\"Array: {arr2}\")\n\nsparse_max = SparseTable(arr2, operation='max')\n\nqueries2 = [(0, 3), (2, 6), (4, 7), (0, 7)]\nprint(\"\\nQueries (0-indexed):\")\nfor L, R in queries2:\n    result = sparse_max.query(L, R)\n    print(f\"  Max in range [{L}, {R}]: {result} (subarray: {arr2[L:R+1]})\")\n\n# Example 3: Performance Comparison\nprint(\"\\n=== Performance Comparison ===\")\nprint(\"\\nNaive approach:\")\nprint(\"  Build: O(1), Query: O(n)\")\nprint(\"  Good for: Single query or frequent updates\")\n\nprint(\"\\nSparse Table:\")\nprint(\"  Build: O(n log n), Query: O(1)\")\nprint(\"  Good for: Many queries, no updates, min/max/GCD operations\")\n\nprint(\"\\nSegment Tree:\")\nprint(\"  Build: O(n), Query: O(log n), Update: O(log n)\")\nprint(\"  Good for: Queries + updates, any associative operation\")\n\nprint(\"\\nFenwick Tree:\")\nprint(\"  Build: O(n log n), Query: O(log n), Update: O(log n)\")\nprint(\"  Good for: Sum/XOR queries + updates, less space than segment tree\")\n\n# Example 4: Why overlapping works for min/max\nprint(\"\\n=== Why Overlapping Works for Min/Max ===\")\nprint(\"Range [2, 5] in array [2, 5, 1, 7, 3, 9]:\")\nprint(\"  Option 1: min([1, 7, 3, 9]) = 1\")\nprint(\"  Option 2: min(min([1, 7]), min([3, 9])) = min(1, 3) = 1\")\nprint(\"  Option 3: min(min([1, 7, 3]), min([7, 3, 9])) = min(1, 3) = 1\")\nprint(\"  → Overlapping doesn't change result for min/max (idempotent)\")\nprint(\"\\nWhy not for sum:\")\nprint(\"  sum([1, 7, 3, 9]) = 20\")\nprint(\"  sum([1, 7, 3]) + sum([7, 3, 9]) = 11 + 19 = 30 ❌ (7 and 3 counted twice!)\")\n\n# Example 5: Table Structure Visualization\nprint(\"\\n=== Sparse Table Structure for [2, 5, 1, 7] ===\")\nsmall_arr = [2, 5, 1, 7]\nst = SparseTable(small_arr, 'min')\nprint(\"\\ntable[i][j] = min of range [i, i+2^j-1]:\")\nprint(\"\\nj=0 (length 1):\")\nfor i in range(4):\n    print(f\"  table[{i}][0] = {st.table[i][0]} (range [{i}, {i}])\")\nprint(\"\\nj=1 (length 2):\")\nfor i in range(3):\n    print(f\"  table[{i}][1] = {st.table[i][1]} (range [{i}, {i+1}])\")\nprint(\"\\nj=2 (length 4):\")\nfor i in range(1):\n    print(f\"  table[{i}][2] = {st.table[i][2]} (range [{i}, {i+3}])\")\n",
      "input": ""
    }
  },
    {
    "slug": "bitmask-dp",
    "title": "Bitmask DP: Using Bits as States",
    "summary": "Master dynamic programming with bitmask states for subset enumeration and assignment problems.",
    "level": "Advanced",
    "category": "Advanced Dynamic Programming",
    "content": [
      "Problem: Need to track which items are used/visited in DP state, but using arrays as keys is inefficient. Solution: Use integers as bitmasks!",
      "Bitmask basics: Store set of n items using n bits. Bit i is 1 if item i is included, 0 otherwise. Example: 1011₂ means items {0,1,3} are selected.",
      "Why powerful: Integer keys are fast, can use simple arrays for DP table, easy to iterate through all subsets (just loop 0 to 2ⁿ-1).",
      "Key operations: Set bit (mask | (1 << i)), unset bit (mask & ~(1 << i)), check bit ((mask >> i) & 1), count bits (popcount).",
      "Classic problem - Traveling Salesman: dp[mask][city] = minimum cost to visit cities in mask, ending at city. Try all unvisited cities next.",
      "Assignment problems: Assign n tasks to n people. dp[mask] = min cost to assign tasks in mask to first popcount(mask) people.",
      "Subset sum variations: dp[mask][sum] = can we form sum using subset mask? Or count ways, find min operations, etc.",
      "Time complexity: Usually O(2ⁿ × n) or O(2ⁿ × n²). Only works for small n (typically n ≤ 20). Space: O(2ⁿ) or O(2ⁿ × n)."
    ],
    "example": {
      "language": "python",
      "code": "# Bitmask DP Examples\n\n# Example 1: Traveling Salesman Problem (TSP)\ndef traveling_salesman(dist):\n    \"\"\"Find minimum cost to visit all cities and return to start.\n    dist[i][j] = distance from city i to city j.\n    \"\"\"\n    n = len(dist)\n    # dp[mask][i] = min cost to visit cities in mask, ending at city i\n    dp = [[float('inf')] * n for _ in range(1 << n)]\n    \n    # Base case: start at city 0\n    dp[1][0] = 0  # mask = 0001 (only city 0 visited)\n    \n    # Try all subsets in increasing order\n    for mask in range(1 << n):\n        for last in range(n):\n            if not (mask & (1 << last)):  # last city not in mask\n                continue\n            if dp[mask][last] == float('inf'):\n                continue\n            \n            # Try visiting each unvisited city next\n            for next_city in range(n):\n                if mask & (1 << next_city):  # already visited\n                    continue\n                \n                new_mask = mask | (1 << next_city)\n                new_cost = dp[mask][last] + dist[last][next_city]\n                dp[new_mask][next_city] = min(dp[new_mask][next_city], new_cost)\n    \n    # Return to starting city 0 from any ending city\n    full_mask = (1 << n) - 1  # all bits set\n    result = float('inf')\n    for last in range(n):\n        result = min(result, dp[full_mask][last] + dist[last][0])\n    \n    return result\n\nprint(\"=== Traveling Salesman Problem ===\")\ndist_matrix = [\n    [0, 10, 15, 20],\n    [10, 0, 35, 25],\n    [15, 35, 0, 30],\n    [20, 25, 30, 0]\n]\nprint(f\"Distance matrix:\")\nfor row in dist_matrix:\n    print(f\"  {row}\")\nprint(f\"Minimum tour cost: {traveling_salesman(dist_matrix)}\")\nprint(\"Optimal tour: 0 → 1 → 3 → 2 → 0 (cost 80)\")\n\n# Example 2: Assignment Problem\ndef min_cost_assignment(cost):\n    \"\"\"Assign n tasks to n people with minimum cost.\n    cost[person][task] = cost for person to do task.\n    \"\"\"\n    n = len(cost)\n    # dp[mask] = min cost to assign tasks in mask to first popcount(mask) people\n    dp = [float('inf')] * (1 << n)\n    dp[0] = 0  # no tasks assigned\n    \n    for mask in range(1 << n):\n        if dp[mask] == float('inf'):\n            continue\n        \n        person = bin(mask).count('1')  # number of tasks already assigned\n        if person >= n:\n            continue\n        \n        # Try assigning each unassigned task to this person\n        for task in range(n):\n            if mask & (1 << task):  # task already assigned\n                continue\n            \n            new_mask = mask | (1 << task)\n            new_cost = dp[mask] + cost[person][task]\n            dp[new_mask] = min(dp[new_mask], new_cost)\n    \n    return dp[(1 << n) - 1]\n\nprint(\"\\n=== Assignment Problem ===\")\ncost_matrix = [\n    [9, 2, 7, 8],\n    [6, 4, 3, 7],\n    [5, 8, 1, 8],\n    [7, 6, 9, 4]\n]\nprint(f\"Cost matrix (person x task):\")\nfor i, row in enumerate(cost_matrix):\n    print(f\"  Person {i}: {row}\")\nprint(f\"Minimum assignment cost: {min_cost_assignment(cost_matrix)}\")\nprint(\"Optimal: P0→T1(2), P1→T2(3), P2→T2(1), P3→T3(4) = 13\")\n\n# Example 3: Subset Sum with Bitmask\ndef count_subset_sums(nums, target):\n    \"\"\"Count subsets of nums that sum to target using bitmask.\"\"\"\n    n = len(nums)\n    count = 0\n    \n    # Try all possible subsets (2^n possibilities)\n    for mask in range(1 << n):\n        subset_sum = 0\n        subset = []\n        \n        for i in range(n):\n            if mask & (1 << i):  # check if bit i is set\n                subset_sum += nums[i]\n                subset.append(nums[i])\n        \n        if subset_sum == target:\n            count += 1\n            print(f\"  Mask {mask:04b}: {subset} = {subset_sum}\")\n    \n    return count\n\nprint(\"\\n=== Subset Sum with Bitmask ===\")\nnums = [1, 2, 3, 4]\ntarget = 5\nprint(f\"Array: {nums}, Target: {target}\")\nprint(f\"Subsets that sum to {target}:\")\ntotal = count_subset_sums(nums, target)\nprint(f\"Total count: {total}\")\n\n# Example 4: Bitmask Operations Cheat Sheet\nprint(\"\\n=== Bitmask Operations Cheat Sheet ===\")\nmask = 0b1010  # binary 1010 = decimal 10 = {1, 3}\nprint(f\"Initial mask: {mask:04b} = {mask} = set {{1, 3}}\")\n\nprint(f\"\\n1. Set bit i (add element i):\")\nmask_set = mask | (1 << 2)  # set bit 2\nprint(f\"   mask | (1 << 2) = {mask_set:04b} = {{1, 2, 3}}\")\n\nprint(f\"\\n2. Unset bit i (remove element i):\")\nmask_unset = mask & ~(1 << 3)  # unset bit 3\nprint(f\"   mask & ~(1 << 3) = {mask_unset:04b} = {{1}}\")\n\nprint(f\"\\n3. Check bit i (is element i in set?):\")\nis_set = (mask >> 1) & 1  # check bit 1\nprint(f\"   (mask >> 1) & 1 = {is_set} (bit 1 is set)\")\n\nprint(f\"\\n4. Toggle bit i:\")\nmask_toggle = mask ^ (1 << 1)  # toggle bit 1\nprint(f\"   mask ^ (1 << 1) = {mask_toggle:04b} (toggled bit 1)\")\n\nprint(f\"\\n5. Count set bits (popcount):\")\ncount = bin(mask).count('1')\nprint(f\"   bin(mask).count('1') = {count} (2 elements in set)\")\n\nprint(f\"\\n6. Iterate through all subsets:\")\nprint(f\"   for mask in range(1 << n):  # 0 to 2^n - 1\")\nfor m in range(1 << 4):\n    elements = [i for i in range(4) if m & (1 << i)]\n    print(f\"     {m:04b} = {{{', '.join(map(str, elements)) if elements else '∅'}}}\")\n\n# Example 5: N-Queens using Bitmask (bonus)\ndef solve_n_queens_bitmask(n):\n    \"\"\"Count solutions to N-Queens using bitmask.\"\"\"\n    def backtrack(row, cols, diag1, diag2):\n        if row == n:\n            return 1\n        \n        count = 0\n        # Try placing queen in each column\n        available = ((1 << n) - 1) & ~(cols | diag1 | diag2)\n        \n        while available:\n            # Get rightmost available position\n            pos = available & -available\n            available ^= pos  # remove this position\n            \n            count += backtrack(\n                row + 1,\n                cols | pos,\n                (diag1 | pos) << 1,\n                (diag2 | pos) >> 1\n            )\n        \n        return count\n    \n    return backtrack(0, 0, 0, 0)\n\nprint(\"\\n=== N-Queens with Bitmask ===\")\nfor n in range(1, 9):\n    solutions = solve_n_queens_bitmask(n)\n    print(f\"  {n}x{n} board: {solutions} solutions\")\n\nprint(\"\\n=== Time Complexity Summary ===\")\nprint(\"TSP: O(2^n × n²) - try all subsets × cities × transitions\")\nprint(\"Assignment: O(2^n × n) - all subsets × tasks\")\nprint(\"Subset enumeration: O(2^n × n) - iterate all subsets\")\nprint(\"N-Queens bitmask: O(n!) but faster constant than backtracking\")\n",
      "input": ""
    }
  },
    {
    "slug": "dp-on-trees",
    "title": "DP on Trees: Subtree Problems",
    "summary": "Master dynamic programming on tree structures for subtree optimization and rerooting techniques.",
    "level": "Advanced",
    "category": "Advanced Dynamic Programming",
    "content": [
      "Tree DP solves optimization problems on trees by combining results from subtrees. Each node's answer depends on its children's answers, computed bottom-up using DFS.",
      "Algorithm 1 - Maximum Independent Set: Select nodes to maximize sum of values, but NO two adjacent nodes can be selected. dp[node][0] = not selected, dp[node][1] = selected.",
      "Max Independent Set logic: If node is selected, children CANNOT be selected. If node NOT selected, children can be selected OR not (take max). Formula: dp[node][1] = value[node] + sum(dp[child][0]), dp[node][0] = sum(max(dp[child][0], dp[child][1])).",
      "Algorithm 2 - Tree Diameter: Find longest path between any two nodes in tree. For each node, track two deepest paths going into different subtrees. Diameter = max(depth1 + depth2) across all nodes.",
      "Tree Diameter logic: At each node, maintain max depth to any leaf in its subtree. Diameter through node = (deepest path in subtree 1) + (deepest path in subtree 2). Return max depth to parent for their calculation.",
      "Algorithm 3 - Subtree Sums: Compute sum of all node values in subtree rooted at each node. Simple bottom-up: subtree_sum[node] = value[node] + sum(subtree_sum[child]) for all children.",
      "Rerooting technique (bonus): Compute answer when EACH node is considered root. First DFS: compute down-direction info. Second DFS: add up-direction info from parent. Used in 'Sum of Distances' problems.",
      "Implementation: Use adjacency list, pass parent to avoid going backward, post-order DFS (children before parent). Time: O(n) per problem. Space: O(n) for recursion + DP table."
    ],

    "example": {
      "language": "python",
      "code": "# DP on Trees Examples\n\nfrom collections import defaultdict\n\n# Example 1: Maximum Independent Set (No Adjacent Nodes)\nclass TreeDP:\n    def __init__(self, n):\n        self.n = n\n        self.graph = defaultdict(list)\n        self.dp = {}  # (node, included) -> max value\n    \n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n    \n    def max_independent_set(self, node, parent, included, values):\n        \"\"\"Maximum sum of node values with no two adjacent nodes selected.\n        included: True if current node is included in set.\n        \"\"\"\n        if (node, parent, included) in self.dp:\n            return self.dp[(node, parent, included)]\n        \n        if included:\n            # If this node is included, children cannot be included\n            result = values[node]\n            for child in self.graph[node]:\n                if child != parent:\n                    result += self.max_independent_set(child, node, False, values)\n        else:\n            # If this node is not included, children can be included or not\n            result = 0\n            for child in self.graph[node]:\n                if child != parent:\n                    # Take max of including or not including child\n                    include_child = self.max_independent_set(child, node, True, values)\n                    exclude_child = self.max_independent_set(child, node, False, values)\n                    result += max(include_child, exclude_child)\n        \n        self.dp[(node, parent, included)] = result\n        return result\n\nprint(\"=== Maximum Independent Set on Tree ===\")\nprint(\"Tree structure:\")\nprint(\"      0(3)\")\nprint(\"     / \\\\\")\nprint(\"   1(2) 2(5)\")\nprint(\"   /      \\\\\")\nprint(\" 3(1)    4(4)\")\n\ntree = TreeDP(5)\ntree.add_edge(0, 1)\ntree.add_edge(0, 2)\ntree.add_edge(1, 3)\ntree.add_edge(2, 4)\nvalues = [3, 2, 5, 1, 4]\n\ninclude_root = tree.max_independent_set(0, -1, True, values)\nexclude_root = tree.max_independent_set(0, -1, False, values)\nresult = max(include_root, exclude_root)\nprint(f\"Node values: {values}\")\nprint(f\"Max independent set sum: {result}\")\nprint(f\"Optimal: nodes {{0, 4}} or {{2, 3}} = 3+4 or 5+1+2 = 8\")\n\n# Example 2: Tree Diameter (Longest Path)\nclass TreeDiameter:\n    def __init__(self, n):\n        self.n = n\n        self.graph = defaultdict(list)\n        self.diameter = 0\n    \n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n    \n    def dfs(self, node, parent):\n        \"\"\"Returns max depth from this node. Updates diameter.\"\"\"\n        max_depth1 = 0  # Deepest path\n        max_depth2 = 0  # Second deepest path\n        \n        for child in self.graph[node]:\n            if child != parent:\n                child_depth = self.dfs(child, node) + 1\n                \n                # Keep track of two deepest paths\n                if child_depth > max_depth1:\n                    max_depth2 = max_depth1\n                    max_depth1 = child_depth\n                elif child_depth > max_depth2:\n                    max_depth2 = child_depth\n        \n        # Diameter through this node is sum of two deepest paths\n        self.diameter = max(self.diameter, max_depth1 + max_depth2)\n        \n        return max_depth1\n    \n    def find_diameter(self):\n        self.diameter = 0\n        self.dfs(0, -1)\n        return self.diameter\n\nprint(\"\\n=== Tree Diameter (Longest Path) ===\")\nprint(\"Tree structure:\")\nprint(\"      0\")\nprint(\"     /|\\\\\")\nprint(\"    1 2 3\")\nprint(\"   /  |\")\nprint(\"  4   5\")\nprint(\"  |\")\nprint(\"  6\")\n\ntree2 = TreeDiameter(7)\nedges = [(0,1), (0,2), (0,3), (1,4), (4,6), (2,5)]\nfor u, v in edges:\n    tree2.add_edge(u, v)\n\ndiameter = tree2.find_diameter()\nprint(f\"Tree diameter: {diameter}\")\nprint(f\"Longest path: 6 -> 4 -> 1 -> 0 -> 2 -> 5 (length 5)\")\n\n# Example 3: Subtree Sums\nclass SubtreeSums:\n    def __init__(self, n):\n        self.n = n\n        self.graph = defaultdict(list)\n        self.subtree_sum = [0] * n\n        self.subtree_size = [0] * n\n    \n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n    \n    def dfs(self, node, parent, values):\n        \"\"\"Compute sum and size of subtree rooted at node.\"\"\"\n        self.subtree_sum[node] = values[node]\n        self.subtree_size[node] = 1\n        \n        for child in self.graph[node]:\n            if child != parent:\n                self.dfs(child, node, values)\n                self.subtree_sum[node] += self.subtree_sum[child]\n                self.subtree_size[node] += self.subtree_size[child]\n    \n    def compute(self, values):\n        self.dfs(0, -1, values)\n        return self.subtree_sum, self.subtree_size\n\nprint(\"\\n=== Subtree Sums ===\")\nprint(\"Tree with values:\")\nprint(\"      0(5)\")\nprint(\"     /    \\\\\")\nprint(\"   1(3)   2(7)\")\nprint(\"   / \\\\      \\\\\")\nprint(\" 3(2) 4(1) 5(4)\")\n\ntree3 = SubtreeSums(6)\nedges3 = [(0,1), (0,2), (1,3), (1,4), (2,5)]\nfor u, v in edges3:\n    tree3.add_edge(u, v)\nvalues3 = [5, 3, 7, 2, 1, 4]\n\nsums, sizes = tree3.compute(values3)\nprint(f\"\\nNode values: {values3}\")\nprint(f\"\\nSubtree sums:\")\nfor i in range(6):\n    print(f\"  Node {i}: sum={sums[i]}, size={sizes[i]}\")\n\n# Example 4: Rerooting Technique\nclass RerootingDP:\n    def __init__(self, n):\n        self.n = n\n        self.graph = defaultdict(list)\n        self.down = [0] * n  # Answer considering only subtree\n        self.up = [0] * n    # Answer considering whole tree when node is root\n    \n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n    \n    def dfs_down(self, node, parent):\n        \"\"\"Compute down[node] = max depth in subtree.\"\"\"\n        self.down[node] = 0\n        for child in self.graph[node]:\n            if child != parent:\n                self.dfs_down(child, node)\n                self.down[node] = max(self.down[node], self.down[child] + 1)\n    \n    def dfs_up(self, node, parent):\n        \"\"\"Compute up[node] = answer when node is considered root.\"\"\"\n        # up[node] is already set by parent\n        \n        # Collect all child depths\n        depths = []\n        for child in self.graph[node]:\n            if child != parent:\n                depths.append(self.down[child] + 1)\n        \n        depths.sort(reverse=True)\n        \n        # For each child, compute what it would see looking up\n        for child in self.graph[node]:\n            if child != parent:\n                child_depth = self.down[child] + 1\n                \n                # Max depth excluding this child's subtree\n                if depths and depths[0] == child_depth:\n                    parent_contribution = depths[1] + 1 if len(depths) > 1 else 1\n                else:\n                    parent_contribution = depths[0] + 1 if depths else 1\n                \n                self.up[child] = max(self.up[node] + 1, parent_contribution)\n                self.dfs_up(child, node)\n    \n    def compute_all_heights(self):\n        \"\"\"For each node, find max distance to any other node.\"\"\"\n        self.dfs_down(0, -1)\n        self.up[0] = self.down[0]\n        self.dfs_up(0, -1)\n        \n        result = []\n        for i in range(self.n):\n            result.append(max(self.down[i], self.up[i]))\n        return result\n\nprint(\"\\n=== Rerooting: Max Distance from Each Node ===\")\nprint(\"Tree structure:\")\nprint(\"    0 -- 1 -- 2\")\nprint(\"         |\")\nprint(\"         3\")\n\ntree4 = RerootingDP(4)\nedges4 = [(0,1), (1,2), (1,3)]\nfor u, v in edges4:\n    tree4.add_edge(u, v)\n\nheights = tree4.compute_all_heights()\nprint(f\"\\nMax distance from each node:\")\nfor i, h in enumerate(heights):\n    print(f\"  Node {i}: {h}\")\nprint(f\"\\nExplanation:\")\nprint(f\"  Node 0: farthest is node 2 or 3 (distance 2)\")\nprint(f\"  Node 1: farthest is node 0, 2, or 3 (distance 1)\")\nprint(f\"  Node 2: farthest is node 0 or 3 (distance 2)\")\nprint(f\"  Node 3: farthest is node 0 or 2 (distance 2)\")\n\nprint(\"\\n=== Complexity Analysis ===\")\nprint(\"All tree DP examples: O(n) time, O(n) space\")\nprint(\"Key: Each node visited once in DFS traversal\")\nprint(\"Rerooting: Two DFS passes, still O(n) total\")\n",
      "input": ""
    }
  },
    {
    "slug": "state-machine-dp",
    "title": "State Machine DP: Transition States",
    "summary": "Master DP with state transitions for buy/sell stock problems and sequential decision-making.",
    "level": "Advanced",
    "category": "Advanced Dynamic Programming",
    "content": [
      "State Machine DP models problems where you transition between different states, each with different rules and costs. Like a finite state machine with optimal transitions.",
      "Classic example - Stock Trading: States are 'holding stock', 'not holding stock', 'cooldown'. Can only transition between valid states with certain actions (buy, sell, rest).",
      "Why useful: When problem has multiple phases/modes with restrictions on transitions. Each state remembers context, and optimal solution flows through state transitions.",
      "State definition: dp[day][state] = max profit at day i in state s. States might be 'hold', 'sold', 'cooldown', 'can_buy', etc. based on problem constraints.",
      "Transition rules: From state A, what states can you move to? What's the cost? Example: hold → sold (sell, get +price), hold → hold (rest, no change).",
      "Common patterns: Stock problems (k transactions, cooldown, fee), game states (can attack/defend/heal), sequential decisions with dependencies.",
      "Implementation tip: Draw state diagram first! Show all states as nodes, transitions as edges with conditions. Then code becomes straightforward translation.",
      "Time complexity: O(n × states × transitions), usually O(n × k) where k is number of states (small constant). Space: O(n × states) or O(states) with optimization."
    ],
    "example": {
      "language": "python",
      "code": "# State Machine DP Examples\n\n# Example 1: Best Time to Buy and Sell Stock with Cooldown\ndef max_profit_cooldown(prices):\n    \"\"\"After selling, must cooldown for 1 day before buying again.\n    States: hold (have stock), sold (just sold), rest (cooldown/can buy)\n    \"\"\"\n    if not prices:\n        return 0\n    \n    n = len(prices)\n    # dp[i][state] = max profit at day i in state\n    # States: 0=hold, 1=sold, 2=rest\n    dp = [[0] * 3 for _ in range(n)]\n    \n    # Day 0 initialization\n    dp[0][0] = -prices[0]  # bought stock\n    dp[0][1] = 0           # can't sell on day 0\n    dp[0][2] = 0           # rest\n    \n    for i in range(1, n):\n        # hold: either held from yesterday, or buy today (must be in rest state)\n        dp[i][0] = max(dp[i-1][0], dp[i-1][2] - prices[i])\n        \n        # sold: must have held yesterday, sell today\n        dp[i][1] = dp[i-1][0] + prices[i]\n        \n        # rest: either rested yesterday, or cooled down from selling\n        dp[i][2] = max(dp[i-1][2], dp[i-1][1])\n    \n    # Final answer: either sold or resting (not holding)\n    return max(dp[n-1][1], dp[n-1][2])\n\nprint(\"=== Stock with Cooldown ===\")\nprices1 = [1, 2, 3, 0, 2]\nprint(f\"Prices: {prices1}\")\nprint(f\"Max profit: {max_profit_cooldown(prices1)}\")\nprint(\"Strategy: Buy day 0 ($1), sell day 2 ($3), cooldown day 3, buy day 3 ($0), sell day 4 ($2)\")\nprint(\"Profit: -1 + 3 - 0 + 2 = $4\\n\")\n\n# Example 2: Best Time to Buy and Sell Stock with Transaction Fee\ndef max_profit_fee(prices, fee):\n    \"\"\"Pay transaction fee on each sale.\n    States: hold (have stock), cash (no stock, cash available)\n    \"\"\"\n    n = len(prices)\n    # hold[i] = max profit if holding stock at day i\n    # cash[i] = max profit if not holding stock at day i\n    hold = [-prices[0]] + [0] * (n - 1)\n    cash = [0] * n\n    \n    for i in range(1, n):\n        # hold: either held from yesterday, or buy today\n        hold[i] = max(hold[i-1], cash[i-1] - prices[i])\n        \n        # cash: either had cash yesterday, or sell today (pay fee)\n        cash[i] = max(cash[i-1], hold[i-1] + prices[i] - fee)\n    \n    return cash[n-1]\n\nprint(\"=== Stock with Transaction Fee ===\")\nprices2 = [1, 3, 2, 8, 4, 9]\nfee = 2\nprint(f\"Prices: {prices2}\")\nprint(f\"Transaction fee: ${fee}\")\nprint(f\"Max profit: ${max_profit_fee(prices2, fee)}\")\nprint(\"Strategy: Buy day 0 ($1), sell day 3 ($8-$2 fee), buy day 4 ($4), sell day 5 ($9-$2 fee)\")\nprint(\"Profit: -1 + 8 - 2 - 4 + 9 - 2 = $8\\n\")\n\n# Example 3: Best Time to Buy and Sell Stock IV (k transactions)\ndef max_profit_k_transactions(k, prices):\n    \"\"\"At most k buy-sell transactions.\n    States: buy[j] = max profit after j-th buy, sell[j] = max profit after j-th sell\n    \"\"\"\n    if not prices or k == 0:\n        return 0\n    \n    n = len(prices)\n    \n    # If k >= n/2, unlimited transactions (no constraint)\n    if k >= n // 2:\n        return sum(max(prices[i+1] - prices[i], 0) for i in range(n-1))\n    \n    # buy[j] = max profit after buying for j-th transaction\n    # sell[j] = max profit after selling for j-th transaction\n    buy = [float('-inf')] * (k + 1)\n    sell = [0] * (k + 1)\n    \n    for price in prices:\n        # Process in reverse to avoid using updated values\n        for j in range(k, 0, -1):\n            sell[j] = max(sell[j], buy[j] + price)\n            buy[j] = max(buy[j], sell[j-1] - price)\n    \n    return sell[k]\n\nprint(\"=== Stock with K Transactions ===\")\nprices3 = [3, 2, 6, 5, 0, 3]\nk = 2\nprint(f\"Prices: {prices3}\")\nprint(f\"Max transactions: {k}\")\nprint(f\"Max profit: ${max_profit_k_transactions(k, prices3)}\")\nprint(\"Strategy: Buy day 1 ($2), sell day 2 ($6), buy day 4 ($0), sell day 5 ($3)\")\nprint(\"Profit: -2 + 6 - 0 + 3 = $7\\n\")\n\n# Example 4: House Robber II (Circular Array)\ndef rob_circular(nums):\n    \"\"\"Houses in circle, can't rob first and last together.\n    States: rob_current, skip_current\n    \"\"\"\n    if len(nums) == 1:\n        return nums[0]\n    \n    def rob_linear(houses):\n        if not houses:\n            return 0\n        if len(houses) == 1:\n            return houses[0]\n        \n        # dp[i][0] = max money if NOT robbing house i\n        # dp[i][1] = max money if robbing house i\n        skip = 0\n        rob = houses[0]\n        \n        for i in range(1, len(houses)):\n            new_rob = skip + houses[i]  # rob current, must skip previous\n            new_skip = max(skip, rob)   # skip current, take max of previous\n            skip, rob = new_skip, new_rob\n        \n        return max(skip, rob)\n    \n    # Case 1: Rob houses 0 to n-2 (exclude last)\n    # Case 2: Rob houses 1 to n-1 (exclude first)\n    return max(rob_linear(nums[:-1]), rob_linear(nums[1:]))\n\nprint(\"=== House Robber II (Circular) ===\")\nhouses = [2, 3, 2]\nprint(f\"Houses (circular): {houses}\")\nprint(f\"Max money: ${rob_circular(houses)}\")\nprint(\"Strategy: Rob house 1 ($3). Can't rob house 0 and 2 (adjacent to each other in circle)\\n\")\n\n# Example 5: Paint House with K Colors\ndef min_cost_paint_k_colors(costs):\n    \"\"\"n houses, k colors. Adjacent houses can't have same color.\n    States: house painted with color j\n    \"\"\"\n    if not costs:\n        return 0\n    \n    n = len(costs)\n    k = len(costs[0])\n    \n    # dp[i][j] = min cost to paint houses 0..i with house i color j\n    dp = [[float('inf')] * k for _ in range(n)]\n    \n    # Base case: first house\n    for j in range(k):\n        dp[0][j] = costs[0][j]\n    \n    for i in range(1, n):\n        for j in range(k):  # current house color\n            for prev_j in range(k):  # previous house color\n                if j != prev_j:  # different colors\n                    dp[i][j] = min(dp[i][j], dp[i-1][prev_j] + costs[i][j])\n    \n    return min(dp[n-1])\n\nprint(\"=== Paint House (K Colors) ===\")\ncosts_matrix = [\n    [17, 2, 17],  # house 0: red=$17, blue=$2, green=$17\n    [16, 16, 5],  # house 1\n    [14, 3, 19]   # house 2\n]\nprint(f\"Costs matrix (houses x colors):\")\nfor i, row in enumerate(costs_matrix):\n    print(f\"  House {i}: {row}\")\nprint(f\"Min cost: ${min_cost_paint_k_colors(costs_matrix)}\")\nprint(\"Strategy: House 0=blue($2), House 1=green($5), House 2=blue($3) = $10\\n\")\n\n# Example 6: State Machine Visualization Helper\ndef visualize_state_transitions():\n    \"\"\"Show state diagram for stock with cooldown.\"\"\"\n    print(\"=== State Machine Diagram: Stock with Cooldown ===\")\n    print(\"\")\n    print(\"     +------+  sell   +------+\")\n    print(\"     | HOLD |-------->| SOLD |\")\n    print(\"     +------+         +------+\")\n    print(\"        ^  |             |\")\n    print(\"        |  | rest        | (auto cooldown)\")\n    print(\"   buy  |  |             |\")\n    print(\"        |  v             v\")\n    print(\"     +------+  rest   +------+\")\n    print(\"     | REST |<--------| REST |\")\n    print(\"     +------+         +------+\")\n    print(\"\")\n    print(\"Transitions:\")\n    print(\"  HOLD → HOLD: rest (no action)\")\n    print(\"  HOLD → SOLD: sell stock (+price)\")\n    print(\"  SOLD → REST: mandatory cooldown\")\n    print(\"  REST → REST: continue resting\")\n    print(\"  REST → HOLD: buy stock (-price)\")\n    print(\"\")\n\nvisualize_state_transitions()\n\nprint(\"=== Time Complexity Summary ===\")\nprint(\"Stock with cooldown: O(n × 3) = O(n) - 3 states\")\nprint(\"Stock with fee: O(n × 2) = O(n) - 2 states\")\nprint(\"Stock with k transactions: O(n × k) - k states\")\nprint(\"Paint house k colors: O(n × k²) - k states, k transitions each\")\nprint(\"\\nSpace: O(n × states) or optimize to O(states) with rolling array\")\n",
      "input": ""
    }
  },
    {
    "slug": "digit-dp",
    "title": "Digit DP: Counting Numbers by Digits",
    "summary": "Learn to count numbers with specific digit properties efficiently using digit-by-digit dynamic programming.",
    "level": "Advanced",
    "category": "Advanced Dynamic Programming",
    "content": [
      "Digit DP counts integers in range [L, R] satisfying certain digit constraints. Brute force checking each number is too slow; digit DP builds valid numbers digit by digit.",
      "Key insight: Process numbers digit by digit from left to right. At each position, try all valid digits (0-9) while maintaining constraints like 'sum of digits ≤ k'.",
      "State definition: dp[pos][tight][state] where pos=current position, tight=whether we're still bounded by upper limit, state=problem-specific info (digit sum, has certain digit, etc.).",
      "Tight bound tracking: When building number, if we use digit smaller than upper bound's digit at position i, all future positions can use any digit 0-9. This is the 'tight' flag.",
      "Standard pattern: count(R) - count(L-1). Count numbers from 0 to R, then subtract numbers from 0 to L-1. Result is numbers in [L, R].",
      "Common problems: Count numbers with sum of digits = k, numbers without consecutive 1s in binary, numbers with at most n distinct digits, non-decreasing digits.",
      "Leading zeros handling: Often need to track whether we've started placing non-zero digits. Leading zeros don't count toward digit sum or other constraints.",
      "Time complexity: O(length × 2 × states × 10) where length is number of digits. Typically O(18 × 2 × states × 10) for 64-bit integers. Much faster than O(R-L)!"
    ],
    "example": {
      "language": "python",
      "code": "# Digit DP Examples\n\n# Example 1: Count Numbers with Digit Sum Equal to Target\nclass DigitDP:\n    def count_numbers_with_sum(self, n: int, target_sum: int) -> int:\n        \"\"\"Count integers from 0 to n with digit sum = target_sum.\"\"\"\n        if n < 0:\n            return 0\n        \n        digits = [int(d) for d in str(n)]\n        memo = {}\n        \n        def dp(pos, tight, digit_sum, started):\n            \"\"\"pos: current position, tight: bounded by n, \n            digit_sum: sum so far, started: placed non-zero digit\"\"\"\n            \n            # Base case: processed all digits\n            if pos == len(digits):\n                return 1 if (digit_sum == target_sum and started) else 0\n            \n            # Check memo\n            state = (pos, tight, digit_sum, started)\n            if state in memo:\n                return memo[state]\n            \n            # Try all possible digits at this position\n            limit = digits[pos] if tight else 9\n            result = 0\n            \n            for digit in range(0, limit + 1):\n                new_started = started or (digit > 0)\n                new_sum = digit_sum + digit if new_started else 0\n                new_tight = tight and (digit == limit)\n                \n                # Skip if digit_sum already exceeds target\n                if new_sum <= target_sum:\n                    result += dp(pos + 1, new_tight, new_sum, new_started)\n            \n            memo[state] = result\n            return result\n        \n        return dp(0, True, 0, False)\n\ndigit_dp = DigitDP()\nprint(\"=== Count Numbers with Digit Sum ===\")\nfor n, target in [(20, 5), (100, 10), (50, 8)]:\n    count = digit_dp.count_numbers_with_sum(n, target)\n    print(f\"Numbers from 0 to {n} with digit sum {target}: {count}\")\n\n# Example 2: Count Numbers Without Consecutive 1s in Binary\ndef count_no_consecutive_ones(n: int) -> int:\n    \"\"\"Count numbers from 0 to n with no consecutive 1s in binary.\"\"\"\n    if n <= 0:\n        return 1\n    \n    binary = bin(n)[2:]  # Remove '0b' prefix\n    memo = {}\n    \n    def dp(pos, tight, prev_bit):\n        if pos == len(binary):\n            return 1\n        \n        state = (pos, tight, prev_bit)\n        if state in memo:\n            return memo[state]\n        \n        limit = int(binary[pos]) if tight else 1\n        result = 0\n        \n        for bit in range(0, limit + 1):\n            # Skip if previous was 1 and current is 1 (consecutive)\n            if prev_bit == 1 and bit == 1:\n                continue\n            \n            new_tight = tight and (bit == limit)\n            result += dp(pos + 1, new_tight, bit)\n        \n        memo[state] = result\n        return result\n    \n    return dp(0, True, 0)\n\nprint(\"\\n=== Count Numbers Without Consecutive 1s (Binary) ===\")\nfor n in [5, 10, 15, 20]:\n    count = count_no_consecutive_ones(n)\n    print(f\"0 to {n} ({bin(n)}): {count} numbers without consecutive 1s\")\nprint(\"Example: 5 = 101₂, 6 = 110₂ (consecutive 1s), 7 = 111₂ (consecutive)\")\n\n# Example 3: Count Numbers with Non-Decreasing Digits\ndef count_non_decreasing_digits(n: int) -> int:\n    \"\"\"Count numbers from 0 to n where digits are non-decreasing left to right.\"\"\"\n    if n < 0:\n        return 0\n    \n    digits = [int(d) for d in str(n)]\n    memo = {}\n    \n    def dp(pos, tight, prev_digit, started):\n        if pos == len(digits):\n            return 1 if started else 0\n        \n        state = (pos, tight, prev_digit, started)\n        if state in memo:\n            return memo[state]\n        \n        limit = digits[pos] if tight else 9\n        result = 0\n        \n        for digit in range(0, limit + 1):\n            # Skip leading zeros or check non-decreasing\n            if not started:\n                if digit == 0:\n                    result += dp(pos + 1, False, 0, False)\n                else:\n                    result += dp(pos + 1, tight and (digit == limit), digit, True)\n            else:\n                if digit >= prev_digit:\n                    result += dp(pos + 1, tight and (digit == limit), digit, True)\n        \n        memo[state] = result\n        return result\n    \n    return dp(0, True, 0, False)\n\nprint(\"\\n=== Count Numbers with Non-Decreasing Digits ===\")\nfor n in [10, 50, 100, 200]:\n    count = count_non_decreasing_digits(n)\n    print(f\"0 to {n}: {count} numbers with non-decreasing digits\")\nprint(\"Examples: 123, 1239, 1, 5, 1111 (valid) | 132, 1230 (invalid - digits decrease)\")\n\n# Example 4: Count in Range [L, R]\ndef count_in_range(L: int, R: int, condition_func) -> int:\n    \"\"\"Generic: count numbers in [L, R] satisfying condition.\"\"\"\n    # count(R) - count(L-1)\n    count_R = condition_func(R)\n    count_L = condition_func(L - 1) if L > 0 else 0\n    return count_R - count_L\n\nprint(\"\\n=== Count in Range [L, R] ===\")\nL, R = 10, 50\ndigit_sum_5 = count_in_range(L, R, lambda x: DigitDP().count_numbers_with_sum(x, 5))\nprint(f\"Numbers in [{L}, {R}] with digit sum 5: {digit_sum_5}\")\nno_consec_ones = count_in_range(L, R, count_no_consecutive_ones)\nprint(f\"Numbers in [{L}, {R}] without consecutive 1s (binary): {no_consec_ones}\")\n\n# Example 5: Digit DP Template\ndef digit_dp_template(n: int):\n    \"\"\"Template for digit DP problems.\"\"\"\n    if n < 0:\n        return 0\n    \n    digits = [int(d) for d in str(n)]\n    memo = {}\n    \n    def dp(pos, tight, state, started):\n        \"\"\"\n        pos: current digit position (0-indexed from left)\n        tight: True if we're still bounded by n's digits\n        state: problem-specific state (digit_sum, last_digit, etc.)\n        started: True if we've placed a non-zero digit\n        \"\"\"\n        \n        # Base case: processed all positions\n        if pos == len(digits):\n            # Return 1 if valid solution, 0 otherwise\n            return 1 if (started and is_valid(state)) else 0\n        \n        # Memoization\n        key = (pos, tight, state, started)\n        if key in memo:\n            return memo[key]\n        \n        # Determine digit limit at this position\n        limit = digits[pos] if tight else 9\n        result = 0\n        \n        # Try all possible digits\n        for digit in range(0, limit + 1):\n            # Update state based on digit choice\n            new_state = update_state(state, digit, started)\n            new_started = started or (digit > 0)\n            new_tight = tight and (digit == limit)\n            \n            # Prune invalid branches early if possible\n            if is_valid_partial(new_state):\n                result += dp(pos + 1, new_tight, new_state, new_started)\n        \n        memo[key] = result\n        return result\n    \n    # Helper functions (customize for specific problem)\n    def is_valid(state):\n        # Check if final state satisfies problem constraints\n        return True\n    \n    def update_state(state, digit, started):\n        # Update state with new digit\n        return state\n    \n    def is_valid_partial(state):\n        # Check if current state can lead to valid solution\n        return True\n    \n    return dp(0, True, initial_state, False)\n\nprint(\"\\n=== Digit DP Template Structure ===\")\nprint(\"1. Convert number to digit array\")\nprint(\"2. Define state: (pos, tight, problem_state, started)\")\nprint(\"3. Recursion: try all digits 0-9 (or 0-limit if tight)\")\nprint(\"4. Update state and recurse\")\nprint(\"5. Memoize based on state tuple\")\nprint(\"6. Base case: all positions processed → check validity\")\n\nprint(\"\\n=== Complexity Analysis ===\")\nprint(\"Digits: d = log₁₀(n) ≈ 18 for 64-bit\")\nprint(\"States: tight (2) × custom_states × started (2)\")\nprint(\"Transitions: 10 digits to try each\")\nprint(\"Total: O(d × 2 × states × 2 × 10) = O(d × states × 40)\")\nprint(\"Much faster than O(n) when n is large!\")\n",
      "input": ""
    }
  },
    {
    "slug": "dp-optimizations",
    "title": "DP Optimizations: Advanced Techniques",
    "summary": "Master advanced DP optimization techniques to reduce time complexity from O(n²) to O(n log n) or O(n).",
    "level": "Advanced",
    "category": "Advanced Dynamic Programming",
    "content": [
      "Standard DP often runs in O(n²) or O(n³). For large inputs, this is too slow. Advanced techniques can reduce complexity by exploiting special properties of the problem.",
      "Convex Hull Trick: Optimizes DP transitions of form dp[i] = min(dp[j] + a[j] × b[i]) from O(n²) to O(n log n) or O(n) when coefficients have monotonic properties.",
      "When to use Convex Hull: DP has linear transition (y = mx + b form), and either slopes or query points are monotonic. Maintains lower envelope of lines.",
      "Divide and Conquer DP: For dp[i][j] = min(dp[i-1][k] + cost(k, j)), if cost satisfies quadrangle inequality, can optimize from O(kn²) to O(kn log n).",
      "Monotonic Queue/Deque: Sliding window minimum/maximum in O(n) instead of O(n log n) with heap. Each element enters and exits deque at most once.",
      "Space optimization: 2D DP often reducible to 1D by processing row by row. Save space from O(n²) to O(n). Example: knapsack, edit distance.",
      "Matrix exponentiation: Linear recurrence like Fibonacci in O(k³ log n) instead of O(n) where k is matrix size, n is term number. Useful for huge n.",
      "When to optimize: If basic DP gets TLE (Time Limit Exceeded). Check if problem has monotonic properties, quadrangle inequality, or matrix form. Don't over-optimize easy problems!"
    ],
    "example": {
      "language": "python",
      "code": "# DP Optimization Examples\n\nfrom collections import deque\nimport heapq\n\n# Example 1: Convex Hull Trick - Minimum Cost Fencing\ndef min_cost_fence_convex_hull(heights, width_cost):\n    \"\"\"Fence posts at heights[i]. Cost to build fence from i to j:\n    dp[j] = min(dp[i] + heights[i] * (j - i) * width_cost) for all i < j\n    This is O(n²) naively. Convex hull trick makes it O(n log n) or O(n).\n    \"\"\"\n    n = len(heights)\n    if n == 0:\n        return 0\n    \n    # Each transition is a line: y = heights[i] * width_cost * x - heights[i] * width_cost * i + dp[i]\n    # Slope = heights[i] * width_cost, intercept = -heights[i] * width_cost * i + dp[i]\n    \n    class Line:\n        def __init__(self, m, c):\n            self.m = m  # slope\n            self.c = c  # intercept\n        \n        def eval(self, x):\n            return self.m * x + self.c\n    \n    def is_bad(l1, l2, l3):\n        \"\"\"Check if line l2 is redundant (never optimal).\"\"\"\n        # l2 is bad if intersection of l1-l3 is left of l1-l2\n        # (c3 - c1) / (m1 - m3) < (c2 - c1) / (m1 - m2)\n        return (l3.c - l1.c) * (l1.m - l2.m) < (l2.c - l1.c) * (l1.m - l3.m)\n    \n    hull = []  # Convex hull of lines\n    dp = [0] * n\n    \n    # Add initial line\n    hull.append(Line(heights[0] * width_cost, -heights[0] * width_cost * 0 + 0))\n    \n    for j in range(1, n):\n        # Query: find minimum at x = j\n        # Binary search on hull (if slopes are monotonic)\n        left, right = 0, len(hull) - 1\n        while left < right:\n            mid = (left + right) // 2\n            if hull[mid].eval(j) > hull[mid + 1].eval(j):\n                left = mid + 1\n            else:\n                right = mid\n        \n        dp[j] = hull[left].eval(j)\n        \n        # Add new line for dp[j]\n        new_line = Line(heights[j] * width_cost, -heights[j] * width_cost * j + dp[j])\n        \n        # Remove lines that become sub-optimal\n        while len(hull) >= 2 and is_bad(hull[-2], hull[-1], new_line):\n            hull.pop()\n        \n        hull.append(new_line)\n    \n    return dp[n - 1]\n\nprint(\"=== Convex Hull Trick Example ===\")\nheights = [1, 3, 2, 5, 4]\nwidth_cost = 2\nresult = min_cost_fence_convex_hull(heights, width_cost)\nprint(f\"Heights: {heights}\")\nprint(f\"Width cost: {width_cost}\")\nprint(f\"Minimum fence cost: {result}\")\nprint(\"Complexity: O(n log n) instead of O(n²)\\n\")\n\n# Example 2: Monotonic Queue - Sliding Window Maximum\ndef sliding_window_max_optimized(nums, k):\n    \"\"\"Find max in each window of size k. O(n) with monotonic deque.\"\"\"\n    dq = deque()  # Store indices in decreasing order of values\n    result = []\n    \n    for i in range(len(nums)):\n        # Remove elements outside window\n        while dq and dq[0] < i - k + 1:\n            dq.popleft()\n        \n        # Remove smaller elements (they'll never be max)\n        while dq and nums[i] > nums[dq[-1]]:\n            dq.pop()\n        \n        dq.append(i)\n        \n        # Add to result once window is full\n        if i >= k - 1:\n            result.append(nums[dq[0]])\n    \n    return result\n\nprint(\"=== Monotonic Queue Optimization ===\")\nnums = [1, 3, -1, -3, 5, 3, 6, 7]\nk = 3\nresult = sliding_window_max_optimized(nums, k)\nprint(f\"Array: {nums}\")\nprint(f\"Window size: {k}\")\nprint(f\"Sliding window maximums: {result}\")\nprint(\"Complexity: O(n) instead of O(n log n) with heap\\n\")\n\n# Example 3: Space Optimization - 2D to 1D DP\ndef knapsack_space_optimized(weights, values, capacity):\n    \"\"\"0/1 Knapsack with O(capacity) space instead of O(n × capacity).\"\"\"\n    n = len(weights)\n    dp = [0] * (capacity + 1)\n    \n    for i in range(n):\n        # Process from right to left to avoid using updated values\n        for w in range(capacity, weights[i] - 1, -1):\n            dp[w] = max(dp[w], dp[w - weights[i]] + values[i])\n    \n    return dp[capacity]\n\nprint(\"=== Space Optimization: 2D to 1D ===\")\nweights = [2, 3, 4, 5]\nvalues = [3, 4, 5, 6]\ncapacity = 8\nresult = knapsack_space_optimized(weights, values, capacity)\nprint(f\"Weights: {weights}\")\nprint(f\"Values: {values}\")\nprint(f\"Capacity: {capacity}\")\nprint(f\"Maximum value: {result}\")\nprint(\"Space: O(capacity) instead of O(n × capacity)\\n\")\n\n# Example 4: Matrix Exponentiation - Fibonacci\ndef matrix_multiply(A, B):\n    \"\"\"Multiply two 2x2 matrices.\"\"\"\n    return [\n        [A[0][0]*B[0][0] + A[0][1]*B[1][0], A[0][0]*B[0][1] + A[0][1]*B[1][1]],\n        [A[1][0]*B[0][0] + A[1][1]*B[1][0], A[1][0]*B[0][1] + A[1][1]*B[1][1]]\n    ]\n\ndef matrix_power(M, n):\n    \"\"\"Compute M^n using binary exponentiation. O(log n).\"\"\"\n    if n == 1:\n        return M\n    if n % 2 == 0:\n        half = matrix_power(M, n // 2)\n        return matrix_multiply(half, half)\n    else:\n        return matrix_multiply(M, matrix_power(M, n - 1))\n\ndef fibonacci_matrix(n):\n    \"\"\"Compute n-th Fibonacci number in O(log n).\"\"\"\n    if n <= 1:\n        return n\n    \n    # F(n) = [[1,1],[1,0]]^n * [[1],[0]]\n    M = [[1, 1], [1, 0]]\n    result = matrix_power(M, n)\n    return result[0][1]\n\nprint(\"=== Matrix Exponentiation: Fast Fibonacci ===\")\nfor n in [10, 20, 30, 50]:\n    fib = fibonacci_matrix(n)\n    print(f\"F({n}) = {fib}\")\nprint(\"Complexity: O(log n) instead of O(n)\\n\")\n\n# Example 5: Divide and Conquer DP\ndef divide_conquer_dp_example():\n    \"\"\"Conceptual example: optimal binary search tree.\n    dp[i][j] = min cost for keys i to j\n    opt[i][j] = optimal split point\n    Property: opt[i][j-1] <= opt[i][j] <= opt[i+1][j] (monotonicity)\n    Can reduce from O(n³) to O(n² log n).\n    \"\"\"\n    print(\"=== Divide and Conquer DP ===\")\n    print(\"When cost function satisfies quadrangle inequality:\")\n    print(\"  cost(a, c) + cost(b, d) <= cost(a, d) + cost(b, c) for a<=b<=c<=d\")\n    print(\"\")\n    print(\"Then optimal split points are monotonic:\")\n    print(\"  opt[i][j-1] <= opt[i][j] <= opt[i+1][j]\")\n    print(\"\")\n    print(\"This allows divide & conquer:\")\n    print(\"  Instead of checking all k in [i, j], only check [opt_l, opt_r]\")\n    print(\"  Reduces O(n³) to O(n² log n)\")\n    print(\"\")\n    print(\"Examples: Optimal BST, Knuth's optimization, partition problems\")\n    print(\"\")\n\ndivide_conquer_dp_example()\n\n# Summary\nprint(\"=== DP Optimization Techniques Summary ===\")\nprint(\"\")\nprint(\"1. Convex Hull Trick:\")\nprint(\"   Use when: Linear transitions (y = mx + b) with monotonic slopes/queries\")\nprint(\"   Speedup: O(n²) → O(n log n) or O(n)\")\nprint(\"\")\nprint(\"2. Monotonic Queue/Deque:\")\nprint(\"   Use when: Sliding window min/max, range DP with monotonicity\")\nprint(\"   Speedup: O(n log n) → O(n)\")\nprint(\"\")\nprint(\"3. Space Optimization:\")\nprint(\"   Use when: DP only depends on previous row/column\")\nprint(\"   Benefit: O(n²) space → O(n) space\")\nprint(\"\")\nprint(\"4. Matrix Exponentiation:\")\nprint(\"   Use when: Linear recurrence, need huge n-th term\")\nprint(\"   Speedup: O(n) → O(k³ log n) where k = matrix size\")\nprint(\"\")\nprint(\"5. Divide & Conquer DP:\")\nprint(\"   Use when: Quadrangle inequality holds, monotonic opt\")\nprint(\"   Speedup: O(kn²) → O(kn log n)\")\nprint(\"\")\nprint(\"6. When to use:\")\nprint(\"   - Basic DP gets TLE on large inputs\")\nprint(\"   - Problem has special structure (monotonicity, convexity, etc.)\")\nprint(\"   - Don't over-optimize unless necessary!\")\n",
      "input": ""
    }
  },
    {
    "slug": "minimum-spanning-tree",
    "title": "Minimum Spanning Tree (MST)",
    "summary": "Master Kruskal's and Prim's algorithms to find minimum cost tree connecting all vertices in a graph.",
    "level": "Advanced",
    "category": "Advanced Graph Algorithms",
    "content": [
      "Minimum Spanning Tree connects all vertices in weighted graph using minimum total edge weight. Tree = no cycles, spanning = includes all vertices. Used in network design, clustering.",
      "Algorithm 1 - Kruskal's: Greedy approach. Sort all edges by weight. Process edges from smallest to largest. Add edge if it doesn't create cycle (use Union-Find to check). Stop when n-1 edges added.",
      "Kruskal's example: Edges [(1,2,1), (2,3,2), (1,3,3), (3,4,4)]. Sort: 1,2,3,4. Add (1,2,1) ✓, add (2,3,2) ✓, skip (1,3,3) because creates cycle, add (3,4,4) ✓. MST cost = 1+2+4 = 7.",
      "Algorithm 2 - Prim's: Greedy approach. Start from any vertex. Maintain set of visited vertices. At each step, add minimum weight edge connecting visited to unvisited vertex. Repeat until all vertices visited.",
      "Prim's example: Start at vertex 1. Visited={1}. Cheapest edge: (1,2,1), add it. Visited={1,2}. Cheapest: (2,3,2), add it. Visited={1,2,3}. Cheapest: (3,4,4), add it. Done! Cost = 7.",
      "Kruskal vs Prim: Kruskal better for sparse graphs (few edges), uses Union-Find. Prim better for dense graphs (many edges), uses priority queue. Both give same MST weight (may differ in edges if ties).",
      "Time complexity: Kruskal O(E log E) for sorting edges + Union-Find operations. Prim O(E log V) with binary heap, O(E + V log V) with Fibonacci heap. Space: O(V + E).",
      "Use cases: Network cable layout (minimize cable cost), road construction (minimize road length), clustering (connect similar items), circuit design (minimize wire length)."
    ],
    "example": {
      "language": "python",
      "code": "# Minimum Spanning Tree - Kruskal's and Prim's Algorithms\n\nimport heapq\nfrom collections import defaultdict\n\n# Union-Find for Kruskal's Algorithm\nclass UnionFind:\n    def __init__(self, n):\n        self.parent = list(range(n))\n        self.rank = [0] * n\n    \n    def find(self, x):\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n    \n    def union(self, x, y):\n        root_x, root_y = self.find(x), self.find(y)\n        if root_x == root_y:\n            return False\n        if self.rank[root_x] < self.rank[root_y]:\n            self.parent[root_x] = root_y\n        elif self.rank[root_x] > self.rank[root_y]:\n            self.parent[root_y] = root_x\n        else:\n            self.parent[root_y] = root_x\n            self.rank[root_x] += 1\n        return True\n\n# Kruskal's Algorithm\ndef kruskal_mst(n, edges):\n    \"\"\"Find MST using Kruskal's algorithm.\n    n: number of vertices (0 to n-1)\n    edges: list of (u, v, weight) tuples\n    Returns: (total_weight, mst_edges)\n    \"\"\"\n    # Sort edges by weight\n    edges.sort(key=lambda x: x[2])\n    \n    uf = UnionFind(n)\n    mst_edges = []\n    total_weight = 0\n    \n    print(\"Kruskal's Algorithm - Processing edges in order:\")\n    for u, v, weight in edges:\n        # Check if adding edge creates cycle\n        if uf.union(u, v):\n            mst_edges.append((u, v, weight))\n            total_weight += weight\n            print(f\"  Add edge ({u}, {v}) weight {weight} ✓\")\n        else:\n            print(f\"  Skip edge ({u}, {v}) weight {weight} (creates cycle) ✗\")\n        \n        # Stop when we have n-1 edges\n        if len(mst_edges) == n - 1:\n            break\n    \n    return total_weight, mst_edges\n\n# Prim's Algorithm\ndef prim_mst(n, edges):\n    \"\"\"Find MST using Prim's algorithm.\n    n: number of vertices (0 to n-1)\n    edges: list of (u, v, weight) tuples\n    Returns: (total_weight, mst_edges)\n    \"\"\"\n    # Build adjacency list\n    graph = defaultdict(list)\n    for u, v, weight in edges:\n        graph[u].append((v, weight))\n        graph[v].append((u, weight))\n    \n    # Start from vertex 0\n    visited = set([0])\n    mst_edges = []\n    total_weight = 0\n    \n    # Priority queue: (weight, from_vertex, to_vertex)\n    pq = [(weight, 0, v) for v, weight in graph[0]]\n    heapq.heapify(pq)\n    \n    print(\"\\nPrim's Algorithm - Growing MST from vertex 0:\")\n    print(f\"  Start at vertex 0\")\n    \n    while pq and len(visited) < n:\n        weight, u, v = heapq.heappop(pq)\n        \n        # Skip if vertex already in MST\n        if v in visited:\n            continue\n        \n        # Add edge to MST\n        visited.add(v)\n        mst_edges.append((u, v, weight))\n        total_weight += weight\n        print(f\"  Add edge ({u}, {v}) weight {weight}. Visited: {sorted(visited)}\")\n        \n        # Add new edges from newly added vertex\n        for next_v, next_weight in graph[v]:\n            if next_v not in visited:\n                heapq.heappush(pq, (next_weight, v, next_v))\n    \n    return total_weight, mst_edges\n\n# Example Graph\nprint(\"=== Minimum Spanning Tree Example ===\")\nprint(\"\\nGraph (vertices 0-4):\")\nprint(\"  0 -- 1 (weight 2)\")\nprint(\"  0 -- 3 (weight 6)\")\nprint(\"  1 -- 2 (weight 3)\")\nprint(\"  1 -- 3 (weight 8)\")\nprint(\"  1 -- 4 (weight 5)\")\nprint(\"  2 -- 4 (weight 7)\")\nprint(\"  3 -- 4 (weight 9)\")\nprint()\n\nn = 5\nedges = [\n    (0, 1, 2),\n    (0, 3, 6),\n    (1, 2, 3),\n    (1, 3, 8),\n    (1, 4, 5),\n    (2, 4, 7),\n    (3, 4, 9)\n]\n\n# Run Kruskal's\nkruskal_weight, kruskal_edges = kruskal_mst(n, edges.copy())\nprint(f\"\\nKruskal's MST Total Weight: {kruskal_weight}\")\nprint(f\"MST Edges: {kruskal_edges}\")\n\n# Run Prim's\nprim_weight, prim_edges = prim_mst(n, edges)\nprint(f\"\\nPrim's MST Total Weight: {prim_weight}\")\nprint(f\"MST Edges: {prim_edges}\")\n\n# Example 2: Disconnected Graph\nprint(\"\\n\\n=== Disconnected Graph Example ===\")\nprint(\"Graph: Two separate components\")\nprint(\"  Component 1: 0--1 (weight 1), 1--2 (weight 2)\")\nprint(\"  Component 2: 3--4 (weight 3)\")\nprint()\n\nedges2 = [(0, 1, 1), (1, 2, 2), (3, 4, 3)]\nkruskal_weight2, kruskal_edges2 = kruskal_mst(5, edges2)\nprint(f\"\\nKruskal's MST Total Weight: {kruskal_weight2}\")\nprint(f\"MST Edges: {kruskal_edges2}\")\nprint(f\"Note: Only {len(kruskal_edges2)} edges (not 4). Graph is disconnected!\")\n\n# Complexity Summary\nprint(\"\\n=== Time Complexity Comparison ===\")\nprint(\"Kruskal's: O(E log E) - dominated by edge sorting\")\nprint(\"  Good for: Sparse graphs (E ≈ V)\")\nprint(\"\\nPrim's: O(E log V) with binary heap\")\nprint(\"  Good for: Dense graphs (E ≈ V²)\")\nprint(\"\\nBoth produce optimal MST with same total weight!\")\n",
      "input": ""
    }
  },
    {
    "slug": "advanced-shortest-path",
    "title": "Advanced Shortest Path Algorithms",
    "summary": "Master Bellman-Ford for negative weights, Floyd-Warshall for all pairs, and A* for heuristic search.",
    "level": "Advanced",
    "category": "Advanced Graph Algorithms",
    "content": [
      "Beyond Dijkstra: Dijkstra finds shortest path in O(E log V) but fails with negative weights. Need advanced algorithms for negative edges, all-pairs shortest path, and optimal pathfinding.",
      "Algorithm 1 - Bellman-Ford: Handles negative edge weights! Relaxes ALL edges V-1 times. If distance decreases on Vth iteration, negative cycle exists. Time: O(VE), slower than Dijkstra but more general.",
      "Bellman-Ford logic: Start with dist[source]=0, all others=∞. For V-1 iterations, check every edge (u,v): if dist[u] + weight(u,v) < dist[v], update dist[v]. After V-1 iterations, distances are finalized unless negative cycle exists.",
      "Algorithm 2 - Floyd-Warshall: Finds shortest paths between ALL pairs of vertices. Uses dynamic programming. Try each vertex as intermediate point: if path through k is shorter, use it. Time: O(V³), works with negative weights.",
      "Floyd-Warshall logic: dp[i][j][k] = shortest path from i to j using vertices 0..k as intermediates. Recurrence: dp[i][j][k] = min(dp[i][j][k-1], dp[i][k][k-1] + dp[k][j][k-1]). Space optimized to 2D array.",
      "Algorithm 3 - A* Search: Heuristic-based pathfinding. Like Dijkstra but uses f(n) = g(n) + h(n) where g(n) = actual cost so far, h(n) = heuristic estimate to goal. If heuristic is admissible, finds optimal path faster.",
      "A* example: In grid, use Manhattan distance as heuristic h(n) = |x-goal_x| + |y-goal_y|. Never overestimates true distance, so A* guaranteed optimal. Explores fewer nodes than Dijkstra by moving toward goal.",
      "When to use: Bellman-Ford for negative weights or cycle detection. Floyd-Warshall for dense graphs needing all-pairs distances. A* for pathfinding in games/maps with good heuristic. Dijkstra still best for non-negative single-source."
    ],
    "example": {
      "language": "python",
      "code": "# Advanced Shortest Path Algorithms\n\nimport heapq\nfrom collections import defaultdict\n\n# Bellman-Ford Algorithm\ndef bellman_ford(n, edges, source):\n    \"\"\"Find shortest paths from source, handling negative weights.\n    n: number of vertices (0 to n-1)\n    edges: list of (u, v, weight) tuples\n    source: starting vertex\n    Returns: (distances, has_negative_cycle)\n    \"\"\"\n    dist = [float('inf')] * n\n    dist[source] = 0\n    \n    print(f\"Bellman-Ford from source {source}:\")\n    print(f\"Initial distances: {dist}\")\n    \n    # Relax edges V-1 times\n    for i in range(n - 1):\n        updated = False\n        print(f\"\\nIteration {i + 1}:\")\n        for u, v, weight in edges:\n            if dist[u] != float('inf') and dist[u] + weight < dist[v]:\n                dist[v] = dist[u] + weight\n                print(f\"  Relax edge ({u}, {v}): dist[{v}] = {dist[v]}\")\n                updated = True\n        if not updated:\n            print(f\"  No updates, converged early!\")\n            break\n    \n    # Check for negative cycle\n    has_negative_cycle = False\n    print(f\"\\nChecking for negative cycles...\")\n    for u, v, weight in edges:\n        if dist[u] != float('inf') and dist[u] + weight < dist[v]:\n            print(f\"  Negative cycle detected via edge ({u}, {v})!\")\n            has_negative_cycle = True\n            break\n    \n    if not has_negative_cycle:\n        print(f\"  No negative cycles found.\")\n    \n    return dist, has_negative_cycle\n\n# Example 1: Bellman-Ford with negative weights\nprint(\"=== Bellman-Ford Example ===\")\nprint(\"Graph with negative edge:\")\nprint(\"  0 → 1 (weight 4)\")\nprint(\"  0 → 2 (weight 5)\")\nprint(\"  1 → 2 (weight -3)\")\nprint(\"  2 → 3 (weight 2)\")\nprint()\n\nedges1 = [(0, 1, 4), (0, 2, 5), (1, 2, -3), (2, 3, 2)]\ndist1, negative_cycle1 = bellman_ford(4, edges1, 0)\nprint(f\"\\nFinal distances from vertex 0: {dist1}\")\nprint(f\"Path: 0 → 1 → 2 (cost 1) is shorter than 0 → 2 (cost 5)!\")\n\n# Example 2: Bellman-Ford with negative cycle\nprint(\"\\n\\n=== Negative Cycle Detection ===\")\nprint(\"Graph with negative cycle:\")\nprint(\"  0 → 1 (weight 1)\")\nprint(\"  1 → 2 (weight -1)\")\nprint(\"  2 → 1 (weight -1)  ← creates negative cycle\")\nprint()\n\nedges2 = [(0, 1, 1), (1, 2, -1), (2, 1, -1)]\ndist2, negative_cycle2 = bellman_ford(3, edges2, 0)\nprint(f\"\\nHas negative cycle: {negative_cycle2}\")\nprint(f\"Cycle: 1 → 2 → 1 has total weight -2 (infinite decrease!)\")\n\n# Floyd-Warshall Algorithm\ndef floyd_warshall(n, edges):\n    \"\"\"Find shortest paths between all pairs of vertices.\n    Returns: 2D distance matrix\n    \"\"\"\n    # Initialize distance matrix\n    dist = [[float('inf')] * n for _ in range(n)]\n    \n    # Distance from vertex to itself is 0\n    for i in range(n):\n        dist[i][i] = 0\n    \n    # Add edges\n    for u, v, weight in edges:\n        dist[u][v] = weight\n    \n    print(\"Floyd-Warshall Algorithm:\")\n    print(\"Initial distance matrix:\")\n    for row in dist:\n        print(f\"  {[x if x != float('inf') else '∞' for x in row]}\")\n    \n    # Try each vertex as intermediate\n    for k in range(n):\n        print(f\"\\nUsing vertex {k} as intermediate:\")\n        for i in range(n):\n            for j in range(n):\n                if dist[i][k] + dist[k][j] < dist[i][j]:\n                    old = dist[i][j]\n                    dist[i][j] = dist[i][k] + dist[k][j]\n                    print(f\"  dist[{i}][{j}] updated: {old if old != float('inf') else '∞'} → {dist[i][j]} (via {k})\")\n    \n    return dist\n\n# Example 3: Floyd-Warshall\nprint(\"\\n\\n=== Floyd-Warshall Example ===\")\nprint(\"Graph (4 vertices):\")\nprint(\"  0 → 1 (3), 0 → 3 (7)\")\nprint(\"  1 → 2 (1), 1 → 3 (2)\")\nprint(\"  2 → 3 (1)\")\nprint()\n\nedges3 = [(0, 1, 3), (0, 3, 7), (1, 2, 1), (1, 3, 2), (2, 3, 1)]\ndist_matrix = floyd_warshall(4, edges3)\n\nprint(\"\\nFinal all-pairs shortest distances:\")\nfor i, row in enumerate(dist_matrix):\n    print(f\"  From {i}: {[x if x != float('inf') else '∞' for x in row]}\")\n\n# A* Search Algorithm\ndef a_star(grid, start, goal):\n    \"\"\"A* pathfinding on 2D grid.\n    grid: 2D array (0 = walkable, 1 = obstacle)\n    start, goal: (row, col) tuples\n    Returns: path as list of coordinates\n    \"\"\"\n    rows, cols = len(grid), len(grid[0])\n    \n    def heuristic(pos):\n        \"\"\"Manhattan distance to goal.\"\"\"\n        return abs(pos[0] - goal[0]) + abs(pos[1] - goal[1])\n    \n    # Priority queue: (f_score, g_score, position, path)\n    pq = [(heuristic(start), 0, start, [start])]\n    visited = set()\n    \n    print(f\"A* Search from {start} to {goal}:\")\n    \n    while pq:\n        f, g, pos, path = heapq.heappop(pq)\n        \n        if pos in visited:\n            continue\n        visited.add(pos)\n        \n        print(f\"  Exploring {pos}: g={g}, h={heuristic(pos)}, f={f}\")\n        \n        if pos == goal:\n            print(f\"\\n  Goal reached! Path length: {len(path)}\")\n            return path\n        \n        # Try 4 directions\n        for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            nr, nc = pos[0] + dr, pos[1] + dc\n            new_pos = (nr, nc)\n            \n            if 0 <= nr < rows and 0 <= nc < cols and grid[nr][nc] == 0:\n                if new_pos not in visited:\n                    new_g = g + 1\n                    new_f = new_g + heuristic(new_pos)\n                    heapq.heappush(pq, (new_f, new_g, new_pos, path + [new_pos]))\n    \n    return None  # No path found\n\n# Example 4: A* Search\nprint(\"\\n\\n=== A* Search Example ===\")\nprint(\"Grid (0=walkable, 1=wall):\")\ngrid = [\n    [0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 1],\n    [0, 0, 0, 0, 0]\n]\nfor row in grid:\n    print(f\"  {row}\")\nprint()\n\npath = a_star(grid, (0, 0), (4, 4))\nif path:\n    print(f\"\\nPath found: {' → '.join(map(str, path))}\")\n    print(f\"Path length: {len(path) - 1} steps\")\n\n# Complexity Summary\nprint(\"\\n\\n=== Time Complexity Comparison ===\")\nprint(\"Dijkstra: O(E log V) - single source, non-negative weights\")\nprint(\"Bellman-Ford: O(VE) - single source, handles negative weights\")\nprint(\"Floyd-Warshall: O(V³) - all pairs, handles negative weights\")\nprint(\"A*: O(E log V) best case - heuristic guides search efficiently\")\nprint(\"\\nSpace: All O(V) except Floyd-Warshall O(V²)\")\n",
      "input": ""
    }
  },
    {
    "slug": "strongly-connected-components",
    "title": "Strongly Connected Components (SCC)",
    "summary": "Learn to find strongly connected components in directed graphs using Kosaraju's and Tarjan's algorithms.",
    "level": "Advanced",
    "category": "Advanced Graph Algorithms",
    "content": [
      "Strongly Connected Component (SCC) in directed graph: maximal set of vertices where every vertex is reachable from every other vertex in the set. Directed graph partitioned into SCCs.",
      "Example: Graph with edges 0→1, 1→2, 2→0 (cycle), 2→3, 3→4, 4→3 (another cycle). Has 2 SCCs: {0,1,2} and {3,4}. Within each SCC, all vertices mutually reachable.",
      "Algorithm 1 - Kosaraju's: Two-pass DFS algorithm. Pass 1: DFS on original graph, record finish times. Pass 2: DFS on reversed graph in decreasing finish time order. Each DFS tree in pass 2 is one SCC.",
      "Kosaraju's logic: Finish time order ensures we process sink SCCs first in reversed graph. When DFS from highest finish time vertex in reversed graph, we stay within its SCC (can't escape to other SCCs).",
      "Algorithm 2 - Tarjan's: Single-pass DFS algorithm. Maintains discovery time and lowest reachable ancestor (low-link). When low[v] = disc[v], v is root of SCC. Uses stack to track current path.",
      "Tarjan's logic: low[v] = min(disc[v], low[child], disc[back_edge_target]). If low[v] = disc[v], no back edge reaches earlier vertex, so v starts new SCC. Pop stack until v to get SCC members.",
      "Kosaraju vs Tarjan: Kosaraju easier to understand (two simple DFS passes). Tarjan more efficient (single pass, no graph reversal). Both O(V+E) time. Choice is preference, same result.",
      "Use cases: Circuit design (feedback loops), compiler optimization (code blocks), web crawling (strongly connected web pages), social network analysis (mutual friend groups), deadlock detection."
    ],
    "example": {
      "language": "python",
      "code": "# Strongly Connected Components\n\nfrom collections import defaultdict, deque\n\n# Kosaraju's Algorithm\nclass KosarajuSCC:\n    def __init__(self, n):\n        self.n = n\n        self.graph = defaultdict(list)\n        self.reversed_graph = defaultdict(list)\n    \n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n        self.reversed_graph[v].append(u)\n    \n    def dfs_finish_time(self, v, visited, stack):\n        \"\"\"First DFS: record finish times.\"\"\"\n        visited.add(v)\n        for neighbor in self.graph[v]:\n            if neighbor not in visited:\n                self.dfs_finish_time(neighbor, visited, stack)\n        stack.append(v)  # Add to stack when finished\n    \n    def dfs_scc(self, v, visited, component):\n        \"\"\"Second DFS: collect SCC members.\"\"\"\n        visited.add(v)\n        component.append(v)\n        for neighbor in self.reversed_graph[v]:\n            if neighbor not in visited:\n                self.dfs_scc(neighbor, visited, component)\n    \n    def find_sccs(self):\n        \"\"\"Find all SCCs using Kosaraju's algorithm.\"\"\"\n        # Pass 1: Get finish times\n        visited = set()\n        stack = []\n        for v in range(self.n):\n            if v not in visited:\n                self.dfs_finish_time(v, visited, stack)\n        \n        print(\"Finish time order (highest to lowest):\")\n        print(f\"  {stack[::-1]}\")\n        \n        # Pass 2: DFS on reversed graph\n        visited = set()\n        sccs = []\n        \n        print(\"\\nFinding SCCs in reversed graph:\")\n        while stack:\n            v = stack.pop()\n            if v not in visited:\n                component = []\n                self.dfs_scc(v, visited, component)\n                sccs.append(component)\n                print(f\"  SCC {len(sccs)}: {sorted(component)}\")\n        \n        return sccs\n\n# Example 1: Kosaraju's Algorithm\nprint(\"=== Kosaraju's Algorithm ===\")\nprint(\"Graph (directed edges):\")\nprint(\"  0 → 1, 1 → 2, 2 → 0  (cycle)\")\nprint(\"  2 → 3, 3 → 4, 4 → 3  (another cycle)\")\nprint(\"  1 → 3\")\nprint()\n\nkosaraju = KosarajuSCC(5)\nedges = [(0, 1), (1, 2), (2, 0), (2, 3), (3, 4), (4, 3), (1, 3)]\nfor u, v in edges:\n    kosaraju.add_edge(u, v)\n\nsccs = kosaraju.find_sccs()\nprint(f\"\\nTotal SCCs found: {len(sccs)}\")\nfor i, scc in enumerate(sccs, 1):\n    print(f\"  SCC {i}: {sorted(scc)}\")\n\n# Tarjan's Algorithm\nclass TarjanSCC:\n    def __init__(self, n):\n        self.n = n\n        self.graph = defaultdict(list)\n        self.disc = [-1] * n  # Discovery time\n        self.low = [-1] * n   # Lowest reachable ancestor\n        self.on_stack = [False] * n\n        self.stack = []\n        self.time = 0\n        self.sccs = []\n    \n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n    \n    def dfs(self, u):\n        \"\"\"Tarjan's DFS.\"\"\"\n        self.disc[u] = self.low[u] = self.time\n        self.time += 1\n        self.stack.append(u)\n        self.on_stack[u] = True\n        \n        print(f\"  Visiting {u}: disc={self.disc[u]}, low={self.low[u]}\")\n        \n        for v in self.graph[u]:\n            if self.disc[v] == -1:\n                # Tree edge\n                self.dfs(v)\n                self.low[u] = min(self.low[u], self.low[v])\n                print(f\"    After visiting child {v}: low[{u}] = {self.low[u]}\")\n            elif self.on_stack[v]:\n                # Back edge to ancestor\n                self.low[u] = min(self.low[u], self.disc[v])\n                print(f\"    Back edge to {v}: low[{u}] = {self.low[u]}\")\n        \n        # If u is root of SCC\n        if self.low[u] == self.disc[u]:\n            scc = []\n            while True:\n                v = self.stack.pop()\n                self.on_stack[v] = False\n                scc.append(v)\n                if v == u:\n                    break\n            self.sccs.append(scc)\n            print(f\"  Found SCC rooted at {u}: {sorted(scc)}\")\n    \n    def find_sccs(self):\n        \"\"\"Find all SCCs using Tarjan's algorithm.\"\"\"\n        print(\"Running Tarjan's DFS:\")\n        for v in range(self.n):\n            if self.disc[v] == -1:\n                self.dfs(v)\n        return self.sccs\n\n# Example 2: Tarjan's Algorithm\nprint(\"\\n\\n=== Tarjan's Algorithm ===\")\nprint(\"Same graph as before:\")\nprint(\"  0 → 1, 1 → 2, 2 → 0  (cycle)\")\nprint(\"  2 → 3, 3 → 4, 4 → 3  (another cycle)\")\nprint(\"  1 → 3\")\nprint()\n\ntarjan = TarjanSCC(5)\nfor u, v in edges:\n    tarjan.add_edge(u, v)\n\nsccs2 = tarjan.find_sccs()\nprint(f\"\\nTotal SCCs found: {len(sccs2)}\")\nfor i, scc in enumerate(sccs2, 1):\n    print(f\"  SCC {i}: {sorted(scc)}\")\n\n# Example 3: Disconnected Graph\nprint(\"\\n\\n=== Disconnected Graph ===\")\nprint(\"Two separate components:\")\nprint(\"  Component 1: 0 → 1 → 0 (cycle)\")\nprint(\"  Component 2: 2 → 3 (no cycle)\")\nprint()\n\nkosaraju2 = KosarajuSCC(4)\nkosaraju2.add_edge(0, 1)\nkosaraju2.add_edge(1, 0)\nkosaraju2.add_edge(2, 3)\n\nsccs3 = kosaraju2.find_sccs()\nprint(f\"\\nTotal SCCs: {len(sccs3)}\")\nfor i, scc in enumerate(sccs3, 1):\n    print(f\"  SCC {i}: {sorted(scc)}\")\n\n# Application: Condensation Graph\ndef build_condensation_graph(n, edges, sccs):\n    \"\"\"Build condensation graph (DAG of SCCs).\"\"\"\n    # Map vertex to SCC index\n    vertex_to_scc = {}\n    for scc_idx, scc in enumerate(sccs):\n        for v in scc:\n            vertex_to_scc[v] = scc_idx\n    \n    # Build condensation edges\n    condensation_edges = set()\n    for u, v in edges:\n        scc_u = vertex_to_scc[u]\n        scc_v = vertex_to_scc[v]\n        if scc_u != scc_v:\n            condensation_edges.add((scc_u, scc_v))\n    \n    return condensation_edges\n\nprint(\"\\n\\n=== Condensation Graph ===\")\nprint(\"Original graph SCCs:\")\nfor i, scc in enumerate(sccs):\n    print(f\"  SCC {i}: {sorted(scc)}\")\n\ncondensation = build_condensation_graph(5, edges, sccs)\nprint(f\"\\nCondensation graph edges (between SCCs):\")\nfor u, v in sorted(condensation):\n    print(f\"  SCC {u} → SCC {v}\")\nprint(\"\\nCondensation graph is always a DAG (no cycles between SCCs)!\")\n\n# Complexity Summary\nprint(\"\\n=== Complexity Comparison ===\")\nprint(\"Kosaraju's: O(V + E) time, O(V) space\")\nprint(\"  - Two DFS passes, easy to understand\")\nprint(\"\\nTarjan's: O(V + E) time, O(V) space\")\nprint(\"  - Single DFS pass, more efficient\")\nprint(\"\\nBoth produce identical SCCs!\")\n",
      "input": ""
    }
  },
    {
    "slug": "network-flow",
    "title": "Network Flow & Bipartite Matching",
    "summary": "Master maximum flow algorithms (Ford-Fulkerson, Edmonds-Karp) and bipartite matching problems.",
    "level": "Advanced",
    "category": "Advanced Graph Algorithms",
    "content": [
      "Network Flow: Directed graph with capacity on each edge. Find maximum flow from source s to sink t such that flow ≤ capacity on each edge, and flow in = flow out at each vertex (except s, t).",
      "Ford-Fulkerson Algorithm: Core idea - repeatedly find augmenting paths from s to t with available capacity, push maximum possible flow through path, update residual capacities. Repeat until no augmenting path exists.",
      "Residual graph: For each edge (u,v) with capacity c and current flow f, residual capacity = c - f. Add reverse edge with capacity f (can undo flow). Augmenting path exists in residual graph means more flow possible.",
      "Edmonds-Karp Algorithm: Ford-Fulkerson using BFS to find shortest augmenting path. Guarantees O(VE²) time. BFS ensures paths get longer, limiting iterations. Most common practical implementation.",
      "Max-flow Min-cut Theorem: Maximum flow value equals minimum cut capacity. Cut = partition of vertices into S (contains s) and T (contains t). Cut capacity = sum of capacities from S to T. Flow can't exceed cut.",
      "Bipartite Matching: Given bipartite graph (vertices split into two sets, edges only between sets), find maximum matching (set of edges with no shared vertices). Reduce to max-flow: add source to left set, sink to right set, all capacities 1.",
      "Applications: Maximum bipartite matching (job assignments, stable marriages), baseball elimination, image segmentation, airline scheduling, project selection, network connectivity (min-cut = weakest link).",
      "Time complexity: Ford-Fulkerson O(E × max_flow) - bad if capacities large. Edmonds-Karp O(VE²) - polynomial time. Dinic's algorithm O(V²E) - faster for dense graphs. Push-relabel O(V²E) or O(V³)."
    ],
    "example": {
      "language": "python",
      "code": "# Network Flow - Edmonds-Karp Algorithm\n\nfrom collections import defaultdict, deque\n\nclass MaxFlow:\n    def __init__(self, n):\n        \"\"\"n vertices: 0 to n-1.\"\"\"\n        self.n = n\n        self.graph = defaultdict(lambda: defaultdict(int))  # graph[u][v] = capacity\n    \n    def add_edge(self, u, v, capacity):\n        \"\"\"Add directed edge with capacity.\"\"\"\n        self.graph[u][v] += capacity\n    \n    def bfs(self, source, sink, parent):\n        \"\"\"BFS to find augmenting path. Returns True if path exists.\"\"\"\n        visited = set([source])\n        queue = deque([source])\n        \n        while queue:\n            u = queue.popleft()\n            \n            for v in self.graph[u]:\n                # Check if unvisited and has residual capacity\n                if v not in visited and self.graph[u][v] > 0:\n                    visited.add(v)\n                    queue.append(v)\n                    parent[v] = u\n                    if v == sink:\n                        return True\n        return False\n    \n    def edmonds_karp(self, source, sink):\n        \"\"\"Find maximum flow from source to sink using Edmonds-Karp.\"\"\"\n        parent = {}\n        max_flow = 0\n        iteration = 0\n        \n        print(f\"Finding max flow from {source} to {sink}:\\n\")\n        \n        while self.bfs(source, sink, parent):\n            iteration += 1\n            \n            # Find minimum capacity along the path\n            path_flow = float('inf')\n            v = sink\n            path = []\n            while v != source:\n                u = parent[v]\n                path.append(f\"{u}→{v}\")\n                path_flow = min(path_flow, self.graph[u][v])\n                v = u\n            \n            path.reverse()\n            print(f\"Iteration {iteration}:\")\n            print(f\"  Path: {' → '.join(path)}\")\n            print(f\"  Path capacity: {path_flow}\")\n            \n            # Update residual capacities\n            v = sink\n            while v != source:\n                u = parent[v]\n                self.graph[u][v] -= path_flow  # Reduce forward capacity\n                self.graph[v][u] += path_flow  # Add reverse capacity\n                v = u\n            \n            max_flow += path_flow\n            print(f\"  Total flow so far: {max_flow}\\n\")\n            parent.clear()\n        \n        return max_flow\n\n# Example 1: Basic Max Flow\nprint(\"=== Maximum Flow Example ===\")\nprint(\"Graph (source=0, sink=5):\")\nprint(\"  0 → 1 (capacity 16)\")\nprint(\"  0 → 2 (capacity 13)\")\nprint(\"  1 → 3 (capacity 12)\")\nprint(\"  2 → 1 (capacity 4)\")\nprint(\"  2 → 4 (capacity 14)\")\nprint(\"  3 → 2 (capacity 9)\")\nprint(\"  3 → 5 (capacity 20)\")\nprint(\"  4 → 3 (capacity 7)\")\nprint(\"  4 → 5 (capacity 4)\")\nprint()\n\nflow_graph = MaxFlow(6)\nedges = [\n    (0, 1, 16), (0, 2, 13),\n    (1, 3, 12), (2, 1, 4), (2, 4, 14),\n    (3, 2, 9), (3, 5, 20),\n    (4, 3, 7), (4, 5, 4)\n]\n\nfor u, v, cap in edges:\n    flow_graph.add_edge(u, v, cap)\n\nmax_flow_value = flow_graph.edmonds_karp(0, 5)\nprint(f\"Maximum flow: {max_flow_value}\")\n\n# Example 2: Bipartite Matching\nclass BipartiteMatching:\n    def __init__(self, left_size, right_size):\n        \"\"\"Bipartite graph with left_size nodes on left, right_size on right.\"\"\"\n        self.left_size = left_size\n        self.right_size = right_size\n        # Vertices: 0=source, 1..left_size=left set, \n        # left_size+1..left_size+right_size=right set, last=sink\n        self.n = left_size + right_size + 2\n        self.source = 0\n        self.sink = self.n - 1\n        self.flow = MaxFlow(self.n)\n        \n        # Connect source to left set with capacity 1\n        for i in range(1, left_size + 1):\n            self.flow.add_edge(self.source, i, 1)\n        \n        # Connect right set to sink with capacity 1\n        for i in range(left_size + 1, left_size + right_size + 1):\n            self.flow.add_edge(i, self.sink, 1)\n    \n    def add_edge(self, left, right):\n        \"\"\"Add edge from left node to right node (0-indexed).\"\"\"\n        left_vertex = 1 + left\n        right_vertex = 1 + self.left_size + right\n        self.flow.add_edge(left_vertex, right_vertex, 1)\n    \n    def max_matching(self):\n        \"\"\"Find maximum matching.\"\"\"\n        return self.flow.edmonds_karp(self.source, self.sink)\n\nprint(\"\\n\\n=== Bipartite Matching Example ===\")\nprint(\"Left set (jobs): {0, 1, 2}\")\nprint(\"Right set (people): {0, 1, 2}\")\nprint(\"Edges (who can do which job):\")\nprint(\"  Job 0 can be done by: Person 0, Person 1\")\nprint(\"  Job 1 can be done by: Person 1, Person 2\")\nprint(\"  Job 2 can be done by: Person 0\")\nprint()\n\nmatching = BipartiteMatching(3, 3)\n# Job -> Person edges\nmatching.add_edge(0, 0)  # Job 0 -> Person 0\nmatching.add_edge(0, 1)  # Job 0 -> Person 1\nmatching.add_edge(1, 1)  # Job 1 -> Person 1\nmatching.add_edge(1, 2)  # Job 1 -> Person 2\nmatching.add_edge(2, 0)  # Job 2 -> Person 0\n\nmax_matching_value = matching.max_matching()\nprint(f\"Maximum matching: {max_matching_value}\")\nprint(\"Possible assignment: Job 0→Person 1, Job 1→Person 2, Job 2→Person 0\")\n\n# Example 3: Min-Cut Application\ndef find_min_cut(n, edges, source, sink):\n    \"\"\"Find minimum cut that separates source and sink.\"\"\"\n    flow_graph = MaxFlow(n)\n    for u, v, cap in edges:\n        flow_graph.add_edge(u, v, cap)\n    \n    max_flow_value = flow_graph.edmonds_karp(source, sink)\n    \n    # Find reachable vertices from source in residual graph\n    visited = set([source])\n    queue = deque([source])\n    while queue:\n        u = queue.popleft()\n        for v in flow_graph.graph[u]:\n            if v not in visited and flow_graph.graph[u][v] > 0:\n                visited.add(v)\n                queue.append(v)\n    \n    # Min cut edges: from visited to non-visited\n    min_cut_edges = []\n    for u in visited:\n        for v in range(n):\n            if v not in visited:\n                # Check original capacity\n                original_capacity = 0\n                for edge_u, edge_v, cap in edges:\n                    if edge_u == u and edge_v == v:\n                        original_capacity = cap\n                        break\n                if original_capacity > 0:\n                    min_cut_edges.append((u, v, original_capacity))\n    \n    return max_flow_value, min_cut_edges, visited\n\nprint(\"\\n\\n=== Min-Cut Example ===\")\nprint(\"Same graph as before, source=0, sink=5\")\nprint()\n\nmin_cut_value, cut_edges, source_side = find_min_cut(6, edges, 0, 5)\nprint(f\"Min-cut value: {min_cut_value} (equals max-flow!)\")\nprint(f\"Source side of cut: {sorted(source_side)}\")\nprint(f\"Sink side of cut: {sorted(set(range(6)) - source_side)}\")\nprint(f\"Cut edges (crossing the partition):\")\nfor u, v, cap in cut_edges:\n    print(f\"  {u} → {v} (capacity {cap})\")\nprint(f\"Total cut capacity: {sum(cap for _, _, cap in cut_edges)}\")\n\n# Complexity Summary\nprint(\"\\n\\n=== Complexity Summary ===\")\nprint(\"Edmonds-Karp: O(VE²) time, O(V + E) space\")\nprint(\"Ford-Fulkerson: O(E × max_flow) - can be slow\")\nprint(\"Dinic's algorithm: O(V²E) - faster for dense graphs\")\nprint(\"\\nBipartite matching: Reduce to max-flow with O(V + E) vertices/edges\")\nprint(\"Result: O(VE²) for maximum matching in bipartite graph\")\n",
      "input": ""
    }
  },
    {
    "slug": "articulation-points-bridges",
    "title": "Articulation Points & Bridges",
    "summary": "Learn to find critical vertices and edges whose removal disconnects the graph using Tarjan's algorithm.",
    "level": "Advanced",
    "category": "Advanced Graph Algorithms",
    "content": [
      "Articulation Point (Cut Vertex): Vertex whose removal increases number of connected components. Critical for network reliability - removing it disconnects parts of network.",
      "Bridge (Cut Edge): Edge whose removal increases number of connected components. Unlike articulation points, even a single bridge can isolate parts of graph.",
      "Tarjan's Algorithm: Single DFS to find both articulation points and bridges. Track discovery time disc[v] and lowest reachable ancestor low[v] for each vertex.",
      "Low-link value: low[v] = min(disc[v], disc[back_edge_targets], low[children]). Represents earliest visited vertex reachable from v's subtree. Key to detecting critical vertices/edges.",
      "Articulation point detection: Vertex u is articulation point if: (1) u is root of DFS tree with 2+ children, OR (2) u has child v where low[v] ≥ disc[u] (can't reach above u without going through u).",
      "Bridge detection: Edge (u,v) is bridge if low[v] > disc[u] (strict inequality!). Means v's subtree cannot reach u or above without using edge (u,v). No alternate path exists.",
      "Implementation: Run DFS, maintain discovery time counter. For each vertex, compute low[v] from children and back edges. Check conditions for articulation point or bridge. Time: O(V+E), single pass.",
      "Use cases: Network design (identify single points of failure), circuit testing (critical connections), social network analysis (key influencers), road networks (critical bridges), internet backbone analysis."
    ],
    "example": {
      "language": "python",
      "code": "# Articulation Points and Bridges - Tarjan's Algorithm\n\nfrom collections import defaultdict\n\nclass CriticalConnections:\n    def __init__(self, n):\n        \"\"\"Undirected graph with n vertices.\"\"\"\n        self.n = n\n        self.graph = defaultdict(list)\n        self.time = 0\n        self.disc = [-1] * n  # Discovery time\n        self.low = [-1] * n   # Lowest reachable ancestor\n        self.parent = [-1] * n\n        self.articulation_points = set()\n        self.bridges = []\n    \n    def add_edge(self, u, v):\n        \"\"\"Add undirected edge.\"\"\"\n        self.graph[u].append(v)\n        self.graph[v].append(u)\n    \n    def dfs(self, u):\n        \"\"\"DFS to find articulation points and bridges.\"\"\"\n        children = 0\n        self.disc[u] = self.low[u] = self.time\n        self.time += 1\n        \n        print(f\"Visiting vertex {u}: disc={self.disc[u]}, low={self.low[u]}\")\n        \n        for v in self.graph[u]:\n            if self.disc[v] == -1:  # Tree edge\n                children += 1\n                self.parent[v] = u\n                self.dfs(v)\n                \n                # Update low[u] from child\n                self.low[u] = min(self.low[u], self.low[v])\n                print(f\"  After visiting child {v}: low[{u}] = {self.low[u]}\")\n                \n                # Check for articulation point\n                # Case 1: u is root and has 2+ children\n                if self.parent[u] == -1 and children > 1:\n                    self.articulation_points.add(u)\n                    print(f\"  → {u} is articulation point (root with {children} children)\")\n                \n                # Case 2: u is not root and low[v] >= disc[u]\n                if self.parent[u] != -1 and self.low[v] >= self.disc[u]:\n                    self.articulation_points.add(u)\n                    print(f\"  → {u} is articulation point (low[{v}]={self.low[v]} >= disc[{u}]={self.disc[u]})\")\n                \n                # Check for bridge: low[v] > disc[u] (strict!)\n                if self.low[v] > self.disc[u]:\n                    self.bridges.append((u, v) if u < v else (v, u))\n                    print(f\"  → Edge ({u}, {v}) is a bridge (low[{v}]={self.low[v]} > disc[{u}]={self.disc[u]})\")\n            \n            elif v != self.parent[u]:  # Back edge\n                self.low[u] = min(self.low[u], self.disc[v])\n                print(f\"  Back edge to {v}: low[{u}] = {self.low[u]}\")\n    \n    def find_critical_connections(self):\n        \"\"\"Find all articulation points and bridges.\"\"\"\n        print(\"Running Tarjan's Algorithm:\\n\")\n        for v in range(self.n):\n            if self.disc[v] == -1:\n                self.dfs(v)\n                print()\n        \n        return self.articulation_points, self.bridges\n\n# Example 1: Find Articulation Points and Bridges\nprint(\"=== Articulation Points and Bridges Example ===\")\nprint(\"Graph (undirected):\")\nprint(\"  0 -- 1 -- 2\")\nprint(\"       |    |\")\nprint(\"       3 -- 4\")\nprint(\"       |\")\nprint(\"       5\")\nprint()\n\ngraph = CriticalConnections(6)\nedges = [(0, 1), (1, 2), (1, 3), (2, 4), (3, 4), (3, 5)]\nfor u, v in edges:\n    graph.add_edge(u, v)\n\nart_points, bridges = graph.find_critical_connections()\n\nprint(\"\\nResults:\")\nprint(f\"Articulation points: {sorted(art_points)}\")\nprint(f\"Bridges: {sorted(bridges)}\")\nprint(\"\\nExplanation:\")\nprint(\"  - Vertex 1: Removing it disconnects 0 from rest → Articulation point\")\nprint(\"  - Vertex 3: Removing it disconnects 5 from rest → Articulation point\")\nprint(\"  - Edge (0,1): Removing it isolates 0 → Bridge\")\nprint(\"  - Edge (3,5): Removing it isolates 5 → Bridge\")\nprint(\"  - Cycle {1,2,3,4}: No bridges within cycle (alternate paths exist)\")\n\n# Example 2: Network with No Critical Elements\nprint(\"\\n\\n=== Network with No Critical Points ===\")\nprint(\"Complete triangle graph: 0 -- 1 -- 2 -- 0\")\nprint()\n\ngraph2 = CriticalConnections(3)\ngraph2.add_edge(0, 1)\ngraph2.add_edge(1, 2)\ngraph2.add_edge(2, 0)\n\nart_points2, bridges2 = graph2.find_critical_connections()\nprint(f\"\\nArticulation points: {sorted(art_points2) if art_points2 else 'None'}\")\nprint(f\"Bridges: {sorted(bridges2) if bridges2 else 'None'}\")\nprint(\"\\nExplanation: Every vertex has alternate paths to others. Highly connected!\")\n\n# Example 3: Star Graph\nprint(\"\\n\\n=== Star Graph (Hub-and-Spoke) ===\")\nprint(\"Center vertex 0 connected to 1, 2, 3, 4\")\nprint(\"      1   2\")\nprint(\"       \\\\ /\")\nprint(\"        0\")\nprint(\"       / \\\\\")\nprint(\"      3   4\")\nprint()\n\ngraph3 = CriticalConnections(5)\nfor i in range(1, 5):\n    graph3.add_edge(0, i)\n\nart_points3, bridges3 = graph3.find_critical_connections()\nprint(f\"\\nArticulation points: {sorted(art_points3)}\")\nprint(f\"Bridges: {sorted(bridges3)}\")\nprint(\"\\nExplanation:\")\nprint(\"  - Vertex 0 is articulation point (root with 4 children)\")\nprint(\"  - ALL edges are bridges (no alternate paths)\")\nprint(\"  - Classic single point of failure architecture!\")\n\n# Application: Strengthen Network\ndef suggest_redundancy(n, articulation_points, bridges):\n    \"\"\"Suggest edges to add to eliminate critical points.\"\"\"\n    print(\"\\n=== Network Strengthening Suggestions ===\")\n    \n    if not articulation_points and not bridges:\n        print(\"✅ Network is already robust! No critical points or bridges.\")\n        return\n    \n    print(f\"Found {len(articulation_points)} articulation points and {len(bridges)} bridges.\")\n    print(\"\\nTo improve reliability:\")\n    \n    if bridges:\n        print(f\"\\n1. Add redundant paths for bridges:\")\n        for u, v in bridges:\n            print(f\"   - Consider adding alternate path around edge ({u}, {v})\")\n    \n    if articulation_points:\n        print(f\"\\n2. Add redundancy around articulation points:\")\n        for ap in sorted(articulation_points):\n            print(f\"   - Add connections bypassing vertex {ap}\")\n    \n    print(\"\\n💡 Goal: Ensure multiple independent paths between all vertex pairs.\")\n\nsuggest_redundancy(6, art_points, bridges)\n\n# Complexity Summary\nprint(\"\\n\\n=== Complexity Summary ===\")\nprint(\"Tarjan's Algorithm: O(V + E) time, O(V) space\")\nprint(\"  - Single DFS traversal\")\nprint(\"  - Computes both articulation points and bridges together\")\nprint(\"  - Very efficient for network analysis\")\nprint(\"\\nBrute force alternative: O(V × (V + E)) - remove each vertex/edge and check connectivity\")\nprint(\"Tarjan's is much faster!\")\n",
      "input": ""
    }
  }
















]
